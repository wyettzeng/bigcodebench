{"task_id": "BigCodeBench/13", "solution": "import subprocess\nimport ftplib\nimport os\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n        print(f\"Successfully connected to FTP server {ftp_server} and logged in with user {ftp_user}\")\n        \n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n        print(f\"Successfully changed to directory {ftp_dir} on server {ftp_server}\")\n        \n        # List all files in the directory\n        files = ftp.nlst()\n        for file in files:\n            filenames.append(file)\n            # Construct the wget command to download the file\n            wget_command = f\"wget -q {ftp_server}/{ftp_dir}/{file}\"\n            # Execute the wget command using subprocess\n            subprocess.run(wget_command, shell=True, check=True)\n            print(f\"Successfully downloaded {file}\")\n    \n    except ftplib.all_errors as e:\n        if \"FTP error\" in str(e):\n            raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        elif \"Login failed\" in str(e):\n            raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        elif \"No such file or directory\" in str(e):\n            raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to download file: {str(e)}\")\n    \n    finally:\n        # Close the FTP connection\n        ftp.quit()\n    \n    return filenames", "raw_solution": "import subprocess\nimport ftplib\nimport os\n\ndef task_func(ftp_server='ftp.dlptest.com', ftp_user='dlpuser', ftp_password='rNrKYTX9g7z3RgJRmxWuGHbeu', ftp_dir='/ftp/test'):\n    filenames = []\n    try:\n        # Connect to the FTP server\n        ftp = ftplib.FTP(ftp_server)\n        ftp.login(user=ftp_user, passwd=ftp_password)\n        print(f\"Successfully connected to FTP server {ftp_server} and logged in with user {ftp_user}\")\n        \n        # Change to the specified directory\n        ftp.cwd(ftp_dir)\n        print(f\"Successfully changed to directory {ftp_dir} on server {ftp_server}\")\n        \n        # List all files in the directory\n        files = ftp.nlst()\n        for file in files:\n            filenames.append(file)\n            # Construct the wget command to download the file\n            wget_command = f\"wget -q {ftp_server}/{ftp_dir}/{file}\"\n            # Execute the wget command using subprocess\n            subprocess.run(wget_command, shell=True, check=True)\n            print(f\"Successfully downloaded {file}\")\n    \n    except ftplib.all_errors as e:\n        if \"FTP error\" in str(e):\n            raise Exception(f\"Failed to connect to FTP server {ftp_server}: {str(e)}\")\n        elif \"Login failed\" in str(e):\n            raise Exception(f\"Failed to log into FTP server {ftp_server} with user {ftp_user}: {str(e)}\")\n        elif \"No such file or directory\" in str(e):\n            raise Exception(f\"Failed to change to directory {ftp_dir} on server {ftp_server}: {str(e)}\")\n    \n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to download file: {str(e)}\")\n    \n    finally:\n        # Close the FTP connection\n        ftp.quit()\n    \n    return filenames"}
{"task_id": "BigCodeBench/15", "solution": "import subprocess\nimport csv\nimport os\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    # List to store the paths of the output files\n    output_files = []\n    \n    # Read the commands from the CSV file\n    with open(commands_file_path, mode='r') as file:\n        reader = csv.reader(file)\n        commands = [row[0] for row in reader]\n    \n    # Execute each command and save the output\n    for index, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n        output_files.append(output_file_path)\n        \n        try:\n            # Execute the command\n            result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            # Write the output to the file\n            with open(output_file_path, 'wb') as output_file:\n                output_file.write(result.stdout)\n        except subprocess.CalledProcessError as e:\n            # Write the error message and exit code to the file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(f\"Error: {e.stderr.decode('utf-8')}\\nExit Code: {e.returncode}\")\n    \n    return output_files", "raw_solution": "import subprocess\nimport csv\nimport os\n\ndef task_func(commands_file_path, output_dir_path):\n    # Check if the commands file exists\n    if not os.path.exists(commands_file_path):\n        raise FileNotFoundError(f\"The file {commands_file_path} does not exist.\")\n    \n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir_path):\n        os.makedirs(output_dir_path)\n    \n    # List to store the paths of the output files\n    output_files = []\n    \n    # Read the commands from the CSV file\n    with open(commands_file_path, mode='r') as file:\n        reader = csv.reader(file)\n        commands = [row[0] for row in reader]\n    \n    # Execute each command and save the output\n    for index, command in enumerate(commands):\n        output_file_path = os.path.join(output_dir_path, f'command_{index}_output.txt')\n        output_files.append(output_file_path)\n        \n        try:\n            # Execute the command\n            result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            # Write the output to the file\n            with open(output_file_path, 'wb') as output_file:\n                output_file.write(result.stdout)\n        except subprocess.CalledProcessError as e:\n            # Write the error message and exit code to the file\n            with open(output_file_path, 'w') as output_file:\n                output_file.write(f\"Error: {e.stderr.decode('utf-8')}\\nExit Code: {e.returncode}\")\n    \n    return output_files"}
{"task_id": "BigCodeBench/17", "solution": "import subprocess\nimport psutil\nimport time\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is found, terminate it\n            proc.kill()\n            time.sleep(1)  # Wait for the process to terminate\n            # Restart the process\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process not found, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\"", "raw_solution": "import subprocess\nimport psutil\nimport time\n\ndef task_func(process_name: str) -> str:\n    # Check if the process is running\n    for proc in psutil.process_iter(['pid', 'name']):\n        if proc.info['name'] == process_name:\n            # Process is found, terminate it\n            proc.kill()\n            time.sleep(1)  # Wait for the process to terminate\n            # Restart the process\n            subprocess.Popen(process_name)\n            return f\"Process found. Restarting {process_name}.\"\n    \n    # Process not found, start it\n    subprocess.Popen(process_name)\n    return f\"Process not found. Starting {process_name}.\""}
{"task_id": "BigCodeBench/19", "solution": "import os\nimport glob\nimport zipfile\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    # Find all files in the directory (not including subdirectories)\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]\n    \n    # If no files are found, return None\n    if not files:\n        return None\n    \n    # Create a zip file named 'files.zip' in the same directory\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    # Return the path to the created zip file\n    return zip_path", "raw_solution": "import os\nimport glob\nimport zipfile\n\ndef task_func(directory):\n    # Check if the specified directory exists\n    if not os.path.exists(directory):\n        raise FileNotFoundError(f\"The directory {directory} does not exist.\")\n    \n    # Find all files in the directory (not including subdirectories)\n    files = glob.glob(os.path.join(directory, '*'))\n    files = [f for f in files if os.path.isfile(f)]\n    \n    # If no files are found, return None\n    if not files:\n        return None\n    \n    # Create a zip file named 'files.zip' in the same directory\n    zip_path = os.path.join(directory, 'files.zip')\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files:\n            zipf.write(file, os.path.basename(file))\n    \n    # Return the path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/34", "solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n    \n    # Check if there are any words left after removing URLs\n    if not re.findall(r'\\b\\w+\\b', text):\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud", "raw_solution": "import re\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Remove URLs from the text\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n    \n    # Check if there are any words left after removing URLs\n    if not re.findall(r'\\b\\w+\\b', text):\n        raise ValueError(\"No words available to generate a word cloud after removing URLs.\")\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Plot the word cloud\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()\n    \n    return wordcloud"}
{"task_id": "BigCodeBench/37", "solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, target_column):\n    # Split the dataframe into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Initialize and train the RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Get feature importances\n    feature_importances = rf_classifier.feature_importances_\n    \n    # Sort feature importances in descending order\n    sorted_idx = feature_importances.argsort()[::-1]\n    sorted_importances = feature_importances[sorted_idx]\n    sorted_feature_names = X.columns[sorted_idx]\n    \n    # Plotting the bar plot\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=sorted_importances, y=sorted_feature_names, palette='viridis')\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    return rf_classifier, plt.gca()", "raw_solution": "from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, target_column):\n    # Split the dataframe into features and target\n    X = df.drop(target_column, axis=1)\n    y = df[target_column]\n    \n    # Initialize and train the RandomForestClassifier\n    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_classifier.fit(X, y)\n    \n    # Get feature importances\n    feature_importances = rf_classifier.feature_importances_\n    \n    # Sort feature importances in descending order\n    sorted_idx = feature_importances.argsort()[::-1]\n    sorted_importances = feature_importances[sorted_idx]\n    sorted_feature_names = X.columns[sorted_idx]\n    \n    # Plotting the bar plot\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=sorted_importances, y=sorted_feature_names, palette='viridis')\n    plt.xlabel('Feature Importance Score')\n    plt.ylabel('Features')\n    plt.title('Visualizing Important Features')\n    plt.show()\n    \n    return rf_classifier, plt.gca()"}
{"task_id": "BigCodeBench/82", "solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\nlogin_manager = LoginManager()\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\nusers = {'user1': User(1, 'user1', generate_password_hash('password123'))}\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, secret_key=secret_key, template_folder=template_folder)\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Logged in as: {current_user.username}'\n\n    return app", "raw_solution": "from flask import Flask, render_template, redirect, url_for\nfrom flask_login import LoginManager, UserMixin, login_user, login_required, logout_user, current_user\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, PasswordField, SubmitField\nfrom wtforms.validators import DataRequired, Length\nfrom werkzeug.security import generate_password_hash, check_password_hash\n\nclass LoginForm(FlaskForm):\n    username = StringField('Username', validators=[DataRequired(), Length(min=4, max=25)])\n    password = PasswordField('Password', validators=[DataRequired(), Length(min=8, max=80)])\n    submit = SubmitField('Log In')\n\nlogin_manager = LoginManager()\n\nclass User(UserMixin):\n    def __init__(self, id, username, password):\n        self.id = id\n        self.username = username\n        self.password = password\n\n    def check_password(self, password):\n        return check_password_hash(self.password, password)\n\nusers = {'user1': User(1, 'user1', generate_password_hash('password123'))}\n\n@login_manager.user_loader\ndef load_user(user_id):\n    return next((user for user in users.values() if user.id == int(user_id)), None)\n\ndef task_func(secret_key, template_folder):\n    app = Flask(__name__, secret_key=secret_key, template_folder=template_folder)\n    login_manager.init_app(app)\n\n    @app.route('/login', methods=['GET', 'POST'])\n    def login():\n        form = LoginForm()\n        if form.validate_on_submit():\n            user = users.get(form.username.data)\n            if user and user.check_password(form.password.data):\n                login_user(user)\n                return redirect(url_for('protected'))\n        return render_template('login.html', form=form)\n\n    @app.route('/logout')\n    @login_required\n    def logout():\n        logout_user()\n        return redirect(url_for('login'))\n\n    @app.route('/protected')\n    @login_required\n    def protected():\n        return f'Logged in as: {current_user.username}'\n\n    return app"}
{"task_id": "BigCodeBench/89", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    \n    Parameters:\n    - data: A pandas DataFrame containing the dataset.\n    - column: The name of the column from which to remove outliers.\n    - outlier_z_score: The Z-score threshold above which data points are considered outliers.\n    \n    Returns:\n    - A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the specified column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores for the specified column\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers from the dataset\n    data_without_outliers = data.drop(outliers)\n    \n    # Plotting\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', label='Data with Outliers')\n    plt.scatter(outliers, data[column].iloc[outliers], color='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outliers)", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\ndef task_func(data, column, outlier_z_score):\n    \"\"\"\n    Identifies and removes outliers from a specified column of a dataset based on the Z-score.\n    \n    Parameters:\n    - data: A pandas DataFrame containing the dataset.\n    - column: The name of the column from which to remove outliers.\n    - outlier_z_score: The Z-score threshold above which data points are considered outliers.\n    \n    Returns:\n    - A tuple containing the original data, the data without outliers, and the indices of the outliers.\n    \"\"\"\n    # Standardize the specified column\n    scaler = StandardScaler()\n    data[column] = scaler.fit_transform(data[[column]])\n    \n    # Calculate Z-scores for the specified column\n    z_scores = np.abs(stats.zscore(data[column]))\n    \n    # Identify outliers\n    outliers = np.where(z_scores > outlier_z_score)[0]\n    \n    # Remove outliers from the dataset\n    data_without_outliers = data.drop(outliers)\n    \n    # Plotting\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(data.index, data[column], color='blue', label='Data with Outliers')\n    plt.scatter(outliers, data[column].iloc[outliers], color='red', label='Outliers')\n    plt.title('Data with Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(data_without_outliers.index, data_without_outliers[column], color='green', label='Data without Outliers')\n    plt.title('Data without Outliers')\n    plt.xlabel('Index')\n    plt.ylabel(column)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return (data, data_without_outliers, outliers)\n\n# Example usage:\n# Assuming 'df' is a pandas DataFrame and 'column_name' is the name of the column to process\n# result = task_func(df, 'column_name', 3)\n# print(result)"}
{"task_id": "BigCodeBench/92", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\ndef task_func(data, n_clusters=3):\n    # Check if 'data' is a pd.DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input 'data' must be a pandas DataFrame.\")\n    \n    # Check if 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Input 'n_clusters' must be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='red')\n    \n    # Add a colorbar to the plot\n    plt.colorbar(scatter)\n    \n    return cluster_labels, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom matplotlib.collections import PathCollection\n\ndef task_func(data, n_clusters=3):\n    # Check if 'data' is a pd.DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Input 'data' must be a pandas DataFrame.\")\n    \n    # Check if 'n_clusters' is an integer greater than 1\n    if not isinstance(n_clusters, int) or n_clusters <= 1:\n        raise ValueError(\"Input 'n_clusters' must be an integer greater than 1.\")\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(data)\n    \n    # Create a scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(data.iloc[:, 0], data.iloc[:, 1], c=cluster_labels, cmap='viridis')\n    \n    # Plot centroids\n    centroids = kmeans.cluster_centers_\n    ax.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='red')\n    \n    # Add a colorbar to the plot\n    plt.colorbar(scatter)\n    \n    return cluster_labels, ax"}
{"task_id": "BigCodeBench/93", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset and record the result.\n    Also, generates a scatter plot of the transformed data.\n\n    Parameters:\n    data (pd.DataFrame): The input dataset.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    pd.DataFrame: The transformed data with principal components.\n    Axes: The matplotlib Axes object containing the scatter plot.\n    \"\"\"\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Convert transformed data to DataFrame\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df['PC1'], transformed_df['PC2'], alpha=0.5)\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef task_func(data, n_components=2):\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a dataset and record the result.\n    Also, generates a scatter plot of the transformed data.\n\n    Parameters:\n    data (pd.DataFrame): The input dataset.\n    n_components (int): The number of principal components to keep.\n\n    Returns:\n    pd.DataFrame: The transformed data with principal components.\n    Axes: The matplotlib Axes object containing the scatter plot.\n    \"\"\"\n    # Check if n_components is a positive integer\n    if not isinstance(n_components, int) or n_components <= 0:\n        raise ValueError(\"n_components must be a positive integer\")\n\n    # Perform PCA\n    pca = PCA(n_components=n_components)\n    transformed_data = pca.fit_transform(data)\n\n    # Convert transformed data to DataFrame\n    transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n\n    # Create scatter plot\n    fig, ax = plt.subplots()\n    ax.scatter(transformed_df['PC1'], transformed_df['PC2'], alpha=0.5)\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_title('PCA Scatter Plot')\n\n    return transformed_df, ax"}
{"task_id": "BigCodeBench/99", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target_names[iris.target]\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    pair_plot = sns.pairplot(df, hue='species')\n\n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot')\n\n    # Return the matplotlib Figure object\n    return pair_plot.fig", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\ndef task_func():\n    # Load the iris dataset\n    iris = load_iris()\n    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    df['species'] = iris.target_names[iris.target]\n\n    # Set the global font to Arial\n    plt.rcParams['font.family'] = 'Arial'\n\n    # Create a pair plot\n    pair_plot = sns.pairplot(df, hue='species')\n\n    # Set the title of the plot\n    pair_plot.fig.suptitle('Iris Dataset Pair Plot')\n\n    # Return the matplotlib Figure object\n    return pair_plot.fig"}
{"task_id": "BigCodeBench/100", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        dates = [datetime.now() - pd.Timedelta(days=i) for i in range(30)]\n        \n        # Generate random values for the time series\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Set the date column as the index\n        df.set_index('Date', inplace=True)\n        \n        # Create a line plot\n        ax = df.plot(kind='line', figsize=(10, 5))\n        \n        # Set the title and labels\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        \n        # Set the font to Arial\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontname('Arial')\n        \n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(seed=42):\n    try:\n        # Set the seed for reproducibility\n        random.seed(seed)\n        \n        # Generate dates for the past 30 days\n        dates = [datetime.now() - pd.Timedelta(days=i) for i in range(30)]\n        \n        # Generate random values for the time series\n        values = [random.randint(0, 100) for _ in range(30)]\n        \n        # Create a DataFrame\n        df = pd.DataFrame({'Date': dates, 'Value': values})\n        \n        # Set the date column as the index\n        df.set_index('Date', inplace=True)\n        \n        # Create a line plot\n        ax = df.plot(kind='line', figsize=(10, 5))\n        \n        # Set the title and labels\n        ax.set_title('Random Time Series Data')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Value')\n        \n        # Set the font to Arial\n        for label in ax.get_xticklabels() + ax.get_yticklabels():\n            label.set_fontname('Arial')\n        \n        return ax\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Example usage:\n# ax = task_func()\n# plt.show()"}
{"task_id": "BigCodeBench/101", "solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    \n    Parameters:\n    - data_url: str, the URL of the Boston Housing dataset.\n    - seed: int, the seed for random number generation.\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n    \n    Raises:\n    - ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Load the Boston Housing dataset\n    boston_data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    boston_data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n    \n    # Calculate the correlation matrix\n    correlation_matrix = boston_data.corr()\n    \n    # Create a heatmap\n    try:\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n        plt.show()\n        return heatmap\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")", "raw_solution": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ndef task_func(data_url=\"http://lib.stat.cmu.edu/datasets/boston\", seed=42):\n    \"\"\"\n    Draws the correlation heatmap of the Boston Housing dataset using Seaborn.\n    \n    Parameters:\n    - data_url: str, the URL of the Boston Housing dataset.\n    - seed: int, the seed for random number generation.\n    \n    Returns:\n    - matplotlib.axes.Axes: The Axes object containing the heatmap plot.\n    \n    Raises:\n    - ValueError: If an error occurs in generating or saving the plot.\n    \"\"\"\n    # Set the seed for reproducibility\n    np.random.seed(seed)\n    \n    # Load the Boston Housing dataset\n    boston_data = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    boston_data.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n    \n    # Calculate the correlation matrix\n    correlation_matrix = boston_data.corr()\n    \n    # Create a heatmap\n    try:\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n        plt.title('Correlation Heatmap of Boston Housing Dataset')\n        plt.show()\n        return heatmap\n    except Exception as e:\n        raise ValueError(f\"An error occurred: {e}\")\n\n# Example usage\ntry:\n    heatmap = task_func()\n    print(heatmap)\nexcept ValueError as e:\n    print(e)"}
{"task_id": "BigCodeBench/108", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Check if 'value' column exists in the DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column contains valid numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data.\")\n    \n    # Check if 'freq' is a valid frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S']\n    if freq not in valid_freqs:\n        raise ValueError(\"Invalid frequency string. Choose from: 'D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S'.\")\n    \n    # Check if 'decomposition_model' is either 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model. Choose from: 'additive' or 'multiplicative'.\")\n    \n    # Perform the decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=freq)\n    \n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n    decomposition.plot(ax=axes)\n    plt.tight_layout()\n    \n    return decomposition, axes", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndef task_func(df, freq='D', decomposition_model='multiplicative'):\n    # Check if df is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n    \n    # Check if 'value' column exists in the DataFrame\n    if 'value' not in df.columns:\n        raise ValueError(\"DataFrame must contain a 'value' column.\")\n    \n    # Check if 'value' column contains valid numeric data\n    if not pd.api.types.is_numeric_dtype(df['value']):\n        raise ValueError(\"The 'value' column must contain numeric data.\")\n    \n    # Check if 'freq' is a valid frequency string\n    valid_freqs = ['D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S']\n    if freq not in valid_freqs:\n        raise ValueError(\"Invalid frequency string. Choose from: 'D', 'W', 'M', 'Q', 'A', 'H', 'T', 'S'.\")\n    \n    # Check if 'decomposition_model' is either 'additive' or 'multiplicative'\n    if decomposition_model not in ['additive', 'multiplicative']:\n        raise ValueError(\"Invalid decomposition model. Choose from: 'additive' or 'multiplicative'.\")\n    \n    # Perform the decomposition\n    decomposition = seasonal_decompose(df['value'], model=decomposition_model, period=freq)\n    \n    # Plot the decomposition\n    fig, axes = plt.subplots(4, 1, figsize=(10, 8))\n    decomposition.plot(ax=axes)\n    plt.tight_layout()\n    \n    return decomposition, axes"}
{"task_id": "BigCodeBench/120", "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range, including both start_date and end_date.\n    \n    Parameters:\n    - start_date (datetime.datetime): The start date of the range (inclusive).\n    - end_date (datetime.datetime): The end date of the range (inclusive).\n    - seed (int): The seed for the random number generator for reproducibility.\n    \n    Returns:\n    - pandas.Series: A Series object containing random dates within the specified range.\n    \n    Raises:\n    - ValueError: If 'start_date' or 'end_date' is not a datetime.datetime instance, or if 'start_date' is later than 'end_date'.\n    \"\"\"\n    # Validate input types and values\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date.\")\n    \n    # Set the seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days between start_date and end_date\n    delta = end_date - start_date\n    num_days = delta.days + 1\n    \n    # Generate random days within the range\n    random_days = [randint(0, num_days - 1) for _ in range(num_days)]\n    \n    # Create a pandas Series of dates\n    random_dates = pd.Series([start_date + timedelta(days=day) for day in random_days])\n    \n    return random_dates", "raw_solution": "import pandas as pd\nfrom datetime import datetime, timedelta\nfrom random import randint, seed as random_seed\n\ndef task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42):\n    \"\"\"\n    Generate a pandas Series of random dates within a specified date range, including both start_date and end_date.\n    \n    Parameters:\n    - start_date (datetime.datetime): The start date of the range (inclusive).\n    - end_date (datetime.datetime): The end date of the range (inclusive).\n    - seed (int): The seed for the random number generator for reproducibility.\n    \n    Returns:\n    - pandas.Series: A Series object containing random dates within the specified range.\n    \n    Raises:\n    - ValueError: If 'start_date' or 'end_date' is not a datetime.datetime instance, or if 'start_date' is later than 'end_date'.\n    \"\"\"\n    # Validate input types and values\n    if not isinstance(start_date, datetime) or not isinstance(end_date, datetime):\n        raise ValueError(\"start_date and end_date must be datetime.datetime instances.\")\n    if start_date > end_date:\n        raise ValueError(\"start_date must be earlier than or equal to end_date.\")\n    \n    # Set the seed for reproducibility\n    random_seed(seed)\n    \n    # Calculate the number of days between start_date and end_date\n    delta = end_date - start_date\n    num_days = delta.days + 1\n    \n    # Generate random days within the range\n    random_days = [randint(0, num_days - 1) for _ in range(num_days)]\n    \n    # Create a pandas Series of dates\n    random_dates = pd.Series([start_date + timedelta(days=day) for day in random_days])\n    \n    return random_dates\n\n# Example usage:\n# random_dates_series = task_func(start_date=datetime(2020, 1, 1), end_date=datetime(2020, 12, 31), seed=42)\n# print(random_dates_series)"}
{"task_id": "BigCodeBench/123", "solution": "import pandas as pd\nimport os\nimport glob\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Calculate the number of files to concatenate\n    num_files = sum(int(x) for x in my_list if x.isdigit())\n    \n    # Check if there are any files to concatenate\n    if num_files == 0:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n    \n    # Get the list of CSV files to concatenate\n    file_list = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    \n    # Check if there are any CSV files in the directory\n    if not file_list:\n        raise FileNotFoundError(\"No CSV files found in the specified directory\")\n    \n    # Concatenate the CSV files into a single DataFrame\n    df = pd.concat([pd.read_csv(file) for file in file_list[:num_files]])\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nimport glob\n\ndef task_func(my_list, file_dir='./data_files/', file_ext='.csv'):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Add '12' to the list\n    my_list.append('12')\n    \n    # Calculate the number of files to concatenate\n    num_files = sum(int(x) for x in my_list if x.isdigit())\n    \n    # Check if there are any files to concatenate\n    if num_files == 0:\n        raise FileNotFoundError(\"No files found in the specified directory\")\n    \n    # Get the list of CSV files to concatenate\n    file_list = glob.glob(os.path.join(file_dir, f'*{file_ext}'))\n    \n    # Check if there are any CSV files in the directory\n    if not file_list:\n        raise FileNotFoundError(\"No CSV files found in the specified directory\")\n    \n    # Concatenate the CSV files into a single DataFrame\n    df = pd.concat([pd.read_csv(file) for file in file_list[:num_files]])\n    \n    return df"}
{"task_id": "BigCodeBench/124", "solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\ndef task_func(my_list, size=100, seed=100):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Check if all elements in my_list are numeric\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements\")\n    \n    # Append 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n    \n    # Ensure the total_sum does not exceed the size limit\n    if total_sum > size:\n        total_sum = size\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Generate a list of random integers\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    \n    # Measure the time taken to generate the list\n    start_time = time.time()\n    # The random number generation is already done in the list comprehension above\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random numbers\n    plt.hist(random_numbers, bins=range(1, 102), edgecolor='black')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    \n    # Return the time taken and the Axes object for the histogram\n    return time_taken, plt.gca()", "raw_solution": "from random import randint, seed as random_seed\nimport time\nimport matplotlib.pyplot as plt\n\ndef task_func(my_list, size=100, seed=100):\n    # Check if my_list is a list\n    if not isinstance(my_list, list):\n        raise TypeError(\"my_list must be a list\")\n    \n    # Check if all elements in my_list are numeric\n    if not all(isinstance(x, (int, float)) for x in my_list):\n        raise ValueError(\"my_list must contain only numeric elements\")\n    \n    # Append 12 to my_list\n    my_list.append(12)\n    \n    # Calculate the sum of elements in my_list\n    total_sum = sum(my_list)\n    \n    # Ensure the total_sum does not exceed the size limit\n    if total_sum > size:\n        total_sum = size\n    \n    # Seed the random number generator\n    random_seed(seed)\n    \n    # Generate a list of random integers\n    random_numbers = [randint(1, 100) for _ in range(total_sum)]\n    \n    # Measure the time taken to generate the list\n    start_time = time.time()\n    # The random number generation is already done in the list comprehension above\n    end_time = time.time()\n    time_taken = end_time - start_time\n    \n    # Plot a histogram of the generated random numbers\n    plt.hist(random_numbers, bins=range(1, 102), edgecolor='black')\n    plt.xlabel('Number')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Random Numbers')\n    plt.show()\n    \n    # Return the time taken and the Axes object for the histogram\n    return time_taken, plt.gca()"}
{"task_id": "BigCodeBench/129", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error occurred: {e}\")\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table')\n    \n    if not table:\n        raise ValueError(\"No table found on the page\")\n    \n    headers = [header.text for header in table.find_all('th')]\n    rows = table.find_all('tr')\n    data = []\n    \n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])  # Remove empty values\n    \n    if headers:\n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n    \n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef task_func(url='http://example.com'):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.ConnectionError:\n        raise ConnectionError(\"Failed to connect to the URL\")\n    except requests.HTTPError as e:\n        raise requests.HTTPError(f\"HTTP error occurred: {e}\")\n    \n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table')\n    \n    if not table:\n        raise ValueError(\"No table found on the page\")\n    \n    headers = [header.text for header in table.find_all('th')]\n    rows = table.find_all('tr')\n    data = []\n    \n    for row in rows:\n        cols = row.find_all(['td', 'th'])\n        cols = [ele.text.strip() for ele in cols]\n        data.append([ele for ele in cols if ele])  # Remove empty values\n    \n    if headers:\n        df = pd.DataFrame(data, columns=headers)\n    else:\n        df = pd.DataFrame(data)\n    \n    return df"}
{"task_id": "BigCodeBench/139", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(df):\n    \"\"\"\n    Draws histograms of numeric columns in a DataFrame and returns the plots.\n    \n    Parameters:\n    - df: pandas DataFrame\n    \n    Returns:\n    - list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    - ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Check if there are any numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    # Create a list to hold the Axes objects\n    axes = []\n    \n    # Loop through each numeric column and create a histogram\n    for col in numeric_cols:\n        ax = df[col].hist(bins=30, figsize=(8, 6))\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(df):\n    \"\"\"\n    Draws histograms of numeric columns in a DataFrame and returns the plots.\n    \n    Parameters:\n    - df: pandas DataFrame\n    \n    Returns:\n    - list: A list of Matplotlib Axes objects, each representing a histogram for a numeric column.\n    \n    Raises:\n    - ValueError: If the input is not a non-empty DataFrame or if there are no numeric columns in the DataFrame.\n    \"\"\"\n    # Check if the input is a non-empty DataFrame\n    if not isinstance(df, pd.DataFrame) or df.empty:\n        raise ValueError(\"Input must be a non-empty DataFrame.\")\n    \n    # Check if there are any numeric columns\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    if not numeric_cols.any():\n        raise ValueError(\"DataFrame must contain at least one numeric column.\")\n    \n    # Create a list to hold the Axes objects\n    axes = []\n    \n    # Loop through each numeric column and create a histogram\n    for col in numeric_cols:\n        ax = df[col].hist(bins=30, figsize=(8, 6))\n        ax.set_title(col)\n        ax.set_xlabel('Value')\n        ax.set_ylabel('Frequency')\n        axes.append(ax)\n    \n    return axes"}
{"task_id": "BigCodeBench/147", "solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((ip, port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = []\n    for ip in ip_network.hosts():\n        thread = Thread(target=worker, args=(str(ip),))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results", "raw_solution": "import socket\nfrom ipaddress import IPv4Network\nfrom threading import Thread\n\ndef task_func(ip_range, port):\n    def check_port(ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(1)\n        result = sock.connect_ex((ip, port))\n        sock.close()\n        return result == 0\n\n    ip_network = IPv4Network(ip_range)\n    results = {}\n\n    def worker(ip):\n        results[ip] = check_port(ip, port)\n\n    threads = []\n    for ip in ip_network.hosts():\n        thread = Thread(target=worker, args=(str(ip),))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return results"}
{"task_id": "BigCodeBench/161", "solution": "import re\nimport pandas as pd\nfrom datetime import datetime\ndef task_func(log_file):\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    \n    # Initialize an empty list to store the log data\n    log_data = []\n    \n    # Open the log file and read line by line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n            if match:\n                # Extract the log type, timestamp, and message from the match object\n                log_type = match.group(1)\n                timestamp = match.group(2)\n                message = match.group(3)\n                \n                # Try to convert the timestamp to a datetime object\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    # If the timestamp is invalid, raise a ValueError\n                    raise ValueError(\"Invalid timestamp in log entry\")\n                \n                # Append the log data to the list\n                log_data.append([log_type, timestamp, message])\n    \n    # If no valid log entries are found, raise a ValueError\n    if not log_data:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a pandas DataFrame from the log data\n    df = pd.DataFrame(log_data, columns=['Type', 'Timestamp', 'Message'])\n    \n    # Define the file path for the new CSV file\n    csv_file = 'structured_logs.csv'\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file, index=False)\n    \n    # Return the file path to the newly created CSV file\n    return csv_file", "raw_solution": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef task_func(log_file):\n    # Define the regular expression pattern for log entries\n    pattern = r'(\\w+): \\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] - (.+)'\n    \n    # Initialize an empty list to store the log data\n    log_data = []\n    \n    # Open the log file and read line by line\n    with open(log_file, 'r') as file:\n        for line in file:\n            # Use regular expression to match the log entry pattern\n            match = re.match(pattern, line)\n            if match:\n                # Extract the log type, timestamp, and message from the match object\n                log_type = match.group(1)\n                timestamp = match.group(2)\n                message = match.group(3)\n                \n                # Try to convert the timestamp to a datetime object\n                try:\n                    datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n                except ValueError:\n                    # If the timestamp is invalid, raise a ValueError\n                    raise ValueError(\"Invalid timestamp in log entry\")\n                \n                # Append the log data to the list\n                log_data.append([log_type, timestamp, message])\n    \n    # If no valid log entries are found, raise a ValueError\n    if not log_data:\n        raise ValueError(\"No valid log entries found\")\n    \n    # Create a pandas DataFrame from the log data\n    df = pd.DataFrame(log_data, columns=['Type', 'Timestamp', 'Message'])\n    \n    # Define the file path for the new CSV file\n    csv_file = 'structured_logs.csv'\n    \n    # Save the DataFrame to a CSV file\n    df.to_csv(csv_file, index=False)\n    \n    # Return the file path to the newly created CSV file\n    return csv_file"}
{"task_id": "BigCodeBench/162", "solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(text, rwidth=0.8):\n    # Split the text into words using regular expression\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Check if there are any words\n    if not words:\n        return plt.figure().add_subplot(111)\n    \n    # Calculate the lengths of the words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=np.arange(min(word_lengths), max(word_lengths) + 2) - 0.5, rwidth=rwidth)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax", "raw_solution": "import re\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(text, rwidth=0.8):\n    # Split the text into words using regular expression\n    words = re.findall(r'\\b\\w+\\b', text)\n    \n    # Check if there are any words\n    if not words:\n        return plt.figure().add_subplot(111)\n    \n    # Calculate the lengths of the words\n    word_lengths = [len(word) for word in words]\n    \n    # Create a histogram of word lengths\n    fig, ax = plt.subplots()\n    ax.hist(word_lengths, bins=np.arange(min(word_lengths), max(word_lengths) + 2) - 0.5, rwidth=rwidth)\n    \n    # Set labels and title\n    ax.set_xlabel('Word Length')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Distribution of Word Lengths')\n    \n    return ax"}
{"task_id": "BigCodeBench/177", "solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\ndef task_func(df):\n    # Check if the DataFrame is empty or does not contain the necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Define the case-insensitive keywords\n    keywords = ['like', 'what']\n\n    # Initialize an empty list to store the content of articles with the specified keywords in their titles\n    relevant_content = []\n\n    # Iterate over the DataFrame rows\n    for index, row in df.iterrows():\n        # Check if the title contains any of the keywords, case-insensitively\n        if any(keyword in row['Title'].lower() for keyword in keywords):\n            # Append the content of the article to the list\n            relevant_content.append(row['Content'])\n\n    # Join all the content into a single string\n    all_content = ' '.join(relevant_content)\n\n    # Remove punctuation from the content\n    all_content = re.sub(f'[{punctuation}]', '', all_content)\n\n    # Tokenize the content into words\n    words = nltk.word_tokenize(all_content)\n\n    # Count the frequency of each word\n    word_frequency = Counter(words)\n\n    # Return the dictionary with word frequencies\n    return dict(word_frequency)", "raw_solution": "import re\nimport nltk\nfrom string import punctuation\nfrom collections import Counter\n\ndef task_func(df):\n    # Check if the DataFrame is empty or does not contain the necessary columns\n    if df.empty or 'Title' not in df.columns or 'Content' not in df.columns:\n        raise ValueError(\"DataFrame is empty or does not contain the necessary columns 'Title' and 'Content'.\")\n\n    # Define the case-insensitive keywords\n    keywords = ['like', 'what']\n\n    # Initialize an empty list to store the content of articles with the specified keywords in their titles\n    relevant_content = []\n\n    # Iterate over the DataFrame rows\n    for index, row in df.iterrows():\n        # Check if the title contains any of the keywords, case-insensitively\n        if any(keyword in row['Title'].lower() for keyword in keywords):\n            # Append the content of the article to the list\n            relevant_content.append(row['Content'])\n\n    # Join all the content into a single string\n    all_content = ' '.join(relevant_content)\n\n    # Remove punctuation from the content\n    all_content = re.sub(f'[{punctuation}]', '', all_content)\n\n    # Tokenize the content into words\n    words = nltk.word_tokenize(all_content)\n\n    # Count the frequency of each word\n    word_frequency = Counter(words)\n\n    # Return the dictionary with word frequencies\n    return dict(word_frequency)"}
{"task_id": "BigCodeBench/184", "solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\ndef task_func(dataframe, text_column):\n    # Preprocess the text data\n    def preprocess_text(text):\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        return words\n\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorize the preprocessed text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column].apply(' '.join))\n\n    # Convert the vectorized data back to a DataFrame\n    result_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return result_df", "raw_solution": "import pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Constants\nSTOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n             'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n             'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n             'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n             'further', 'then', 'once']\n\ndef task_func(dataframe, text_column):\n    # Preprocess the text data\n    def preprocess_text(text):\n        # Remove numbers\n        text = re.sub(r'\\d+', '', text)\n        # Remove punctuation\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Tokenize\n        words = text.split()\n        # Remove stopwords\n        words = [word for word in words if word not in STOPWORDS]\n        return words\n\n    # Apply preprocessing to the specified column\n    dataframe[text_column] = dataframe[text_column].apply(preprocess_text)\n\n    # Vectorize the preprocessed text data\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform(dataframe[text_column].apply(' '.join))\n\n    # Convert the vectorized data back to a DataFrame\n    result_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return result_df"}
{"task_id": "BigCodeBench/187", "solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' and 'Lat' keys are present and their values are tuples\n    if 'Lon' not in dic or 'Lat' not in dic or not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"The dictionary must contain 'Lon' and 'Lat' keys with tuple values.\")\n    \n    # Generate random coordinates within the specified ranges\n    lons = np.random.uniform(dic['Lon'][0], dic['Lon'][1], size=len(cities))\n    lats = np.random.uniform(dic['Lat'][0], dic['Lat'][1], size=len(cities))\n    \n    # Create a list of Point objects\n    points = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create a GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': points})\n    \n    return gdf", "raw_solution": "import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\ndef task_func(dic={'Lon': (-180, 180), 'Lat': (-90, 90)}, cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney']):\n    # Check if 'Lon' and 'Lat' keys are present and their values are tuples\n    if 'Lon' not in dic or 'Lat' not in dic or not isinstance(dic['Lon'], tuple) or not isinstance(dic['Lat'], tuple):\n        raise ValueError(\"The dictionary must contain 'Lon' and 'Lat' keys with tuple values.\")\n    \n    # Generate random coordinates within the specified ranges\n    lons = np.random.uniform(dic['Lon'][0], dic['Lon'][1], size=len(cities))\n    lats = np.random.uniform(dic['Lat'][0], dic['Lat'][1], size=len(cities))\n    \n    # Create a list of Point objects\n    points = [Point(lon, lat) for lon, lat in zip(lons, lats)]\n    \n    # Create a GeoDataFrame\n    gdf = gpd.GeoDataFrame({'City': cities, 'Coordinates': points})\n    \n    return gdf"}
{"task_id": "BigCodeBench/199", "solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate input types\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"All timezones must be strings\")\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n\n    # Create an empty list to store the weather reports\n    weather_reports = []\n\n    # Loop through each city to generate a weather report\n    for city in cities:\n        # Get the timezone for the city\n        city_tz = pytz.timezone(timezones[city])\n\n        # Convert the UTC datetime to the city's local time\n        local_time = utc_datetime.astimezone(city_tz)\n\n        # Select a random weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Append the weather report to the list\n        weather_reports.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    # Create a pandas DataFrame from the weather reports\n    weather_df = pd.DataFrame(weather_reports)\n\n    return weather_df", "raw_solution": "import pandas as pd\nimport pytz\nfrom datetime import datetime\nfrom random import randint, seed as set_seed\n\ndef task_func(\n    utc_datetime,\n    cities=['New York', 'London', 'Beijing', 'Tokyo', 'Sydney'],\n    weather_conditions=['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy'],\n    timezones={\n        'New York': 'America/New_York',\n        'London': 'Europe/London',\n        'Beijing': 'Asia/Shanghai',\n        'Tokyo': 'Asia/Tokyo',\n        'Sydney': 'Australia/Sydney'\n    },\n    seed=42\n):\n    # Validate input types\n    if not isinstance(utc_datetime, datetime):\n        raise ValueError(\"utc_datetime must be a datetime object\")\n    if not all(isinstance(city, str) for city in cities):\n        raise ValueError(\"All cities must be strings\")\n    if not all(isinstance(condition, str) for condition in weather_conditions):\n        raise ValueError(\"All weather conditions must be strings\")\n    if not all(isinstance(tz, str) for tz in timezones.values()):\n        raise ValueError(\"All timezones must be strings\")\n\n    # Set random seed for reproducibility\n    set_seed(seed)\n\n    # Create an empty list to store the weather reports\n    weather_reports = []\n\n    # Loop through each city to generate a weather report\n    for city in cities:\n        # Get the timezone for the city\n        city_tz = pytz.timezone(timezones[city])\n\n        # Convert the UTC datetime to the city's local time\n        local_time = utc_datetime.astimezone(city_tz)\n\n        # Select a random weather condition\n        weather_condition = weather_conditions[randint(0, len(weather_conditions) - 1)]\n\n        # Append the weather report to the list\n        weather_reports.append({\n            'City': city,\n            'Local Time': local_time.strftime('%Y-%m-%d %H:%M:%S %Z'),\n            'Weather Condition': weather_condition\n        })\n\n    # Create a pandas DataFrame from the weather reports\n    weather_df = pd.DataFrame(weather_reports)\n\n    return weather_df"}
{"task_id": "BigCodeBench/208", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate and draw a random sequence of 'elements' number of steps.\n    The steps are either -1 or 1, and the sequence is plotted as a random walk.\n    Returns the descriptive statistics of the random walk and the plot of the random walk.\n    \n    Parameters:\n    elements (int): The number of steps in the random walk.\n    seed (int): The seed for the random number generator.\n    \n    Returns:\n    dict: A dictionary containing the descriptive statistics of the random walk.\n    matplotlib.axes.Axes: The Axes object with the plotted random walk.\n    \"\"\"\n    # Check if elements is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set the seed for the random number generator\n    np.random.seed(seed)\n    \n    # Generate the random walk\n    steps = np.random.choice([-1, 1], size=elements)\n    random_walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th_percentile': np.percentile(random_walk, 5),\n        '25th_percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    \n    return stats, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(elements, seed=0):\n    \"\"\"\n    Generate and draw a random sequence of 'elements' number of steps.\n    The steps are either -1 or 1, and the sequence is plotted as a random walk.\n    Returns the descriptive statistics of the random walk and the plot of the random walk.\n    \n    Parameters:\n    elements (int): The number of steps in the random walk.\n    seed (int): The seed for the random number generator.\n    \n    Returns:\n    dict: A dictionary containing the descriptive statistics of the random walk.\n    matplotlib.axes.Axes: The Axes object with the plotted random walk.\n    \"\"\"\n    # Check if elements is a positive integer\n    if not isinstance(elements, int) or elements <= 0:\n        raise ValueError(\"elements must be a positive integer\")\n    \n    # Set the seed for the random number generator\n    np.random.seed(seed)\n    \n    # Generate the random walk\n    steps = np.random.choice([-1, 1], size=elements)\n    random_walk = np.cumsum(steps)\n    \n    # Calculate descriptive statistics\n    stats = {\n        'count': len(random_walk),\n        'mean': np.mean(random_walk),\n        'std': np.std(random_walk),\n        'min': np.min(random_walk),\n        '5th_percentile': np.percentile(random_walk, 5),\n        '25th_percentile': np.percentile(random_walk, 25),\n        'median': np.median(random_walk),\n        '75th_percentile': np.percentile(random_walk, 75),\n        '95th_percentile': np.percentile(random_walk, 95),\n        'max': np.max(random_walk)\n    }\n    \n    # Plot the random walk\n    fig, ax = plt.subplots()\n    ax.plot(random_walk)\n    ax.set_title('Random Walk')\n    ax.set_xlabel('Steps')\n    ax.set_ylabel('Position')\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/211", "solution": "import requests\nimport os\nimport zipfile\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Save the zip file to a temporary location\n    zip_path = os.path.join(destination_directory, 'temp.zip')\n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the contents of the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get the list of extracted files\n    extracted_files = os.listdir(destination_directory)\n    extracted_files = [f for f in extracted_files if os.path.isfile(os.path.join(destination_directory, f))]\n\n    # Remove the temporary zip file\n    os.remove(zip_path)\n\n    return extracted_files", "raw_solution": "import requests\nimport os\nimport zipfile\n\ndef task_func(url, destination_directory, headers=None):\n    # Ensure the destination directory exists\n    if not os.path.exists(destination_directory):\n        os.makedirs(destination_directory)\n\n    # Download the zip file\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n\n    # Save the zip file to a temporary location\n    zip_path = os.path.join(destination_directory, 'temp.zip')\n    with open(zip_path, 'wb') as f:\n        f.write(response.content)\n\n    # Extract the contents of the zip file\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(destination_directory)\n\n    # Get the list of extracted files\n    extracted_files = os.listdir(destination_directory)\n    extracted_files = [f for f in extracted_files if os.path.isfile(os.path.join(destination_directory, f))]\n\n    # Remove the temporary zip file\n    os.remove(zip_path)\n\n    return extracted_files"}
{"task_id": "BigCodeBench/214", "solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generates a random RGB image and displays it using matplotlib.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound for the random values (inclusive).\n    - range_high (int): Upper bound for the random values (exclusive).\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n    - ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the random seed\n    random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis numbers and ticks\n\n    return ax, image", "raw_solution": "import random\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef task_func(seed=42, image_size=(100, 100, 3), range_low=0, range_high=255):\n    \"\"\"\n    Generates a random RGB image and displays it using matplotlib.\n\n    Parameters:\n    - seed (int): Seed for random number generation.\n    - image_size (tuple): Size of the image (height, width, channels).\n    - range_low (int): Lower bound for the random values (inclusive).\n    - range_high (int): Upper bound for the random values (exclusive).\n\n    Returns:\n    - ax (matplotlib.axes.Axes): Axes object of the plot.\n    - image (numpy.ndarray): The numpy array of the generated image.\n\n    Raises:\n    - ValueError: If range_low is not less than range_high.\n    \"\"\"\n    # Check if range_low is less than range_high\n    if range_low >= range_high:\n        raise ValueError(\"range_low must be less than range_high\")\n\n    # Set the random seed\n    random.seed(seed)\n\n    # Generate a random image\n    image = np.random.randint(range_low, range_high, size=image_size, dtype=np.uint8)\n\n    # Display the image using matplotlib\n    fig, ax = plt.subplots()\n    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax.axis('off')  # Turn off axis numbers and ticks\n\n    return ax, image\n\n# Example usage:\n# ax, image = task_func()\n# plt.show()"}
{"task_id": "BigCodeBench/227", "solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n    \n    # Read the audio data from the file\n    audio_data, sample_rate = sf.read(audio_file)\n    \n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(audio_data**2)))\n    \n    # Create an MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10**(spl/20))\n    \n    # Generate a spectrogram from the normalized matrix\n    fig, ax = plt.subplots()\n    librosa.display.specshow(normalized_matrix, sr=sample_rate, x_axis='time', y_axis='log', ax=ax)\n    \n    # Adjust the amplitude displayed in the spectrogram based on the SPL\n    ax.set_title(f'Spectrogram with SPL: {spl:.2f} dB')\n    \n    return normalized_matrix, fig", "raw_solution": "import numpy as np\nimport os\nimport soundfile as sf\nimport librosa\nimport matplotlib.pyplot as plt\n\ndef task_func(L, M, N, audio_file):\n    # Check if the audio file exists\n    if not os.path.exists(audio_file):\n        raise FileNotFoundError(f\"The specified audio file '{audio_file}' does not exist.\")\n    \n    # Read the audio data from the file\n    audio_data, sample_rate = sf.read(audio_file)\n    \n    # Calculate the sound pressure level (SPL)\n    spl = 20 * np.log10(np.sqrt(np.mean(audio_data**2)))\n    \n    # Create an MxN matrix from the list L\n    matrix = np.array(L).reshape(M, N)\n    \n    # Normalize the matrix based on the SPL\n    normalized_matrix = matrix / (10**(spl/20))\n    \n    # Generate a spectrogram from the normalized matrix\n    fig, ax = plt.subplots()\n    librosa.display.specshow(normalized_matrix, sr=sample_rate, x_axis='time', y_axis='log', ax=ax)\n    \n    # Adjust the amplitude displayed in the spectrogram based on the SPL\n    ax.set_title(f'Spectrogram with SPL: {spl:.2f} dB')\n    \n    return normalized_matrix, fig"}
{"task_id": "BigCodeBench/239", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([item for sublist in original for item in sublist if isinstance(item, (int, float))])\n    \n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.gaussian_kde(numeric_values)\n    ax.plot(x, p(x), \"k\", linewidth=2)\n    \n    return numeric_values, stats_dict, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef task_func(original):\n    # Extract numeric values from the list of tuples\n    numeric_values = np.array([item for sublist in original for item in sublist if isinstance(item, (int, float))])\n    \n    # Compute basic statistics\n    stats_dict = {\n        'mean': np.mean(numeric_values),\n        'std_dev': np.std(numeric_values),\n        'min': np.min(numeric_values),\n        'max': np.max(numeric_values)\n    }\n    \n    # Generate histogram with overlaid PDF\n    fig, ax = plt.subplots()\n    ax.hist(numeric_values, density=True, alpha=0.6, bins='auto')\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.gaussian_kde(numeric_values)\n    ax.plot(x, p(x), \"k\", linewidth=2)\n    \n    return numeric_values, stats_dict, ax"}
{"task_id": "BigCodeBench/241", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\ndef task_func(original):\n    \"\"\"\n    Create a numeric array from the \"original\" list, normalize the array, and draw the original and normalized arrays.\n    \n    Parameters:\n    original (list): A list of numeric values.\n    \n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using MinMaxScaler\n    normalized_array = preprocessing.MinMaxScaler().fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Plotting the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n    \n    return original_array, normalized_array, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\ndef task_func(original):\n    \"\"\"\n    Create a numeric array from the \"original\" list, normalize the array, and draw the original and normalized arrays.\n    \n    Parameters:\n    original (list): A list of numeric values.\n    \n    Returns:\n    np.array: A numpy array for the original data.\n    np.array: Normalized array.\n    matplotlib.axes.Axes: Axes object with the plotted data.\n    \"\"\"\n    # Convert the original list to a numpy array\n    original_array = np.array(original)\n    \n    # Normalize the array using MinMaxScaler\n    normalized_array = preprocessing.MinMaxScaler().fit_transform(original_array.reshape(-1, 1)).flatten()\n    \n    # Plotting the original and normalized arrays\n    fig, ax = plt.subplots()\n    ax.plot(original_array, label='Original Data')\n    ax.plot(normalized_array, label='Normalized Data')\n    ax.set_title('Original vs. Normalized Data')\n    ax.legend()\n    \n    return original_array, normalized_array, ax\n\n# Example usage:\noriginal_data = [1, 2, 3, 4, 5]\noriginal_array, normalized_array, ax = task_func(original_data)\nplt.show()"}
{"task_id": "BigCodeBench/267", "solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Generate a signal based on the values in \"data\"\n    time = np.arange(len(data)) / sample_rate\n    signal = np.sum([data[key] * np.sin(2 * np.pi * key * time) for key in data], axis=0)\n    \n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Calculate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n    \n    # Plot the FFT of the signal\n    plt.figure()\n    plt.plot(freqs[:len(freqs)//2], np.abs(fft_result[:len(fft_result)//2]))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.show()\n    \n    # Return the FFT of the signal and the plot\n    return fft_result, plt.gca()", "raw_solution": "import numpy as np\nfrom scipy import fftpack\nimport matplotlib.pyplot as plt\n\ndef task_func(data, sample_rate=8000):\n    # Add a new key \"a\" with the value 1 to the dictionary\n    data['a'] = 1\n    \n    # Generate a signal based on the values in \"data\"\n    time = np.arange(len(data)) / sample_rate\n    signal = np.sum([data[key] * np.sin(2 * np.pi * key * time) for key in data], axis=0)\n    \n    # Run a Fast Fourier Transform (FFT) on the signal\n    fft_result = fftpack.fft(signal)\n    \n    # Calculate the frequency axis\n    freqs = fftpack.fftfreq(len(signal), d=1/sample_rate)\n    \n    # Plot the FFT of the signal\n    plt.figure()\n    plt.plot(freqs[:len(freqs)//2], np.abs(fft_result[:len(fft_result)//2]))\n    plt.title('FFT of the signal')\n    plt.xlabel('Frequency (Hz)')\n    plt.ylabel('Amplitude')\n    plt.show()\n    \n    # Return the FFT of the signal and the plot\n    return fft_result, plt.gca()"}
{"task_id": "BigCodeBench/273", "solution": "import cgi\nimport http.server\nimport json\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nclass JSONDataHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers['Content-Type']\n        if content_type != 'application/json':\n            self.send_error(400, 'Content-Type header is not application/json')\n            return\n\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_error(400, 'Invalid JSON')\n            return\n\n        if 'data' not in data:\n            self.send_error(400, 'No data key in request')\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\ndef task_func():\n    return JSONDataHandler", "raw_solution": "import cgi\nimport http.server\nimport json\n\nSUCCESS_RESPONSE = {\n    'status': 'success',\n    'message': 'Data received successfully.'\n}\nERROR_RESPONSE = {\n    'status': 'error',\n    'message': 'Invalid data received.'\n}\n\nclass JSONDataHandler(http.server.BaseHTTPRequestHandler):\n    def do_POST(self):\n        content_type = self.headers['Content-Type']\n        if content_type != 'application/json':\n            self.send_error(400, 'Content-Type header is not application/json')\n            return\n\n        content_length = int(self.headers['Content-Length'])\n        post_data = self.rfile.read(content_length)\n        try:\n            data = json.loads(post_data)\n        except json.JSONDecodeError:\n            self.send_error(400, 'Invalid JSON')\n            return\n\n        if 'data' not in data:\n            self.send_error(400, 'No data key in request')\n            return\n\n        self.send_response(200)\n        self.send_header('Content-Type', 'application/json')\n        self.end_headers()\n        self.wfile.write(json.dumps(SUCCESS_RESPONSE).encode('utf-8'))\n\ndef task_func():\n    return JSONDataHandler"}
{"task_id": "BigCodeBench/274", "solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                email_data = json.loads(post_data.decode('utf-8'))\n            except json.JSONDecodeError:\n                self.send_error(400, 'Invalid JSON')\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_error(400, f'Missing key: {key}')\n                    return\n\n            try:\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, email_data['to'], msg.as_string())\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, 'Authentication Failed')\n                return\n\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/plain')\n            self.send_header('Content-Length', '0')\n            self.end_headers()\n\n    return EmailHandler", "raw_solution": "import cgi\nimport http.server\nimport smtplib\nfrom email.mime.text import MIMEText\nimport json\n\ndef task_func(smtp_server, smtp_port, smtp_username, smtp_password):\n    class EmailHandler(http.server.BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                email_data = json.loads(post_data.decode('utf-8'))\n            except json.JSONDecodeError:\n                self.send_error(400, 'Invalid JSON')\n                return\n\n            required_keys = ['subject', 'message', 'to']\n            for key in required_keys:\n                if key not in email_data:\n                    self.send_error(400, f'Missing key: {key}')\n                    return\n\n            try:\n                msg = MIMEText(email_data['message'])\n                msg['Subject'] = email_data['subject']\n                msg['From'] = smtp_username\n                msg['To'] = email_data['to']\n\n                with smtplib.SMTP(smtp_server, smtp_port) as server:\n                    server.starttls()\n                    server.login(smtp_username, smtp_password)\n                    server.sendmail(smtp_username, email_data['to'], msg.as_string())\n            except smtplib.SMTPAuthenticationError:\n                self.send_error(535, 'Authentication Failed')\n                return\n\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/plain')\n            self.send_header('Content-Length', '0')\n            self.end_headers()\n\n    return EmailHandler"}
{"task_id": "BigCodeBench/287", "solution": "from collections import Counter\nimport os\nimport json\ndef task_func(filename, directory):\n    word_counts = Counter()\n    total_words = 0\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    words = f.read().split()\n                    word_counts.update(words)\n                    total_words += len(words)\n\n    # Export the counts to a JSON file\n    with open(filename, 'w') as json_file:\n        json.dump(dict(word_counts), json_file)\n\n    return total_words", "raw_solution": "from collections import Counter\nimport os\nimport json\n\ndef task_func(filename, directory):\n    word_counts = Counter()\n    total_words = 0\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.txt'):\n                file_path = os.path.join(root, file)\n                with open(file_path, 'r') as f:\n                    words = f.read().split()\n                    word_counts.update(words)\n                    total_words += len(words)\n\n    # Export the counts to a JSON file\n    with open(filename, 'w') as json_file:\n        json.dump(dict(word_counts), json_file)\n\n    return total_words"}
{"task_id": "BigCodeBench/302", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame with columns 'Date' and 'Value'.\n    plot (bool): If True, returns a matplotlib Axes object containing the heatmap plot.\n\n    Returns:\n    pd.DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n    Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the 'Value' column contains valid lists\n    if not all(isinstance(x, list) for x in df['Value']):\n        raise ValueError(\"The 'Value' column contains invalid lists.\")\n    \n    # Split lists in the 'Value' column into separate columns\n    df_expanded = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Calculate the Pearson correlation coefficient\n    correlation_matrix = df_expanded.corr(method='pearson')\n    \n    # Optionally visualize the correlation matrix using a heatmap\n    if plot:\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n        heatmap.set_title('Correlation Heatmap')\n        plt.show()\n        return correlation_matrix, heatmap\n    else:\n        return correlation_matrix", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Constants\nCOLUMNS = ['Date', 'Value']\n\ndef task_func(df, plot=False):\n    \"\"\"\n    Processes a pandas DataFrame by splitting lists in the 'Value' column into separate columns,\n    calculates the Pearson correlation coefficient between these columns, and optionally visualizes\n    the correlation matrix using a heatmap.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame with columns 'Date' and 'Value'.\n    plot (bool): If True, returns a matplotlib Axes object containing the heatmap plot.\n\n    Returns:\n    pd.DataFrame: A pandas DataFrame containing the correlation coefficients among the lists in the 'Value' column.\n    Axes (optional): A matplotlib Axes object containing the heatmap plot, returned if 'plot' is True.\n    \"\"\"\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the 'Value' column contains valid lists\n    if not all(isinstance(x, list) for x in df['Value']):\n        raise ValueError(\"The 'Value' column contains invalid lists.\")\n    \n    # Split lists in the 'Value' column into separate columns\n    df_expanded = pd.DataFrame(df['Value'].tolist(), index=df.index)\n    \n    # Calculate the Pearson correlation coefficient\n    correlation_matrix = df_expanded.corr(method='pearson')\n    \n    # Optionally visualize the correlation matrix using a heatmap\n    if plot:\n        plt.figure(figsize=(10, 8))\n        heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n        heatmap.set_title('Correlation Heatmap')\n        plt.show()\n        return correlation_matrix, heatmap\n    else:\n        return correlation_matrix\n\n# Example usage:\n# df = pd.DataFrame({\n#     'Date': ['2021-01-01', '2021-01-02', '2021-01-03'],\n#     'Value': [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# })\n# result, heatmap = task_func(df, plot=True)\n# print(result)"}
{"task_id": "BigCodeBench/308", "solution": "import pandas as pd\nfrom statistics import mean\nimport random\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\ndef task_func(additional_fields = []):\n    # Create a dictionary to store the grades\n    grades = {field: [] for field in FIELDS}\n    \n    # Populate the grades dictionary with random values\n    for _ in range(len(STUDENTS)):\n        for field in FIELDS:\n            grades[field].append(random.randint(0, 100))\n    \n    # Create a DataFrame from the grades dictionary\n    df = pd.DataFrame(grades)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    df.loc['Average'] = df.mean()\n    \n    return df", "raw_solution": "import pandas as pd\nfrom statistics import mean\nimport random\n\n# Constants for generating the report data\nFIELDS = ['Physics', 'Math', 'Chemistry', 'Biology', 'English', 'History']\nSTUDENTS = ['Student_' + str(i) for i in range(1, 101)]\n\ndef task_func(additional_fields = []):\n    # Create a dictionary to store the grades\n    grades = {field: [] for field in FIELDS}\n    \n    # Populate the grades dictionary with random values\n    for _ in range(len(STUDENTS)):\n        for field in FIELDS:\n            grades[field].append(random.randint(0, 100))\n    \n    # Create a DataFrame from the grades dictionary\n    df = pd.DataFrame(grades)\n    \n    # Calculate the average grade for each student\n    df['Average Grade'] = df.mean(axis=1)\n    \n    # Calculate the average grade for each subject\n    df.loc['Average'] = df.mean()\n    \n    return df"}
{"task_id": "BigCodeBench/310", "solution": "import os\nimport csv\nimport random\nfrom statistics import mean\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\ndef task_func(filename):\n    # Generate random data for 100 people\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{random.randint(1, 1000)}\"\n        age = random.randint(18, 65)\n        height = random.uniform(140, 200)\n        weight = random.uniform(40, 120)\n        data.append([name, age, height, weight])\n\n    # Calculate average values\n    avg_age = mean([person[1] for person in data])\n    avg_height = mean([person[2] for person in data])\n    avg_weight = mean([person[3] for person in data])\n\n    # Append average values to data\n    data.append(['Average', avg_age, avg_height, avg_weight])\n\n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    # Return the path of the created CSV file\n    return os.path.abspath(filename)", "raw_solution": "import os\nimport csv\nimport random\nfrom statistics import mean\n\n# Constants\nCOLUMNS = ['Name', 'Age', 'Height', 'Weight']\nPEOPLE_COUNT = 100\n\ndef task_func(filename):\n    # Generate random data for 100 people\n    data = []\n    for _ in range(PEOPLE_COUNT):\n        name = f\"Person{random.randint(1, 1000)}\"\n        age = random.randint(18, 65)\n        height = random.uniform(140, 200)\n        weight = random.uniform(40, 120)\n        data.append([name, age, height, weight])\n\n    # Calculate average values\n    avg_age = mean([person[1] for person in data])\n    avg_height = mean([person[2] for person in data])\n    avg_weight = mean([person[3] for person in data])\n\n    # Append average values to data\n    data.append(['Average', avg_age, avg_height, avg_weight])\n\n    # Write data to CSV file\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(COLUMNS)\n        writer.writerows(data)\n\n    # Return the path of the created CSV file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/313", "solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\ndef task_func(directory):\n    # Dictionary to store files organized by their first non-bracketed text\n    organized_files = {}\n    # Regex to find the first text not enclosed in square brackets\n    pattern = re.compile(r'(?<!\\[)\\b\\w+\\b(?![^\\]]*\\])')\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Read the first line of the file\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                first_line = f.readline().strip()\n                # Find the first non-bracketed text\n                match = pattern.search(first_line)\n                if match:\n                    key = match.group(0)\n                    if key not in organized_files:\n                        organized_files[key] = []\n                    organized_files[key].append(file_path)\n                else:\n                    # If no match, log the file path\n                    print(f\"No matching text found in {file_path}\")\n\n    # Create subdirectories and move files\n    for key, file_paths in organized_files.items():\n        subdir_path = os.path.join(directory, key)\n        os.makedirs(subdir_path, exist_ok=True)\n        for file_path in file_paths:\n            shutil.move(file_path, subdir_path)\n\n    # Return the directory path and the dictionary of organized files\n    return directory, organized_files", "raw_solution": "import re\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef task_func(directory):\n    # Dictionary to store files organized by their first non-bracketed text\n    organized_files = {}\n    # Regex to find the first text not enclosed in square brackets\n    pattern = re.compile(r'(?<!\\[)\\b\\w+\\b(?![^\\]]*\\])')\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            # Read the first line of the file\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                first_line = f.readline().strip()\n                # Find the first non-bracketed text\n                match = pattern.search(first_line)\n                if match:\n                    key = match.group(0)\n                    if key not in organized_files:\n                        organized_files[key] = []\n                    organized_files[key].append(file_path)\n                else:\n                    # If no match, log the file path\n                    print(f\"No matching text found in {file_path}\")\n\n    # Create subdirectories and move files\n    for key, file_paths in organized_files.items():\n        subdir_path = os.path.join(directory, key)\n        os.makedirs(subdir_path, exist_ok=True)\n        for file_path in file_paths:\n            shutil.move(file_path, subdir_path)\n\n    # Return the directory path and the dictionary of organized files\n    return directory, organized_files"}
{"task_id": "BigCodeBench/324", "solution": "import subprocess\nimport time\nimport threading\ndef task_func(file_list):\n    exit_codes = []\n\n    def run_file(file):\n        result = subprocess.run(['python', file], capture_output=True, text=True)\n        exit_codes.append(result.returncode)\n\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return exit_codes", "raw_solution": "import subprocess\nimport time\nimport threading\n\ndef task_func(file_list):\n    exit_codes = []\n\n    def run_file(file):\n        result = subprocess.run(['python', file], capture_output=True, text=True)\n        exit_codes.append(result.returncode)\n\n    threads = []\n    for file in file_list:\n        thread = threading.Thread(target=run_file, args=(file,))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    return exit_codes"}
{"task_id": "BigCodeBench/326", "solution": "import subprocess\nimport os\nimport sys\nimport glob\ndef task_func(directory_path):\n    result = []\n    # Use glob to find all .bat files in the given directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    for bat_file in bat_files:\n        try:\n            # Execute the .bat file and capture the exit code\n            exit_code = subprocess.call(bat_file, shell=True)\n            result.append((bat_file, exit_code))\n        except Exception as e:\n            # If an error occurs, append the file name and None as the exit code\n            result.append((bat_file, None))\n    return result", "raw_solution": "import subprocess\nimport os\nimport sys\nimport glob\n\ndef task_func(directory_path):\n    result = []\n    # Use glob to find all .bat files in the given directory\n    bat_files = glob.glob(os.path.join(directory_path, '*.bat'))\n    for bat_file in bat_files:\n        try:\n            # Execute the .bat file and capture the exit code\n            exit_code = subprocess.call(bat_file, shell=True)\n            result.append((bat_file, exit_code))\n        except Exception as e:\n            # If an error occurs, append the file name and None as the exit code\n            result.append((bat_file, None))\n    return result"}
{"task_id": "BigCodeBench/341", "solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df, col):\n    \"\"\"\n    Generates a matplotlib figure with a histogram and box plot for the specified column in a pandas DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n    col (str): The name of the column to plot.\n\n    Returns:\n    matplotlib.figure.Figure: A matplotlib figure object containing the histogram and box plot.\n\n    Raises:\n    ValueError: If the input df is not a DataFrame, is empty, or does not contain the specified column.\n    \"\"\"\n    # Check if the input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column '{col}'.\")\n\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plot histogram with kernel density estimate\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n    axes[0].set_xlabel(col)\n    axes[0].set_ylabel('Frequency')\n\n    # Plot box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    axes[1].set_xlabel(col)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return fig", "raw_solution": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df, col):\n    \"\"\"\n    Generates a matplotlib figure with a histogram and box plot for the specified column in a pandas DataFrame.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame.\n    col (str): The name of the column to plot.\n\n    Returns:\n    matplotlib.figure.Figure: A matplotlib figure object containing the histogram and box plot.\n\n    Raises:\n    ValueError: If the input df is not a DataFrame, is empty, or does not contain the specified column.\n    \"\"\"\n    # Check if the input is a DataFrame\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"The input df must be a DataFrame.\")\n    \n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The input df must not be empty.\")\n    \n    # Check if the specified column exists in the DataFrame\n    if col not in df.columns:\n        raise ValueError(f\"The input df must contain the specified column '{col}'.\")\n\n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Plot histogram with kernel density estimate\n    sns.histplot(df[col], kde=True, ax=axes[0])\n    axes[0].set_title(f'Histogram of {col}')\n    axes[0].set_xlabel(col)\n    axes[0].set_ylabel('Frequency')\n\n    # Plot box plot\n    sns.boxplot(x=df[col], ax=axes[1])\n    axes[1].set_title(f'Box Plot of {col}')\n    axes[1].set_xlabel(col)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return fig"}
{"task_id": "BigCodeBench/346", "solution": "import subprocess\nimport os\nimport sys\nimport time\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n    \n    Args:\n    script_path (str): The path to the Python script to run.\n    wait (bool): If True, waits for the process to complete. If False, returns None.\n    *args: Additional arguments to pass to the script.\n    \n    Returns:\n    int: The return code of the subprocess. If 'wait' is False, returns None.\n    \n    Raises:\n    ValueError: If the script does not exist.\n    subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.exists(script_path):\n        raise ValueError(f\"The script {script_path} does not exist.\")\n    \n    # Prepare the command to run the script\n    command = [sys.executable, script_path] + list(args)\n    \n    # Run the script as a subprocess\n    try:\n        if wait:\n            # Wait for the process to complete and get the return code\n            result = subprocess.run(command, check=True, capture_output=True, text=True)\n            return result.returncode\n        else:\n            # Start the process but do not wait for it to complete\n            subprocess.Popen(command)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Raise an exception if the script raises an exception\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)", "raw_solution": "import subprocess\nimport os\nimport sys\nimport time\n\ndef task_func(script_path, wait=True, *args):\n    \"\"\"\n    Run a Python script as a process with predefined arguments.\n    \n    Args:\n    script_path (str): The path to the Python script to run.\n    wait (bool): If True, waits for the process to complete. If False, returns None.\n    *args: Additional arguments to pass to the script.\n    \n    Returns:\n    int: The return code of the subprocess. If 'wait' is False, returns None.\n    \n    Raises:\n    ValueError: If the script does not exist.\n    subprocess.CalledProcessError: If the script raises an exception.\n    \"\"\"\n    # Check if the script exists\n    if not os.path.exists(script_path):\n        raise ValueError(f\"The script {script_path} does not exist.\")\n    \n    # Prepare the command to run the script\n    command = [sys.executable, script_path] + list(args)\n    \n    # Run the script as a subprocess\n    try:\n        if wait:\n            # Wait for the process to complete and get the return code\n            result = subprocess.run(command, check=True, capture_output=True, text=True)\n            return result.returncode\n        else:\n            # Start the process but do not wait for it to complete\n            subprocess.Popen(command)\n            return None\n    except subprocess.CalledProcessError as e:\n        # Raise an exception if the script raises an exception\n        raise subprocess.CalledProcessError(e.returncode, e.cmd, e.output, e.stderr)\n\n# Example usage:\n# return_code = task_func('path_to_your_script.py', wait=True, arg1='value1', arg2='value2')\n# print(f\"Return code: {return_code}\")"}
{"task_id": "BigCodeBench/360", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation\n    stats = df.describe().T[['mean', 'std']]\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], capsize=5)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    \n    return stats.to_dict(), fig", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef task_func(file_location, sheet_name):\n    # Check if the file exists\n    if not os.path.exists(file_location):\n        raise FileNotFoundError(\"The specified file does not exist.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_location, sheet_name=sheet_name)\n    except KeyError:\n        raise ValueError(\"The specified sheet does not exist in the workbook.\")\n    \n    # Calculate mean and standard deviation\n    stats = df.describe().T[['mean', 'std']]\n    \n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(stats.index, stats['mean'], yerr=stats['std'], capsize=5)\n    ax.set_title('Mean and Standard Deviation')\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Values')\n    \n    return stats.to_dict(), fig"}
{"task_id": "BigCodeBench/367", "solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n    \n    # Dictionary to count activities per day of the week\n    activity_count = defaultdict(int)\n    \n    # Count activities for each day of the week\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_count[day_of_week] += 1\n    \n    # Prepare data for plotting\n    days = list(activity_count.keys())\n    counts = list(activity_count.values())\n    \n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts, color='blue')\n    \n    # Set labels and title\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax", "raw_solution": "from datetime import datetime\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\ndef task_func(activities):\n    # Check if all activities are datetime objects\n    if not all(isinstance(activity, datetime) for activity in activities):\n        raise TypeError(\"All activities must be datetime objects\")\n    \n    # Dictionary to count activities per day of the week\n    activity_count = defaultdict(int)\n    \n    # Count activities for each day of the week\n    for activity in activities:\n        day_of_week = activity.strftime('%A')\n        activity_count[day_of_week] += 1\n    \n    # Prepare data for plotting\n    days = list(activity_count.keys())\n    counts = list(activity_count.values())\n    \n    # Create bar chart\n    fig, ax = plt.subplots()\n    ax.bar(days, counts, color='blue')\n    \n    # Set labels and title\n    ax.set_xlabel('Day of the Week')\n    ax.set_ylabel('Number of Activities')\n    ax.set_title('Weekly Activity')\n    \n    return ax"}
{"task_id": "BigCodeBench/368", "solution": "import os\nimport shutil\nimport random\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the seed for the random number generator for reproducibility\n    random.seed(seed)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n    \n    # Select a random file from the list\n    file_to_move = random.choice(files)\n    \n    # Construct the full path of the source file\n    src_file_path = os.path.join(src_dir, file_to_move)\n    \n    # Construct the full path of the destination file\n    dest_file_path = os.path.join(dest_dir, file_to_move)\n    \n    # Move the file from the source directory to the destination directory\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the file moved\n    return file_to_move", "raw_solution": "import os\nimport shutil\nimport random\n\ndef task_func(src_dir: str, dest_dir: str, seed: int = 100) -> str:\n    # Set the seed for the random number generator for reproducibility\n    random.seed(seed)\n    \n    # List all files in the source directory\n    files = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]\n    \n    # Check if there are any files in the source directory\n    if not files:\n        raise ValueError(\"No files found in the source directory.\")\n    \n    # Select a random file from the list\n    file_to_move = random.choice(files)\n    \n    # Construct the full path of the source file\n    src_file_path = os.path.join(src_dir, file_to_move)\n    \n    # Construct the full path of the destination file\n    dest_file_path = os.path.join(dest_dir, file_to_move)\n    \n    # Move the file from the source directory to the destination directory\n    shutil.move(src_file_path, dest_file_path)\n    \n    # Return the name of the file moved\n    return file_to_move"}
{"task_id": "BigCodeBench/374", "solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\ndef task_func(directory_path='./xlsx_files/'):\n    # Initialize the count of processed files\n    processed_files = 0\n    \n    # Use glob to find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Loop through each file found\n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file_path)\n        \n        # Loop through each sheet in the workbook\n        for sheet in workbook.sheetnames:\n            # Select the sheet\n            worksheet = workbook[sheet]\n            \n            # Loop through each cell in the sheet\n            for row in worksheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string and contains double quotes\n                    if isinstance(cell.value, str) and '\"' in cell.value:\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\"', cell.value)\n        \n        # Save the workbook\n        workbook.save(file_path)\n        \n        # Increment the count of processed files\n        processed_files += 1\n    \n    # Return the number of Excel files processed\n    return processed_files", "raw_solution": "import regex as re\nimport glob\nimport os\nfrom openpyxl import load_workbook\n\ndef task_func(directory_path='./xlsx_files/'):\n    # Initialize the count of processed files\n    processed_files = 0\n    \n    # Use glob to find all .xlsx files in the specified directory\n    xlsx_files = glob.glob(os.path.join(directory_path, '*.xlsx'))\n    \n    # Loop through each file found\n    for file_path in xlsx_files:\n        # Load the workbook\n        workbook = load_workbook(file_path)\n        \n        # Loop through each sheet in the workbook\n        for sheet in workbook.sheetnames:\n            # Select the sheet\n            worksheet = workbook[sheet]\n            \n            # Loop through each cell in the sheet\n            for row in worksheet.iter_rows():\n                for cell in row:\n                    # Check if the cell value is a string and contains double quotes\n                    if isinstance(cell.value, str) and '\"' in cell.value:\n                        # Replace double quotes with double backslashes\n                        cell.value = re.sub(r'\"', r'\\\"', cell.value)\n        \n        # Save the workbook\n        workbook.save(file_path)\n        \n        # Increment the count of processed files\n        processed_files += 1\n    \n    # Return the number of Excel files processed\n    return processed_files"}
{"task_id": "BigCodeBench/399", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Generate a plot of a sine wave and cosine wave with the given frequency.\n\n    Parameters:\n    frequency (float): The frequency of the sine and cosine waves.\n    sample_size (int): The number of samples to generate.\n\n    Returns:\n    matplotlib.figure.Figure: The figure object containing the plot.\n    matplotlib.axes.Axes: The axes object of the plot.\n\n    Raises:\n    ValueError: If the frequency is negative.\n    ValueError: If the sample size is non-positive.\n    \"\"\"\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    t = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n\n    return fig, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef task_func(frequency, sample_size=10000):\n    \"\"\"\n    Generate a plot of a sine wave and cosine wave with the given frequency.\n\n    Parameters:\n    frequency (float): The frequency of the sine and cosine waves.\n    sample_size (int): The number of samples to generate.\n\n    Returns:\n    matplotlib.figure.Figure: The figure object containing the plot.\n    matplotlib.axes.Axes: The axes object of the plot.\n\n    Raises:\n    ValueError: If the frequency is negative.\n    ValueError: If the sample size is non-positive.\n    \"\"\"\n    if frequency < 0:\n        raise ValueError(\"Frequency must be non-negative.\")\n    if sample_size <= 0:\n        raise ValueError(\"Sample size must be positive.\")\n\n    t = np.linspace(0, 1, sample_size)\n    sine_wave = np.sin(2 * np.pi * frequency * t)\n    cosine_wave = np.cos(2 * np.pi * frequency * t)\n\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Sine Wave')\n    ax.plot(t, cosine_wave, label='Cosine Wave')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Sine and Cosine Waves')\n    ax.legend()\n\n    return fig, ax"}
{"task_id": "BigCodeBench/401", "solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\ndef task_func(app_name):\n    # Create a Flask application instance\n    app = Flask(app_name)\n    \n    # Retrieve email server details from environment variables or use defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'False').lower() == 'true'\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n    \n    # Configure the Flask-Mail instance\n    mail = Mail(app)\n    app.config['MAIL_SERVER'] = mail_server\n    app.config['MAIL_PORT'] = mail_port\n    app.config['MAIL_USE_TLS'] = mail_use_tls\n    app.config['MAIL_USERNAME'] = mail_username\n    app.config['MAIL_PASSWORD'] = mail_password\n    \n    # Return the Flask-Mail instance and the app's mail configurations\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']", "raw_solution": "from flask import Flask\nimport os\nfrom flask_mail import Mail\n\ndef task_func(app_name):\n    # Create a Flask application instance\n    app = Flask(app_name)\n    \n    # Retrieve email server details from environment variables or use defaults\n    mail_server = os.getenv('MAIL_SERVER', 'localhost')\n    mail_port = int(os.getenv('MAIL_PORT', 25))\n    mail_use_tls = os.getenv('MAIL_USE_TLS', 'False').lower() == 'true'\n    mail_username = os.getenv('MAIL_USERNAME', None)\n    mail_password = os.getenv('MAIL_PASSWORD', None)\n    \n    # Configure the Flask-Mail instance\n    mail = Mail(app)\n    app.config['MAIL_SERVER'] = mail_server\n    app.config['MAIL_PORT'] = mail_port\n    app.config['MAIL_USE_TLS'] = mail_use_tls\n    app.config['MAIL_USERNAME'] = mail_username\n    app.config['MAIL_PASSWORD'] = mail_password\n    \n    # Return the Flask-Mail instance and the app's mail configurations\n    return mail, app.config['MAIL_SERVER'], app.config['MAIL_PORT'], app.config['MAIL_USE_TLS'], app.config['MAIL_USERNAME'], app.config['MAIL_PASSWORD']"}
{"task_id": "BigCodeBench/409", "solution": "import os\nimport pandas as pd\nimport numpy as np\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full file path\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path {excel_file_path}.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_path)\n    except Exception as e:\n        raise ValueError(f\"Failed to read the Excel file: {e}\")\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n    \n    # Calculate mean, median, and standard deviation\n    mean_value = df[column_name].mean()\n    median_value = df[column_name].median()\n    std_deviation = df[column_name].std()\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'standard_deviation': std_deviation\n    }", "raw_solution": "import os\nimport pandas as pd\nimport numpy as np\n\ndef task_func(excel_file_path, file_name, column_name):\n    # Construct the full file path\n    file_path = os.path.join(excel_file_path, file_name)\n    \n    # Check if the file exists\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"The file {file_name} does not exist at the specified path {excel_file_path}.\")\n    \n    # Load the Excel file\n    try:\n        df = pd.read_excel(file_path)\n    except Exception as e:\n        raise ValueError(f\"Failed to read the Excel file: {e}\")\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not found in the Excel file.\")\n    \n    # Calculate mean, median, and standard deviation\n    mean_value = df[column_name].mean()\n    median_value = df[column_name].median()\n    std_deviation = df[column_name].std()\n    \n    # Return the results as a dictionary\n    return {\n        'mean': mean_value,\n        'median': median_value,\n        'standard_deviation': std_deviation\n    }"}
{"task_id": "BigCodeBench/417", "solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\ndef task_func(X, Y):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \n    Parameters:\n    - X: numpy array, input data with shape (n_samples, 2)\n    - Y: numpy array, target labels with shape (n_samples,)\n    \n    Returns:\n    - Sequential: The trained Keras Sequential model.\n    - matplotlib.axes.Axes: The Axes object of the plot.\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct a Sequential model\n    model = Sequential([\n        Dense(10, input_dim=2, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    # Compile the model\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, Y_test))\n    \n    # Plot the model's training and validation loss\n    plt.figure(figsize=(10, 5))\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    return model, plt.gca()", "raw_solution": "from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\ndef task_func(X, Y):\n    \"\"\"\n    Trains a simple neural network on given input data and target labels.\n    \n    Parameters:\n    - X: numpy array, input data with shape (n_samples, 2)\n    - Y: numpy array, target labels with shape (n_samples,)\n    \n    Returns:\n    - Sequential: The trained Keras Sequential model.\n    - matplotlib.axes.Axes: The Axes object of the plot.\n    \"\"\"\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n    \n    # Construct a Sequential model\n    model = Sequential([\n        Dense(10, input_dim=2, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    # Compile the model\n    optimizer = SGD(learning_rate=0.01)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    # Fit the model\n    history = model.fit(X_train, Y_train, epochs=50, batch_size=10, verbose=0, validation_data=(X_test, Y_test))\n    \n    # Plot the model's training and validation loss\n    plt.figure(figsize=(10, 5))\n    plt.plot(history.history['loss'], label='Train')\n    plt.plot(history.history['val_loss'], label='Test')\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    return model, plt.gca()"}
{"task_id": "BigCodeBench/418", "solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(10, input_shape=(X_train.shape[1],), activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC score\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()", "raw_solution": "from tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\ndef task_func(X, Y):\n    # Split the data into training and test sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\n    # Create a Keras Sequential model with one hidden layer\n    model = keras.Sequential([\n        keras.layers.Dense(10, input_shape=(X_train.shape[1],), activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model with binary cross-entropy loss and an SGD optimizer\n    model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n\n    # Fit the model to the training data in a non-verbose mode\n    model.fit(X_train, Y_train, epochs=10, verbose=0)\n\n    # Predict probabilities on the test set\n    Y_pred_prob = model.predict(X_test)\n\n    # Compute ROC curve and AUC score\n    fpr, tpr, _ = roc_curve(Y_test, Y_pred_prob)\n    roc_auc = auc(fpr, tpr)\n\n    # Plot the ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return model, plt.gca()"}
{"task_id": "BigCodeBench/424", "solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Failed to read the image from {image_path}.\")\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image.reshape(-1, 3)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    segmented_pixels = kmeans.cluster_centers_[kmeans.labels_]\n    \n    # Reshape the segmented pixels back to the original image shape\n    segmented_image = segmented_pixels.reshape(image.shape)\n    \n    return image, segmented_image", "raw_solution": "import cv2\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\n\ndef task_func(image_path='image.jpg', n_clusters=3, random_seed=42):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n    \n    # Check if n_clusters is a positive integer\n    if not isinstance(n_clusters, int) or n_clusters <= 0:\n        raise ValueError(\"n_clusters must be a positive integer.\")\n    \n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Failed to read the image from {image_path}.\")\n    \n    # Reshape the image to a 2D array of pixels\n    pixels = image.reshape(-1, 3)\n    \n    # Apply K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_seed)\n    kmeans.fit(pixels)\n    segmented_pixels = kmeans.cluster_centers_[kmeans.labels_]\n    \n    # Reshape the segmented pixels back to the original image shape\n    segmented_image = segmented_pixels.reshape(image.shape)\n    \n    return image, segmented_image"}
{"task_id": "BigCodeBench/443", "solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n\n    Parameters:\n    - P: numpy.ndarray, a matrix.\n    - T: numpy.ndarray, a 3D tensor.\n    - n_clusters: int, the number of clusters for KMeans.\n    - random_state: int, the random state for reproducibility.\n    - n_init: int, the number of times the k-means algorithm will be run with different centroid seeds.\n\n    Returns:\n    - cluster_result: numpy.ndarray, the result of KMeans clustering.\n    - ax: matplotlib.axes.Axes, the visualization of the KMeans clustering.\n    \"\"\"\n    # Calculate the product of matrix P and tensor T\n    result = np.einsum('ij,klm->iklm', P, T)\n    \n    # Flatten the result\n    flattened_result = result.reshape(-1, result.shape[-1])\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n    \n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax", "raw_solution": "import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef task_func(\n    P: np.ndarray,\n    T: np.ndarray,\n    n_clusters: int = 3,\n    random_state: int = 0,\n    n_init: int = 10,\n) -> (np.ndarray, plt.Axes):\n    \"\"\"\n    Calculate the product of a matrix 'P' and a 3D tensor 'T', flatten the result,\n    apply KMeans clustering to the flattened data, and visualize it.\n\n    Parameters:\n    - P: numpy.ndarray, a matrix.\n    - T: numpy.ndarray, a 3D tensor.\n    - n_clusters: int, the number of clusters for KMeans.\n    - random_state: int, the random state for reproducibility.\n    - n_init: int, the number of times the k-means algorithm will be run with different centroid seeds.\n\n    Returns:\n    - cluster_result: numpy.ndarray, the result of KMeans clustering.\n    - ax: matplotlib.axes.Axes, the visualization of the KMeans clustering.\n    \"\"\"\n    # Calculate the product of matrix P and tensor T\n    result = np.einsum('ij,klm->iklm', P, T)\n    \n    # Flatten the result\n    flattened_result = result.reshape(-1, result.shape[-1])\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n    cluster_result = kmeans.fit_predict(flattened_result)\n    \n    # Visualize the KMeans clustering\n    fig, ax = plt.subplots()\n    ax.scatter(flattened_result[:, 0], flattened_result[:, 1], c=cluster_result, cmap='viridis')\n    ax.set_title('KMeans Clustering Visualization')\n    \n    return cluster_result, ax"}
{"task_id": "BigCodeBench/445", "solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n    \n    Parameters:\n    - points: A numpy array of shape (n, 2) representing the points.\n    - seed: An integer seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    - tuple (vor, ax): A tuple containing:\n        - vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        - ax (Axes): The axes of the plotted Voronoi diagram.\n    \"\"\"\n    # Check if the input points are valid\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a numpy array of shape (n, 2).\")\n    \n    # Apply jittering to the points\n    np.random.seed(seed)\n    jitter = 0.1 * np.random.rand(points.shape[0], 2)\n    points_jittered = points + jitter\n    \n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    return vor, ax", "raw_solution": "import numpy as np\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nimport matplotlib.pyplot as plt\n\ndef task_func(points, seed=0):\n    \"\"\"\n    Calculate the Voronoi diagram for a number of points in 2D and plot it.\n    \n    Parameters:\n    - points: A numpy array of shape (n, 2) representing the points.\n    - seed: An integer seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    - tuple (vor, ax): A tuple containing:\n        - vor (Voronoi): A Voronoi object representing the Voronoi diagram of the points.\n        - ax (Axes): The axes of the plotted Voronoi diagram.\n    \"\"\"\n    # Check if the input points are valid\n    if not isinstance(points, np.ndarray) or points.ndim != 2 or points.shape[1] != 2:\n        raise ValueError(\"Input points must be a numpy array of shape (n, 2).\")\n    \n    # Apply jittering to the points\n    np.random.seed(seed)\n    jitter = 0.1 * np.random.rand(points.shape[0], 2)\n    points_jittered = points + jitter\n    \n    # Calculate the Voronoi diagram\n    vor = Voronoi(points_jittered)\n    \n    # Plot the Voronoi diagram\n    fig, ax = plt.subplots()\n    voronoi_plot_2d(vor, ax=ax)\n    \n    return vor, ax"}
{"task_id": "BigCodeBench/454", "solution": "import os\nimport shutil\nimport glob\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    # List to store the paths of moved files\n    moved_files = []\n\n    # Search for files in the source directory with the specified extension\n    for file_path in glob.glob(os.path.join(src_dir, f'*.{ext}')):\n        # Extract the file name from the full path\n        file_name = os.path.basename(file_path)\n\n        # Construct the destination file path\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file to the destination directory\n            shutil.move(file_path, dest_file_path)\n            # Add the moved file path to the list\n            moved_files.append(dest_file_path)\n\n    return moved_files", "raw_solution": "import os\nimport shutil\nimport glob\n\ndef task_func(src_dir, dest_dir, ext):\n    # Check if source and destination directories exist\n    if not os.path.exists(src_dir):\n        raise FileNotFoundError(f\"Source directory {src_dir} does not exist\")\n    if not os.path.exists(dest_dir):\n        raise FileNotFoundError(f\"Destination directory {dest_dir} does not exist\")\n\n    # List to store the paths of moved files\n    moved_files = []\n\n    # Search for files in the source directory with the specified extension\n    for file_path in glob.glob(os.path.join(src_dir, f'*.{ext}')):\n        # Extract the file name from the full path\n        file_name = os.path.basename(file_path)\n\n        # Construct the destination file path\n        dest_file_path = os.path.join(dest_dir, file_name)\n\n        # Check if the file already exists in the destination directory\n        if not os.path.exists(dest_file_path):\n            # Move the file to the destination directory\n            shutil.move(file_path, dest_file_path)\n            # Add the moved file path to the list\n            moved_files.append(dest_file_path)\n\n    return moved_files"}
{"task_id": "BigCodeBench/458", "solution": "import json\nimport re\nimport pandas as pd\ndef task_func(json_str):\n    # Check if the input JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n    \n    # Load the JSON string into a dictionary\n    data_dict = json.loads(json_str)\n    \n    # Function to double numerical values\n    def double_values(value):\n        if isinstance(value, list):\n            return [double_values(x) for x in value]\n        elif isinstance(value, str):\n            # Try to convert string to float and double it\n            try:\n                return float(value) * 2\n            except ValueError:\n                return value\n        elif isinstance(value, (int, float)):\n            return value * 2\n        else:\n            return value\n    \n    # Normalize the dictionary by doubling the numerical values\n    normalized_dict = {key: double_values(value) for key, value in data_dict.items()}\n    \n    # Create a Pandas DataFrame from the normalized dictionary\n    df = pd.DataFrame(normalized_dict)\n    \n    return df", "raw_solution": "import json\nimport re\nimport pandas as pd\n\ndef task_func(json_str):\n    # Check if the input JSON string is empty\n    if not json_str:\n        return pd.DataFrame()\n    \n    # Load the JSON string into a dictionary\n    data_dict = json.loads(json_str)\n    \n    # Function to double numerical values\n    def double_values(value):\n        if isinstance(value, list):\n            return [double_values(x) for x in value]\n        elif isinstance(value, str):\n            # Try to convert string to float and double it\n            try:\n                return float(value) * 2\n            except ValueError:\n                return value\n        elif isinstance(value, (int, float)):\n            return value * 2\n        else:\n            return value\n    \n    # Normalize the dictionary by doubling the numerical values\n    normalized_dict = {key: double_values(value) for key, value in data_dict.items()}\n    \n    # Create a Pandas DataFrame from the normalized dictionary\n    df = pd.DataFrame(normalized_dict)\n    \n    return df"}
{"task_id": "BigCodeBench/461", "solution": "import subprocess\nimport psutil\nimport time\nimport os\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n    \n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0\n    \n    # Start a timer\n    start_time = time.time()\n    \n    while True:\n        # Check if the process has finished\n        if process.poll() is not None:\n            break\n        \n        # Check if the timeout has been reached\n        if time.time() - start_time > timeout:\n            process.terminate()\n            raise TimeoutError(f\"The script at {script_path} exceeded the timeout of {timeout} seconds.\")\n        \n        # Get the process object\n        try:\n            proc = psutil.Process(process.pid)\n        except psutil.NoSuchProcess:\n            process.terminate()\n            raise Exception(f\"The process for the script at {script_path} was not found.\")\n        \n        # Get CPU and memory usage\n        cpu_usage += proc.cpu_times().user + proc.cpu_times().system\n        memory_usage += proc.memory_info().rss\n        \n        # Sleep for a short period to avoid high CPU usage\n        time.sleep(0.1)\n    \n    # Wait for the process to finish\n    process.wait()\n    \n    # Return the CPU and memory usage\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }", "raw_solution": "import subprocess\nimport psutil\nimport time\nimport os\n\ndef task_func(script_path: str, timeout=10) -> dict:\n    # Check if the script path exists\n    if not os.path.exists(script_path):\n        raise FileNotFoundError(f\"The script at {script_path} does not exist.\")\n    \n    # Start the subprocess\n    process = subprocess.Popen(['bash', script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    \n    # Initialize CPU and memory usage\n    cpu_usage = 0.0\n    memory_usage = 0\n    \n    # Start a timer\n    start_time = time.time()\n    \n    while True:\n        # Check if the process has finished\n        if process.poll() is not None:\n            break\n        \n        # Check if the timeout has been reached\n        if time.time() - start_time > timeout:\n            process.terminate()\n            raise TimeoutError(f\"The script at {script_path} exceeded the timeout of {timeout} seconds.\")\n        \n        # Get the process object\n        try:\n            proc = psutil.Process(process.pid)\n        except psutil.NoSuchProcess:\n            process.terminate()\n            raise Exception(f\"The process for the script at {script_path} was not found.\")\n        \n        # Get CPU and memory usage\n        cpu_usage += proc.cpu_times().user + proc.cpu_times().system\n        memory_usage += proc.memory_info().rss\n        \n        # Sleep for a short period to avoid high CPU usage\n        time.sleep(0.1)\n    \n    # Wait for the process to finish\n    process.wait()\n    \n    # Return the CPU and memory usage\n    return {\n        'CPU Usage': cpu_usage,\n        'Memory Usage': memory_usage\n    }"}
{"task_id": "BigCodeBench/477", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Generates a DataFrame with N rows and 3 columns ('x', 'y', 'category') and plots a scatter plot.\n    \n    Parameters:\n    - N: Number of rows in the DataFrame.\n    - CATEGORIES: List of categories to be used in the 'category' column.\n    - seed: Seed for random number generation for reproducibility.\n    \n    Returns:\n    - A tuple containing:\n        - DataFrame: The generated DataFrame.\n        - Axes: The Axes object of the scatter plot.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Ensure each category appears at least once if N >= len(CATEGORIES)\n    if N >= len(CATEGORIES):\n        categories = np.array(CATEGORIES)\n        remaining = np.random.choice(CATEGORIES, size=N - len(CATEGORIES), replace=False)\n        categories = np.concatenate((categories, remaining))\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    \n    # Generate random values for 'x' and 'y'\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'x': x, 'y': y, 'category': categories})\n    \n    # Plot scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=df['category'], cmap='viridis')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y colored by category')\n    \n    return df, ax", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(N=100, CATEGORIES=[\"A\", \"B\", \"C\", \"D\", \"E\"], seed=42):\n    \"\"\"\n    Generates a DataFrame with N rows and 3 columns ('x', 'y', 'category') and plots a scatter plot.\n    \n    Parameters:\n    - N: Number of rows in the DataFrame.\n    - CATEGORIES: List of categories to be used in the 'category' column.\n    - seed: Seed for random number generation for reproducibility.\n    \n    Returns:\n    - A tuple containing:\n        - DataFrame: The generated DataFrame.\n        - Axes: The Axes object of the scatter plot.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Ensure each category appears at least once if N >= len(CATEGORIES)\n    if N >= len(CATEGORIES):\n        categories = np.array(CATEGORIES)\n        remaining = np.random.choice(CATEGORIES, size=N - len(CATEGORIES), replace=False)\n        categories = np.concatenate((categories, remaining))\n    else:\n        categories = np.random.choice(CATEGORIES, size=N, replace=False)\n    \n    # Generate random values for 'x' and 'y'\n    x = np.random.rand(N)\n    y = np.random.rand(N)\n    \n    # Create DataFrame\n    df = pd.DataFrame({'x': x, 'y': y, 'category': categories})\n    \n    # Plot scatter plot\n    fig, ax = plt.subplots()\n    scatter = ax.scatter(df['x'], df['y'], c=df['category'], cmap='viridis')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Scatter Plot of x vs y colored by category')\n    \n    return df, ax\n\n# Example usage:\n# df, ax = task_func(N=150, CATEGORIES=[\"Red\", \"Blue\", \"Green\"], seed=42)\n# plt.show()"}
{"task_id": "BigCodeBench/486", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates a time series with a specified start time, end time, step, and trend.\n    The values are generated from a normal distribution and a linear trend is added.\n    \n    Parameters:\n    - start_time (str): The start time in 'YYYY-MM-DD HH:MM:SS' format.\n    - end_time (str): The end time in 'YYYY-MM-DD HH:MM:SS' format.\n    - step (int): The time step in seconds.\n    - trend (float): The linear trend value to be added to the time series.\n    - seed (int): The random seed for reproducibility.\n    \n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Convert start and end times to datetime objects\n    start = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate time series\n    time_series = pd.date_range(start=start, end=end, freq=f'{step}S')\n    values = np.random.normal(size=len(time_series)) + trend * np.arange(len(time_series))\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_series, 'Value': values})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'], label='Time Series')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Generated Time Series')\n    ax.legend()\n    \n    return ax\nstart_time = '2023-01-01 00:00:00'\nend_time = '2023-01-01 00:01:00'\nstep = 10\ntrend = 0.1", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(start_time, end_time, step, trend, seed=42):\n    \"\"\"\n    Generates a time series with a specified start time, end time, step, and trend.\n    The values are generated from a normal distribution and a linear trend is added.\n    \n    Parameters:\n    - start_time (str): The start time in 'YYYY-MM-DD HH:MM:SS' format.\n    - end_time (str): The end time in 'YYYY-MM-DD HH:MM:SS' format.\n    - step (int): The time step in seconds.\n    - trend (float): The linear trend value to be added to the time series.\n    - seed (int): The random seed for reproducibility.\n    \n    Returns:\n    - ax (matplotlib.pyplot.Axes): The Axes object of the generated plot.\n    \"\"\"\n    # Convert start and end times to datetime objects\n    start = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n    end = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n    \n    # Generate time series\n    time_series = pd.date_range(start=start, end=end, freq=f'{step}S')\n    values = np.random.normal(size=len(time_series)) + trend * np.arange(len(time_series))\n    \n    # Create DataFrame\n    df = pd.DataFrame({'Time': time_series, 'Value': values})\n    \n    # Plotting\n    fig, ax = plt.subplots()\n    ax.plot(df['Time'], df['Value'], label='Time Series')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Value')\n    ax.set_title('Generated Time Series')\n    ax.legend()\n    \n    return ax\n\n# Example usage\nstart_time = '2023-01-01 00:00:00'\nend_time = '2023-01-01 00:01:00'\nstep = 10\ntrend = 0.1\nax = task_func(start_time, end_time, step, trend)\nplt.show()"}
{"task_id": "BigCodeBench/492", "solution": "import pandas as pd\nfrom datetime import datetime\nimport random\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list of 5 strings\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"All elements in products must be strings\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Get current date\n    end_date = datetime.now()\n\n    # Initialize an empty list to store sales data\n    sales_data = []\n\n    # Generate sales data for each day between start_date and end_date\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({'Product': product, 'Date': current_date, 'Sales': sales})\n        current_date += timedelta(days=1)\n\n    # Create a DataFrame from the sales data\n    df = pd.DataFrame(sales_data)\n\n    return df", "raw_solution": "import pandas as pd\nfrom datetime import datetime\nimport random\n\ndef task_func(\n    epoch_milliseconds,\n    random_seed=0,\n    products=[\"Product1\", \"Product2\", \"Product3\", \"Product4\", \"Product5\"],\n):\n    # Check input validity\n    if not isinstance(epoch_milliseconds, int) or epoch_milliseconds < 0:\n        raise ValueError(\"epoch_milliseconds must be a non-negative integer\")\n    if not isinstance(random_seed, int):\n        raise ValueError(\"random_seed must be an integer\")\n    if not isinstance(products, list) or len(products) != 5:\n        raise ValueError(\"products must be a list of 5 strings\")\n    for product in products:\n        if not isinstance(product, str):\n            raise ValueError(\"All elements in products must be strings\")\n\n    # Set random seed for reproducibility\n    random.seed(random_seed)\n\n    # Convert epoch time to datetime\n    start_date = datetime.fromtimestamp(epoch_milliseconds / 1000)\n\n    # Get current date\n    end_date = datetime.now()\n\n    # Initialize an empty list to store sales data\n    sales_data = []\n\n    # Generate sales data for each day between start_date and end_date\n    current_date = start_date\n    while current_date <= end_date:\n        for product in products:\n            sales = random.randint(10, 50)\n            sales_data.append({'Product': product, 'Date': current_date, 'Sales': sales})\n        current_date += timedelta(days=1)\n\n    # Create a DataFrame from the sales data\n    df = pd.DataFrame(sales_data)\n\n    return df"}
{"task_id": "BigCodeBench/501", "solution": "import xlwt\nimport os\nimport pandas as pd\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is a string\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Try to parse the JSON string\n    try:\n        data = pd.read_json(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON string\")\n    \n    # Create a temporary Excel file\n    temp_filename = \"temp.xlsx\"\n    data.to_excel(temp_filename, sheet_name=sheet_name, index=False)\n    \n    # Move the temporary file to the specified filename\n    os.rename(temp_filename, filename)\n    \n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)", "raw_solution": "import xlwt\nimport os\nimport pandas as pd\n\ndef task_func(json_str, filename, sheet_name=\"sheet1\"):\n    # Check if json_str is a string\n    if not isinstance(json_str, (str, bytes, bytearray)):\n        raise TypeError(\"json_str must be a string, bytes, or bytearray\")\n    \n    # Try to parse the JSON string\n    try:\n        data = pd.read_json(json_str)\n    except ValueError:\n        raise ValueError(\"Invalid JSON string\")\n    \n    # Create a temporary Excel file\n    temp_filename = \"temp.xlsx\"\n    data.to_excel(temp_filename, sheet_name=sheet_name, index=False)\n    \n    # Move the temporary file to the specified filename\n    os.rename(temp_filename, filename)\n    \n    # Return the absolute path of the created Excel file\n    return os.path.abspath(filename)"}
{"task_id": "BigCodeBench/502", "solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create an empty list to store the data\n    data = []\n    \n    # Loop through the number of days in the past\n    for i in range(days_in_past):\n        # Calculate the date for the current day\n        date = datetime.now() - timedelta(days=i)\n        \n        # Loop through each activity\n        for activity in activities:\n            # Generate a random duration for the current activity\n            duration = random.randint(0, 120)\n            \n            # Append the data to the list\n            data.append([date, activity, duration])\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    # Convert the Date column to datetime\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    \n    # Set the Date column as the index\n    df.set_index(\"Date\", inplace=True)\n    \n    # Create a lineplot using Seaborn\n    ax = sns.lineplot(data=df, x=df.index, y=\"Duration\", hue=\"Activity\")\n    \n    # Return the ax and df objects\n    return ax, df", "raw_solution": "from datetime import datetime, timedelta\nimport pandas as pd\nimport random\nimport seaborn as sns\n\ndef task_func(days_in_past=7, random_seed=0):\n    # Set the random seed for reproducibility\n    random.seed(random_seed)\n    \n    # Define the list of activities\n    activities = [\"Running\", \"Swimming\", \"Cycling\", \"Yoga\", \"Weight Training\"]\n    \n    # Create an empty list to store the data\n    data = []\n    \n    # Loop through the number of days in the past\n    for i in range(days_in_past):\n        # Calculate the date for the current day\n        date = datetime.now() - timedelta(days=i)\n        \n        # Loop through each activity\n        for activity in activities:\n            # Generate a random duration for the current activity\n            duration = random.randint(0, 120)\n            \n            # Append the data to the list\n            data.append([date, activity, duration])\n    \n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=[\"Date\", \"Activity\", \"Duration\"])\n    \n    # Convert the Date column to datetime\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    \n    # Set the Date column as the index\n    df.set_index(\"Date\", inplace=True)\n    \n    # Create a lineplot using Seaborn\n    ax = sns.lineplot(data=df, x=df.index, y=\"Duration\", hue=\"Activity\")\n    \n    # Return the ax and df objects\n    return ax, df"}
{"task_id": "BigCodeBench/503", "solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generates a DataFrame of random stock prices for a specified number of days in the past.\n\n    Parameters:\n    days_in_past (int): The number of days in the past for which to generate stock prices.\n    stock_names (list): A list of stock names to include in the DataFrame.\n    random_seed (int): The seed for the random number generator for reproducibility.\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a date range for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Create a DataFrame with random stock prices\n    stock_prices = pd.DataFrame(index=date_range, columns=stock_names)\n    for stock in stock_names:\n        stock_prices[stock] = np.random.rand(days_in_past)\n    \n    return stock_prices", "raw_solution": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    days_in_past=7, stock_names=[\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"FB\"], random_seed=0\n):\n    \"\"\"\n    Generates a DataFrame of random stock prices for a specified number of days in the past.\n\n    Parameters:\n    days_in_past (int): The number of days in the past for which to generate stock prices.\n    stock_names (list): A list of stock names to include in the DataFrame.\n    random_seed (int): The seed for the random number generator for reproducibility.\n\n    Returns:\n    pandas.DataFrame: A DataFrame containing random stock prices for the specified number of days.\n    \"\"\"\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Create a date range for the specified number of days in the past\n    end_date = datetime.now()\n    start_date = end_date - timedelta(days=days_in_past)\n    date_range = pd.date_range(start=start_date, end=end_date)\n    \n    # Create a DataFrame with random stock prices\n    stock_prices = pd.DataFrame(index=date_range, columns=stock_names)\n    for stock in stock_names:\n        stock_prices[stock] = np.random.rand(days_in_past)\n    \n    return stock_prices\n\n# Example usage:\n# stock_df = task_func(days_in_past=10, stock_names=[\"AAPL\", \"GOOGL\"], random_seed=42)\n# print(stock_df)"}
{"task_id": "BigCodeBench/509", "solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, mode='r', newline='', encoding='utf-8') as file1, \\\n             open(file_path2, mode='r', newline='', encoding='utf-8') as file2:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Read all lines from both files\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            \n            # Check if files are empty\n            if not lines1:\n                raise ValueError(\"File 1 is empty\")\n            if not lines2:\n                raise ValueError(\"File 2 is empty\")\n            \n            # Compare lines and create a difference report\n            differences = []\n            max_length = max(len(lines1), len(lines2))\n            for i in range(max_length):\n                line1 = lines1[i] if i < len(lines1) else []\n                line2 = lines2[i] if i < len(lines2) else []\n                diff = list(ndiff(['\\t' + ','.join(line1)], ['\\t' + ','.join(line2)]))\n                for line in diff:\n                    if line.startswith('- '):\n                        differences.append({'Line Number': i + 1, 'Status': '-', 'Content': line[2:]})\n                    elif line.startswith('+ '):\n                        differences.append({'Line Number': i + 1, 'Status': '+', 'Content': line[2:]})\n                    elif line.startswith('  '):\n                        differences.append({'Line Number': i + 1, 'Status': ' ', 'Content': line[2:]})\n            \n            # Create a DataFrame from the differences\n            df = pd.DataFrame(differences)\n            return df\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files were not found.\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")", "raw_solution": "import pandas as pd\nimport csv\nfrom difflib import ndiff\n\ndef task_func(file_path1, file_path2, delimiter=',', quotechar='\"'):\n    try:\n        with open(file_path1, mode='r', newline='', encoding='utf-8') as file1, \\\n             open(file_path2, mode='r', newline='', encoding='utf-8') as file2:\n            reader1 = csv.reader(file1, delimiter=delimiter, quotechar=quotechar)\n            reader2 = csv.reader(file2, delimiter=delimiter, quotechar=quotechar)\n            \n            # Read all lines from both files\n            lines1 = list(reader1)\n            lines2 = list(reader2)\n            \n            # Check if files are empty\n            if not lines1:\n                raise ValueError(\"File 1 is empty\")\n            if not lines2:\n                raise ValueError(\"File 2 is empty\")\n            \n            # Compare lines and create a difference report\n            differences = []\n            max_length = max(len(lines1), len(lines2))\n            for i in range(max_length):\n                line1 = lines1[i] if i < len(lines1) else []\n                line2 = lines2[i] if i < len(lines2) else []\n                diff = list(ndiff(['\\t' + ','.join(line1)], ['\\t' + ','.join(line2)]))\n                for line in diff:\n                    if line.startswith('- '):\n                        differences.append({'Line Number': i + 1, 'Status': '-', 'Content': line[2:]})\n                    elif line.startswith('+ '):\n                        differences.append({'Line Number': i + 1, 'Status': '+', 'Content': line[2:]})\n                    elif line.startswith('  '):\n                        differences.append({'Line Number': i + 1, 'Status': ' ', 'Content': line[2:]})\n            \n            # Create a DataFrame from the differences\n            df = pd.DataFrame(differences)\n            return df\n    \n    except FileNotFoundError:\n        raise FileNotFoundError(\"One or both of the files were not found.\")\n    except Exception as e:\n        raise Exception(f\"An error occurred: {e}\")\n\n# Example usage:\n# df = task_func('file1.csv', 'file2.csv')\n# print(df)"}
{"task_id": "BigCodeBench/511", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in the data.\")\n    \n    # Calculate statistics\n    sum_value = df[column].sum()\n    mean_value = df[column].mean()\n    min_value = df[column].min()\n    max_value = df[column].max()\n    \n    # Create a dictionary with the statistics\n    stats = {\n        'sum': sum_value,\n        'mean': mean_value,\n        'min': min_value,\n        'max': max_value\n    }\n    \n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(df[column], labels=df[column], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    return stats, ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Convert data to DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the column exists in the DataFrame\n    if column not in df.columns:\n        raise ValueError(f\"Column '{column}' not found in the data.\")\n    \n    # Calculate statistics\n    sum_value = df[column].sum()\n    mean_value = df[column].mean()\n    min_value = df[column].min()\n    max_value = df[column].max()\n    \n    # Create a dictionary with the statistics\n    stats = {\n        'sum': sum_value,\n        'mean': mean_value,\n        'min': min_value,\n        'max': max_value\n    }\n    \n    # Create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(df[column], labels=df[column], autopct='%1.1f%%')\n    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    return stats, ax"}
{"task_id": "BigCodeBench/513", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(column, data):\n    # Check if the data list is empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    \n    # Convert the data list to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the specified column is valid\n    if column not in df.columns:\n        raise KeyError(\"Invalid column name\")\n    \n    # Check if the numeric values for steps, calories burned, and distance walked are non-negative\n    if column in ['steps', 'calories_burned', 'distance_walked']:\n        if (df[column] < 0).any():\n            raise ValueError(\"Numeric values for steps, calories burned, and distance walked must be non-negative\")\n    \n    # Calculate the sum, mean, min, max of the column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Draw a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column], marker='o')\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    \n    return (stats, ax)", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(column, data):\n    # Check if the data list is empty\n    if not data:\n        raise ValueError(\"Data list is empty\")\n    \n    # Convert the data list to a pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Check if the specified column is valid\n    if column not in df.columns:\n        raise KeyError(\"Invalid column name\")\n    \n    # Check if the numeric values for steps, calories burned, and distance walked are non-negative\n    if column in ['steps', 'calories_burned', 'distance_walked']:\n        if (df[column] < 0).any():\n            raise ValueError(\"Numeric values for steps, calories burned, and distance walked must be non-negative\")\n    \n    # Calculate the sum, mean, min, max of the column\n    stats = {\n        'sum': df[column].sum(),\n        'mean': df[column].mean(),\n        'min': df[column].min(),\n        'max': df[column].max()\n    }\n    \n    # Draw a line chart\n    fig, ax = plt.subplots()\n    ax.plot(df['Date'], df[column], marker='o')\n    ax.set_title(f'Line Chart of {column}')\n    ax.set_xlabel('Date')\n    ax.set_ylabel(column)\n    \n    return (stats, ax)"}
{"task_id": "BigCodeBench/526", "solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\ndef task_func(input_file=\"data.json\"):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a defaultdict to store the sums and counts for each key\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                sums[key] += value\n                counts[key] += 1\n    \n    # Calculate mean and median for each key\n    means = {key: sums[key] / counts[key] if counts[key] > 0 else np.nan for key in sums}\n    medians = {key: np.median([item[key] for item in data if isinstance(item[key], (int, float))]) for key in sums}\n    \n    # Create a DataFrame from the results\n    df = pd.DataFrame({'mean': means, 'median': medians})\n    \n    # Sort the DataFrame by the index (variable names)\n    df = df.sort_index()\n    \n    return df", "raw_solution": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef task_func(input_file=\"data.json\"):\n    # Read the JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize a defaultdict to store the sums and counts for each key\n    sums = defaultdict(float)\n    counts = defaultdict(int)\n    \n    # Iterate over each dictionary in the list\n    for item in data:\n        for key, value in item.items():\n            # Check if the value is numeric\n            if isinstance(value, (int, float)):\n                sums[key] += value\n                counts[key] += 1\n    \n    # Calculate mean and median for each key\n    means = {key: sums[key] / counts[key] if counts[key] > 0 else np.nan for key in sums}\n    medians = {key: np.median([item[key] for item in data if isinstance(item[key], (int, float))]) for key in sums}\n    \n    # Create a DataFrame from the results\n    df = pd.DataFrame({'mean': means, 'median': medians})\n    \n    # Sort the DataFrame by the index (variable names)\n    df = df.sort_index()\n    \n    return df"}
{"task_id": "BigCodeBench/528", "solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must be in CSV format.\")\n\n    # Read the CSV file and store rows in a list\n    with open(file_path, mode='r', newline='') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    # Count occurrences of each row\n    row_counts = Counter(tuple(row) for row in rows)\n\n    # Filter out rows that occur only once\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to a pandas DataFrame\n    df = pd.DataFrame(list(duplicates.keys()), columns=[f'Column_{i+1}' for i in range(len(rows[0]))])\n\n    # Plotting the bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(duplicates.keys(), duplicates.values(), color='blue')\n    plt.xlabel('Duplicate Rows')\n    plt.ylabel('Count')\n    plt.title('Duplicate Rows in CSV File')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return duplicates, plt.gca()", "raw_solution": "import csv\nfrom collections import Counter\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef task_func(file_path):\n    # Check if the file has a .csv extension\n    if not file_path.endswith('.csv'):\n        raise ValueError(\"The file must be in CSV format.\")\n\n    # Read the CSV file and store rows in a list\n    with open(file_path, mode='r', newline='') as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    # Count occurrences of each row\n    row_counts = Counter(tuple(row) for row in rows)\n\n    # Filter out rows that occur only once\n    duplicates = {row: count for row, count in row_counts.items() if count > 1}\n\n    # Convert duplicates to a pandas DataFrame\n    df = pd.DataFrame(list(duplicates.keys()), columns=[f'Column_{i+1}' for i in range(len(rows[0]))])\n\n    # Plotting the bar chart\n    plt.figure(figsize=(10, 6))\n    plt.bar(duplicates.keys(), duplicates.values(), color='blue')\n    plt.xlabel('Duplicate Rows')\n    plt.ylabel('Count')\n    plt.title('Duplicate Rows in CSV File')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    return duplicates, plt.gca()"}
{"task_id": "BigCodeBench/530", "solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Round down ages to the nearest integer and ensure they are non-negative\n    df['age'] = df['age'].apply(lambda x: int(np.floor(x)) if x >= 0 else np.nan)\n    if df['age'].isnull().any():\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated('name', keep=False)]\n    \n    # If there are no duplicates, return an empty Counter and None for the plot\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Record the age distribution for duplicate names\n    age_distribution = Counter(duplicates['age'])\n    \n    # Create a histogram plot for the age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicates['age'], bins=np.arange(min(duplicates['age']), max(duplicates['age']) + 1.5) - 0.5, kde=False)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution Among Duplicate Names')\n    \n    return age_distribution, plt.gca()", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef task_func(df: pd.DataFrame) -> (Counter, plt.Axes):\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"DataFrame is empty\")\n    \n    # Round down ages to the nearest integer and ensure they are non-negative\n    df['age'] = df['age'].apply(lambda x: int(np.floor(x)) if x >= 0 else np.nan)\n    if df['age'].isnull().any():\n        raise ValueError(\"Age cannot be negative\")\n    \n    # Identify duplicate names\n    duplicates = df[df.duplicated('name', keep=False)]\n    \n    # If there are no duplicates, return an empty Counter and None for the plot\n    if duplicates.empty:\n        return Counter(), None\n    \n    # Record the age distribution for duplicate names\n    age_distribution = Counter(duplicates['age'])\n    \n    # Create a histogram plot for the age distribution\n    plt.figure(figsize=(10, 6))\n    sns.histplot(duplicates['age'], bins=np.arange(min(duplicates['age']), max(duplicates['age']) + 1.5) - 0.5, kde=False)\n    plt.xlabel('Age')\n    plt.ylabel('Count')\n    plt.title('Age Distribution Among Duplicate Names')\n    \n    return age_distribution, plt.gca()"}
{"task_id": "BigCodeBench/532", "solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndef task_func(df, bins=4):\n    # Count duplicates in the 'value' column\n    duplicates = df['value'].value_counts()\n    duplicates = duplicates[duplicates > 1]\n    \n    # Plot histogram and normal distribution curve\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, np.mean(df['value']), np.std(df['value']))\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return (Counter(duplicates), ax)", "raw_solution": "import numpy as np\nfrom collections import Counter\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef task_func(df, bins=4):\n    # Count duplicates in the 'value' column\n    duplicates = df['value'].value_counts()\n    duplicates = duplicates[duplicates > 1]\n    \n    # Plot histogram and normal distribution curve\n    fig, ax = plt.subplots()\n    n, bins, patches = ax.hist(df['value'], bins=bins, color='green', alpha=0.6)\n    xmin, xmax = ax.get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, np.mean(df['value']), np.std(df['value']))\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set plot title and labels\n    ax.set_title('Distribution')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    \n    return (Counter(duplicates), ax)"}
{"task_id": "BigCodeBench/553", "solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    \n    Parameters:\n    a (list): List of row indices for the DataFrame.\n    b (list): List of values to be used for generating random values in the DataFrame.\n    \n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Set the title and labels\n    ax.set_title('Random Values Bar Chart')\n    ax.set_xlabel('Row Indices')\n    ax.set_ylabel('Random Values')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\na = ['row1', 'row2', 'row3']\nb = [3, 2, 4]", "raw_solution": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Constants\nCOLUMNS = ['A', 'B', 'C', 'D', 'E']\n\ndef task_func(a, b):\n    \"\"\"\n    Generates a pandas DataFrame with random values based on lists 'a' and 'b', and plots it as a bar chart.\n    \n    Parameters:\n    a (list): List of row indices for the DataFrame.\n    b (list): List of values to be used for generating random values in the DataFrame.\n    \n    Returns:\n    matplotlib.axes.Axes: The Axes object of the plotted bar chart.\n    \"\"\"\n    # Create a DataFrame with random values\n    df = pd.DataFrame(np.random.rand(len(a), len(b)), index=a, columns=COLUMNS[:len(b)])\n    \n    # Plot the DataFrame as a bar chart\n    ax = df.plot(kind='bar', figsize=(10, 6))\n    \n    # Set the title and labels\n    ax.set_title('Random Values Bar Chart')\n    ax.set_xlabel('Row Indices')\n    ax.set_ylabel('Random Values')\n    \n    # Show the plot\n    plt.show()\n    \n    return ax\n\n# Example usage\na = ['row1', 'row2', 'row3']\nb = [3, 2, 4]\nax = task_func(a, b)"}
{"task_id": "BigCodeBench/560", "solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with columns 'month' and 'value'.\n\n    Returns:\n    matplotlib.axes.Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure the 'month' column is of datetime type\n    data['month'] = pd.to_datetime(data['month'], format='%B')\n    \n    # Set the year to 2023 for the plot title\n    year = 2023\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(data['month'], data['value'], color='blue')\n    \n    # Set the title, x-label, and y-label\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotate x-axis labels for better readability\n    ax.set_xticklabels(data['month'].dt.strftime('%B'), rotation=45)\n    \n    return ax", "raw_solution": "from datetime import datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef task_func(data):\n    \"\"\"\n    Plots a bar chart of monthly data values for a single year.\n\n    Parameters:\n    data (pd.DataFrame): A pandas DataFrame with columns 'month' and 'value'.\n\n    Returns:\n    matplotlib.axes.Axes: A matplotlib.axes.Axes object representing the plot.\n    \"\"\"\n    # Ensure the 'month' column is of datetime type\n    data['month'] = pd.to_datetime(data['month'], format='%B')\n    \n    # Set the year to 2023 for the plot title\n    year = 2023\n    \n    # Create the bar chart\n    fig, ax = plt.subplots()\n    ax.bar(data['month'], data['value'], color='blue')\n    \n    # Set the title, x-label, and y-label\n    ax.set_title(f'Monthly Data for {year}')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('Value')\n    \n    # Rotate x-axis labels for better readability\n    ax.set_xticklabels(data['month'].dt.strftime('%B'), rotation=45)\n    \n    return ax"}
{"task_id": "BigCodeBench/567", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef task_func(data):\n    # Convert the input string to a pandas Series\n    data_series = pd.Series(data.split(','))\n    \n    # Convert the Series to numeric values, coercing errors to NaN\n    data_numeric = pd.to_numeric(data_series, errors='coerce')\n    \n    # Filter out NaN values\n    data_filtered = data_numeric.dropna()\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(data_filtered.min(), data_filtered.max() + 2) - 0.5\n    \n    # Create the histogram\n    ax = data_filtered.hist(bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef task_func(data):\n    # Convert the input string to a pandas Series\n    data_series = pd.Series(data.split(','))\n    \n    # Convert the Series to numeric values, coercing errors to NaN\n    data_numeric = pd.to_numeric(data_series, errors='coerce')\n    \n    # Filter out NaN values\n    data_filtered = data_numeric.dropna()\n    \n    # Calculate the bins for the histogram\n    bins = np.arange(data_filtered.min(), data_filtered.max() + 2) - 0.5\n    \n    # Create the histogram\n    ax = data_filtered.hist(bins=bins, edgecolor='black')\n    \n    # Set the labels and title\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Histogram of Values')\n    \n    # Return the Axes object\n    return ax"}
{"task_id": "BigCodeBench/574", "solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a time array\n    t = np.linspace(0, 1, array_length)\n    \n    # Generate a noisy sine wave\n    sine_wave = np.sin(2 * np.pi * t) + noise_level * np.random.normal(size=array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(2 * np.pi * frequency * x + phase) + offset\n    \n    # Initial guess for the parameters\n    p0 = [1, 1, 0, 0]\n    \n    # Perform curve fitting\n    params, covariance = curve_fit(sine_func, t, sine_wave, p0=p0)\n    \n    # Extract the fitted parameters\n    amplitude, frequency, phase, offset = params\n    \n    # Generate the fitted sine wave\n    fitted_sine_wave = sine_func(t, amplitude, frequency, phase, offset)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Noisy Sine Wave')\n    ax.plot(t, fitted_sine_wave, label='Fitted Curve', color='red')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    \n    return ax", "raw_solution": "from scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef task_func(array_length=100, noise_level=0.2):\n    # Generate a time array\n    t = np.linspace(0, 1, array_length)\n    \n    # Generate a noisy sine wave\n    sine_wave = np.sin(2 * np.pi * t) + noise_level * np.random.normal(size=array_length)\n    \n    # Define the sine function to fit\n    def sine_func(x, amplitude, frequency, phase, offset):\n        return amplitude * np.sin(2 * np.pi * frequency * x + phase) + offset\n    \n    # Initial guess for the parameters\n    p0 = [1, 1, 0, 0]\n    \n    # Perform curve fitting\n    params, covariance = curve_fit(sine_func, t, sine_wave, p0=p0)\n    \n    # Extract the fitted parameters\n    amplitude, frequency, phase, offset = params\n    \n    # Generate the fitted sine wave\n    fitted_sine_wave = sine_func(t, amplitude, frequency, phase, offset)\n    \n    # Create a plot\n    fig, ax = plt.subplots()\n    ax.plot(t, sine_wave, label='Noisy Sine Wave')\n    ax.plot(t, fitted_sine_wave, label='Fitted Curve', color='red')\n    ax.legend()\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Noisy Sine Wave and Fitted Curve')\n    \n    return ax"}
{"task_id": "BigCodeBench/579", "solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef normalize_text(text):\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\ndef task_func(csv_file):\n    try:\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = ' '.join([normalize_text(row[0]) for row in reader])\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The CSV file cannot be found at the specified path.\")\n    except IOError:\n        raise IOError(\"An error occurred while reading the file.\")\n\n    words = text_data.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    fig, ax = plt.subplots()\n    ax.bar([word for word, count in most_common_words], [count for word, count in most_common_words])\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return ax, most_common_words", "raw_solution": "import unicodedata\nimport csv\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef normalize_text(text):\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n\ndef task_func(csv_file):\n    try:\n        with open(csv_file, mode='r', encoding='utf-8') as file:\n            reader = csv.reader(file)\n            text_data = ' '.join([normalize_text(row[0]) for row in reader])\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The CSV file cannot be found at the specified path.\")\n    except IOError:\n        raise IOError(\"An error occurred while reading the file.\")\n\n    words = text_data.split()\n    word_counts = Counter(words)\n    most_common_words = word_counts.most_common(10)\n\n    fig, ax = plt.subplots()\n    ax.bar([word for word, count in most_common_words], [count for word, count in most_common_words])\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequencies')\n    ax.set_title('10 Most Common Words')\n\n    return ax, most_common_words"}
{"task_id": "BigCodeBench/582", "solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure object\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the probability density function (PDF) of the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, 1)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set the title and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability')\n    \n    return fig", "raw_solution": "import numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\ndef task_func(size=1000):\n    # Generate normally distributed random numbers\n    data = np.random.normal(loc=0, scale=1, size=size)\n    \n    # Create a figure object\n    fig, ax = plt.subplots()\n    \n    # Plot the histogram of the data\n    ax.hist(data, bins=30, density=True, alpha=0.6, color='g')\n    \n    # Plot the probability density function (PDF) of the normal distribution\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = stats.norm.pdf(x, 0, 1)\n    ax.plot(x, p, 'k', linewidth=2)\n    \n    # Set the title and labels\n    ax.set_title('Histogram and PDF of Normally Distributed Random Numbers')\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Probability')\n    \n    return fig"}
{"task_id": "BigCodeBench/583", "solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n    \n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Generate a random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n    \n    # Encrypt the private key using AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce)\n    encrypted_private_key = cipher.encrypt(private_key.save_pkcs1())\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'wb') as f:\n        f.write(b64encode(encrypted_private_key))\n    \n    # Return the public key, filename, password, and nonce\n    return public_key, filename, password, nonce", "raw_solution": "import rsa\nimport os\nfrom Crypto.Random import get_random_bytes\nfrom Crypto.Cipher import AES\nfrom base64 import b64encode, b64decode\n\ndef task_func():\n    # Generate RSA key pair\n    public_key, private_key = rsa.newkeys(2048)\n    \n    # Generate a random filename\n    random_bytes = get_random_bytes(8)\n    filename = f\"private_key_{random_bytes.hex()}.txt\"\n    \n    # Generate a random password and nonce for AES encryption\n    password = get_random_bytes(16)\n    nonce = get_random_bytes(16)\n    \n    # Encrypt the private key using AES\n    cipher = AES.new(password, AES.MODE_GCM, nonce)\n    encrypted_private_key = cipher.encrypt(private_key.save_pkcs1())\n    \n    # Save the encrypted private key to a file\n    with open(filename, 'wb') as f:\n        f.write(b64encode(encrypted_private_key))\n    \n    # Return the public key, filename, password, and nonce\n    return public_key, filename, password, nonce"}
{"task_id": "BigCodeBench/587", "solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)\n    \n    # Encrypt the file data using AES\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n    cipher = Cipher(algorithms.AES(aes_key), modes.CFB(aes_key), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_file_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Save the encrypted file and the encrypted AES key to new files\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(encrypted_file_data)\n    \n    encrypted_aes_key_path = file_path + '.aes.enc'\n    with open(encrypted_aes_key_path, 'wb') as file:\n        file.write(encrypted_aes_key)\n    \n    return public_key, encrypted_file_path, encrypted_aes_key_path", "raw_solution": "import rsa\nimport os\nfrom cryptography.hazmat.backends import default_backend\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.primitives import padding\nfrom base64 import b64encode\n\ndef task_func(file_path):\n    # Generate RSA public and private keys\n    (public_key, private_key) = rsa.newkeys(2048)\n    \n    # Read the file to be encrypted\n    with open(file_path, 'rb') as file:\n        file_data = file.read()\n    \n    # Generate a random AES key\n    aes_key = os.urandom(32)\n    \n    # Encrypt the file data using AES\n    padder = padding.PKCS7(algorithms.AES.block_size).padder()\n    padded_data = padder.update(file_data) + padder.finalize()\n    cipher = Cipher(algorithms.AES(aes_key), modes.CFB(aes_key), backend=default_backend())\n    encryptor = cipher.encryptor()\n    encrypted_file_data = encryptor.update(padded_data) + encryptor.finalize()\n    \n    # Encrypt the AES key with the RSA public key\n    encrypted_aes_key = rsa.encrypt(aes_key, public_key)\n    \n    # Save the encrypted file and the encrypted AES key to new files\n    encrypted_file_path = file_path + '.enc'\n    with open(encrypted_file_path, 'wb') as file:\n        file.write(encrypted_file_data)\n    \n    encrypted_aes_key_path = file_path + '.aes.enc'\n    with open(encrypted_aes_key_path, 'wb') as file:\n        file.write(encrypted_aes_key)\n    \n    return public_key, encrypted_file_path, encrypted_aes_key_path"}
{"task_id": "BigCodeBench/590", "solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    \n    try:\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except urllib.error.URLError as e:\n        raise URLError(\"Network error or server issue: \" + str(e.reason))\n    \n    doc = pq(html_content)\n    anchor_tags = doc('a')\n    \n    data = []\n    for tag in anchor_tags:\n        text = pq(tag).text()\n        href = pq(tag).attr('href')\n        data.append({'text': text, 'href': href})\n    \n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame(data)\n    df['fetch_time'] = fetch_time\n    \n    return df", "raw_solution": "import urllib.request\nfrom pyquery import PyQuery as pq\nfrom datetime import datetime\nimport pandas as pd\n\ndef task_func(url):\n    if not url:\n        raise ValueError(\"URL cannot be empty\")\n    \n    try:\n        response = urllib.request.urlopen(url)\n        html_content = response.read()\n    except urllib.error.URLError as e:\n        raise URLError(\"Network error or server issue: \" + str(e.reason))\n    \n    doc = pq(html_content)\n    anchor_tags = doc('a')\n    \n    data = []\n    for tag in anchor_tags:\n        text = pq(tag).text()\n        href = pq(tag).attr('href')\n        data.append({'text': text, 'href': href})\n    \n    fetch_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    df = pd.DataFrame(data)\n    df['fetch_time'] = fetch_time\n    \n    return df"}
{"task_id": "BigCodeBench/592", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the output file path\n    output_file = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Open the CSV file for writing\n    with open(output_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header row\n        writer.writerow(['Time'] + SENSORS)\n        \n        # Generate and write data for the specified number of hours\n        current_time = datetime.now()\n        for _ in range(hours * 60):  # Each minute of data\n            row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n            for sensor in SENSORS:\n                if sensor == 'Temperature':\n                    row.append(randint(20, 35))  # Random temperature between 20 and 35 degrees Celsius\n                elif sensor == 'Humidity':\n                    row.append(randint(30, 60))  # Random humidity between 30 and 60%\n                elif sensor == 'Pressure':\n                    row.append(randint(1000, 1020))  # Random pressure between 1000 and 1020 hPa\n            writer.writerow(row)\n            current_time += timedelta(minutes=1)\n\n    return hours", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nSENSORS = ['Temperature', 'Humidity', 'Pressure']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Define the output file path\n    output_file = os.path.join(output_dir, 'sensor_data.csv')\n\n    # Open the CSV file for writing\n    with open(output_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header row\n        writer.writerow(['Time'] + SENSORS)\n        \n        # Generate and write data for the specified number of hours\n        current_time = datetime.now()\n        for _ in range(hours * 60):  # Each minute of data\n            row = [current_time.strftime('%Y-%m-%d %H:%M:%S')]\n            for sensor in SENSORS:\n                if sensor == 'Temperature':\n                    row.append(randint(20, 35))  # Random temperature between 20 and 35 degrees Celsius\n                elif sensor == 'Humidity':\n                    row.append(randint(30, 60))  # Random humidity between 30 and 60%\n                elif sensor == 'Pressure':\n                    row.append(randint(1000, 1020))  # Random pressure between 1000 and 1020 hPa\n            writer.writerow(row)\n            current_time += timedelta(minutes=1)\n\n    return hours\n\n# Example usage:\n# task_func(24)  # Generates 24 hours of sensor data and saves it to 'output/sensor_data.csv'"}
{"task_id": "BigCodeBench/593", "solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = {'Time': [], 'Car': [], 'Bus': [], 'Truck': [], 'Bike': []}\n    current_time = datetime.now()\n    for _ in range(hours):\n        data['Time'].append(current_time.strftime('%H:%M'))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))  # Random count between 0 and 100\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Plot the data\n    df = pd.DataFrame(data)\n    ax = df.plot(kind='line', x='Time', y=VEHICLE_TYPES, marker='o')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file_path, ax", "raw_solution": "import csv\nimport os\nfrom datetime import datetime, timedelta\nfrom random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Constants\nVEHICLE_TYPES = ['Car', 'Bus', 'Truck', 'Bike']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate traffic data\n    data = {'Time': [], 'Car': [], 'Bus': [], 'Truck': [], 'Bike': []}\n    current_time = datetime.now()\n    for _ in range(hours):\n        data['Time'].append(current_time.strftime('%H:%M'))\n        for vehicle in VEHICLE_TYPES:\n            data[vehicle].append(randint(0, 100))  # Random count between 0 and 100\n        current_time += timedelta(hours=1)\n\n    # Save data to CSV\n    csv_file_path = os.path.join(output_dir, 'traffic_data.csv')\n    with open(csv_file_path, mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=data.keys())\n        writer.writeheader()\n        writer.writerows(data)\n\n    # Plot the data\n    df = pd.DataFrame(data)\n    ax = df.plot(kind='line', x='Time', y=VEHICLE_TYPES, marker='o')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Vehicle Count')\n    ax.set_title('Traffic Data Over Time')\n\n    return csv_file_path, ax"}
{"task_id": "BigCodeBench/594", "solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate the current time\n    current_time = datetime.now()\n\n    # Define the path for the output CSV file\n    output_file_path = os.path.join(output_dir, 'weather_data.csv')\n\n    # Open the CSV file for writing\n    with open(output_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time', 'Condition'])\n\n        # Generate weather data for the specified number of hours\n        for _ in range(hours):\n            # Write the current time and a random weather condition\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S'), WEATHER_CONDITIONS[randint(0, 4)]])\n            # Increment the time by one hour\n            current_time += timedelta(hours=1)\n\n    # Define the path for the backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n\n    # Create the backup directory if it doesn't exist\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Define the path for the backup file\n    backup_file_path = os.path.join(backup_dir, 'weather_data_backup.csv')\n\n    # Copy the original file to the backup directory\n    shutil.copy(output_file_path, backup_file_path)\n\n    # Return the path of the generated CSV file\n    return output_file_path", "raw_solution": "import csv\nimport os\nimport shutil\nfrom datetime import datetime, timedelta\nfrom random import randint\n\n# Constants\nWEATHER_CONDITIONS = ['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Stormy']\nOUTPUT_DIR = './output'\n\ndef task_func(hours, output_dir=OUTPUT_DIR):\n    # Create the output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate the current time\n    current_time = datetime.now()\n\n    # Define the path for the output CSV file\n    output_file_path = os.path.join(output_dir, 'weather_data.csv')\n\n    # Open the CSV file for writing\n    with open(output_file_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header row\n        writer.writerow(['Time', 'Condition'])\n\n        # Generate weather data for the specified number of hours\n        for _ in range(hours):\n            # Write the current time and a random weather condition\n            writer.writerow([current_time.strftime('%Y-%m-%d %H:%M:%S'), WEATHER_CONDITIONS[randint(0, 4)]])\n            # Increment the time by one hour\n            current_time += timedelta(hours=1)\n\n    # Define the path for the backup directory\n    backup_dir = os.path.join(output_dir, 'backup')\n\n    # Create the backup directory if it doesn't exist\n    if not os.path.exists(backup_dir):\n        os.makedirs(backup_dir)\n\n    # Define the path for the backup file\n    backup_file_path = os.path.join(backup_dir, 'weather_data_backup.csv')\n\n    # Copy the original file to the backup directory\n    shutil.copy(output_file_path, backup_file_path)\n\n    # Return the path of the generated CSV file\n    return output_file_path"}
{"task_id": "BigCodeBench/618", "solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results and plots for goals and penalty costs.\n\n    Parameters:\n    goals (int): Number of goals to generate.\n    penalties (int): Number of penalties to generate.\n\n    Returns:\n    pd.DataFrame: A dataframe containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate match results\n    data = {\n        'Team': [TEAMS[randint(0, len(TEAMS) - 1)] for _ in range(goals)],\n        'Goals': [randint(0, 5) for _ in range(goals)],\n        'Penalty Cost': [randint(0, 10) * PENALTY_COST for _ in range(penalties)]\n    }\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n\n    return df, axes.tolist()", "raw_solution": "from random import randint\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Constants\nTEAMS = ['Team A', 'Team B', 'Team C', 'Team D', 'Team E']\nPENALTY_COST = 1000  # in dollars\n\ndef task_func(goals, penalties):\n    \"\"\"\n    Generates a DataFrame of football match results and plots for goals and penalty costs.\n\n    Parameters:\n    goals (int): Number of goals to generate.\n    penalties (int): Number of penalties to generate.\n\n    Returns:\n    pd.DataFrame: A dataframe containing match results.\n    list: A list containing two seaborn plot objects (Axes) for goals and penalty costs.\n    \"\"\"\n    # Generate match results\n    data = {\n        'Team': [TEAMS[randint(0, len(TEAMS) - 1)] for _ in range(goals)],\n        'Goals': [randint(0, 5) for _ in range(goals)],\n        'Penalty Cost': [randint(0, 10) * PENALTY_COST for _ in range(penalties)]\n    }\n    df = pd.DataFrame(data)\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    sns.barplot(x='Team', y='Goals', data=df, ax=axes[0])\n    axes[0].set_title('Goals per Team')\n    sns.barplot(x='Team', y='Penalty Cost', data=df, ax=axes[1])\n    axes[1].set_title('Penalty Costs per Team')\n\n    return df, axes.tolist()\n\n# Example usage\ndf, plots = task_func(10, 5)"}
{"task_id": "BigCodeBench/636", "solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n\n    Parameters:\n    rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    tuple: A tuple containing the following elements:\n        DataFrame: The generated DataFrame with random integer values.\n        Axes: The matplotlib Axes object containing the bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.count()\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-zero values')\n    ax.set_title('Non-zero values count per column')\n\n    return df, ax\nrows = 10", "raw_solution": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Constants\nCOLUMNS = ['Column1', 'Column2', 'Column3', 'Column4', 'Column5']\n\ndef task_func(rows):\n    \"\"\"\n    Generates a DataFrame with random integer values between 0 and 9 for a given number of rows.\n    Counts the non-zero values in each column and visualizes this information using a bar plot.\n\n    Parameters:\n    rows (int): The number of rows in the DataFrame.\n\n    Returns:\n    tuple: A tuple containing the following elements:\n        DataFrame: The generated DataFrame with random integer values.\n        Axes: The matplotlib Axes object containing the bar plot.\n    \"\"\"\n    # Generate DataFrame with random integer values\n    data = np.random.randint(0, 10, size=(rows, len(COLUMNS)))\n    df = pd.DataFrame(data, columns=COLUMNS)\n\n    # Count non-zero values in each column\n    non_zero_counts = df.count()\n\n    # Create a bar plot\n    fig, ax = plt.subplots()\n    non_zero_counts.plot(kind='bar', ax=ax)\n    ax.set_xlabel('Columns')\n    ax.set_ylabel('Non-zero values')\n    ax.set_title('Non-zero values count per column')\n\n    return df, ax\n\n# Example usage\nrows = 10\ndf, ax = task_func(rows)\nplt.show()"}
{"task_id": "BigCodeBench/637", "solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ndef task_func(num_students):\n    # Define the courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate random grades for each student in each course\n    grades = {course: np.random.randint(50, 100, num_students) for course in courses}\n    \n    # Create a DataFrame from the grades\n    df = pd.DataFrame(grades)\n    \n    # Calculate the average grade in each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot for the average grades and passing counts\n    fig, ax = plt.subplots()\n    ax.bar(courses, avg_grades, label='Average Grade')\n    ax.bar(courses, passing_counts, bottom=avg_grades, label='Passing Count')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend()\n    \n    # Return the DataFrame and the Axes object\n    return df, ax", "raw_solution": "from random import sample\nfrom typing import Tuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\ndef task_func(num_students):\n    # Define the courses\n    courses = ['Math', 'Science', 'History', 'English', 'Art']\n    \n    # Generate random grades for each student in each course\n    grades = {course: np.random.randint(50, 100, num_students) for course in courses}\n    \n    # Create a DataFrame from the grades\n    df = pd.DataFrame(grades)\n    \n    # Calculate the average grade in each course\n    avg_grades = df.mean()\n    \n    # Calculate the number of students with a passing grade (>= 60)\n    passing_counts = (df >= 60).sum()\n    \n    # Create a bar plot for the average grades and passing counts\n    fig, ax = plt.subplots()\n    ax.bar(courses, avg_grades, label='Average Grade')\n    ax.bar(courses, passing_counts, bottom=avg_grades, label='Passing Count')\n    ax.set_title('Course-wise Average and Passing Grade Counts')\n    ax.legend()\n    \n    # Return the DataFrame and the Axes object\n    return df, ax"}
{"task_id": "BigCodeBench/654", "solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \n    Parameters:\n    array (np.array): A 2D numpy array where the first column contains the indices and the second column contains the values.\n    target_value (int): The target value to match in the first column of the array.\n    \n    Returns:\n    tuple: A tuple containing the optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Filter the array to include only rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    \n    # Extract indices and values for fitting\n    indices = filtered_array[:, 0]\n    values = filtered_array[:, 1]\n    \n    # Define the exponential decay function\n    def exponential_decay(x, a, b):\n        return a * np.exp(-b * x)\n    \n    # Initial guess for the parameters\n    p0 = [1, 0.1]\n    \n    # Perform the curve fitting\n    popt, pcov = optimize.curve_fit(exponential_decay, indices, values, p0=p0)\n    \n    # Create a figure and axis for plotting\n    fig, ax = plt.subplots()\n    \n    # Plot the original data points\n    ax.scatter(indices, values, label='Data')\n    \n    # Plot the fitted curve\n    x_fit = np.linspace(min(indices), max(indices), 100)\n    y_fit = exponential_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n    \n    # Add labels and legend\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return popt, ax", "raw_solution": "import matplotlib.pyplot as plt\nimport scipy.optimize as optimize\nimport numpy as np\n\ndef task_func(array, target_value):\n    \"\"\"\n    Fits an exponential decay function to the indices in the array where the first column matches the target value.\n    \n    Parameters:\n    array (np.array): A 2D numpy array where the first column contains the indices and the second column contains the values.\n    target_value (int): The target value to match in the first column of the array.\n    \n    Returns:\n    tuple: A tuple containing the optimized parameters of the fitting function (popt) and the matplotlib Axes object.\n    \"\"\"\n    # Filter the array to include only rows where the first column matches the target value\n    filtered_array = array[array[:, 0] == target_value]\n    \n    # Extract indices and values for fitting\n    indices = filtered_array[:, 0]\n    values = filtered_array[:, 1]\n    \n    # Define the exponential decay function\n    def exponential_decay(x, a, b):\n        return a * np.exp(-b * x)\n    \n    # Initial guess for the parameters\n    p0 = [1, 0.1]\n    \n    # Perform the curve fitting\n    popt, pcov = optimize.curve_fit(exponential_decay, indices, values, p0=p0)\n    \n    # Create a figure and axis for plotting\n    fig, ax = plt.subplots()\n    \n    # Plot the original data points\n    ax.scatter(indices, values, label='Data')\n    \n    # Plot the fitted curve\n    x_fit = np.linspace(min(indices), max(indices), 100)\n    y_fit = exponential_decay(x_fit, *popt)\n    ax.plot(x_fit, y_fit, 'r-', label='Fitted Curve')\n    \n    # Add labels and legend\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Value')\n    ax.legend()\n    \n    return popt, ax\n\n# Example usage:\n# array = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10]])\n# target_value = 5\n# popt, ax = task_func(array, target_value)\n# plt.show()"}
{"task_id": "BigCodeBench/655", "solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\ndef task_func(texts, num_topics):\n    \"\"\"\n    Extracts topics from a collection of text documents using Non-Negative Matrix Factorization (NMF).\n    \n    Parameters:\n    texts (list of str): A list of text documents.\n    num_topics (int): The number of topics to extract.\n    \n    Returns:\n    list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        # Remove stopwords\n        words = [word for word in cleaned_text.split() if word not in STOPWORDS]\n        processed_texts.append(' '.join(words))\n    \n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(X)\n    \n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract the most significant words for each topic\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words_indices = topic.argsort()[:-11:-1]  # Get the indices of the top 10 words\n        top_words = [feature_names[i] for i in top_words_indices]\n        topics.append(top_words)\n    \n    return topics\ntexts = [\n    \"Python is a great programming language\",\n    \"Machine learning is fascinating\",\n    \"Data science involves data analysis and machine learning\",\n    \"NLP is a subfield of AI that deals with natural language processing\"\n]\nnum_topics = 2", "raw_solution": "import re\nimport nltk\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Ensure nltk's stopwords are downloaded\nnltk.download('stopwords')\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\nSTOPWORDS = nltk.corpus.stopwords.words('english')\n\ndef task_func(texts, num_topics):\n    \"\"\"\n    Extracts topics from a collection of text documents using Non-Negative Matrix Factorization (NMF).\n    \n    Parameters:\n    texts (list of str): A list of text documents.\n    num_topics (int): The number of topics to extract.\n    \n    Returns:\n    list of list of str: A list where each element is a list of words representing a topic.\n    \"\"\"\n    # Preprocess the texts\n    processed_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters (excluding spaces)\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        # Remove stopwords\n        words = [word for word in cleaned_text.split() if word not in STOPWORDS]\n        processed_texts.append(' '.join(words))\n    \n    # Vectorize the processed texts using TF-IDF\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(processed_texts)\n    \n    # Apply NMF to extract topics\n    nmf = NMF(n_components=num_topics, random_state=42)\n    nmf.fit(X)\n    \n    # Get the feature names (words)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Extract the most significant words for each topic\n    topics = []\n    for topic_idx, topic in enumerate(nmf.components_):\n        top_words_indices = topic.argsort()[:-11:-1]  # Get the indices of the top 10 words\n        top_words = [feature_names[i] for i in top_words_indices]\n        topics.append(top_words)\n    \n    return topics\n\n# Example usage\ntexts = [\n    \"Python is a great programming language\",\n    \"Machine learning is fascinating\",\n    \"Data science involves data analysis and machine learning\",\n    \"NLP is a subfield of AI that deals with natural language processing\"\n]\nnum_topics = 2\ntopics = task_func(texts, num_topics)"}
{"task_id": "BigCodeBench/657", "solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \n    Parameters:\n    texts (list of str): List of texts to process.\n    stopwords (list of str, optional): List of stopwords to remove. If None, uses nltk.corpus.stopwords.words('english').\n    \n    Returns:\n    Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    # Download stopwords if not already downloaded\n    if stopwords is None:\n        nltk.download('stopwords')\n        stopwords = nltk.corpus.stopwords.words('english')\n    \n    # Clean and tokenize the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        # Tokenize\n        tokens = cleaned_text.split()\n        # Remove stopwords\n        tokens = [token for token in tokens if token not in stopwords]\n        cleaned_texts.append(tokens)\n    \n    # Train Word2Vec model\n    word2vec_model = Word2Vec(cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return word2vec_model", "raw_solution": "import re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(texts, stopwords=None):\n    \"\"\"\n    Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords.\n    \n    Parameters:\n    texts (list of str): List of texts to process.\n    stopwords (list of str, optional): List of stopwords to remove. If None, uses nltk.corpus.stopwords.words('english').\n    \n    Returns:\n    Word2Vec: A trained Word2Vec model.\n    \"\"\"\n    # Download stopwords if not already downloaded\n    if stopwords is None:\n        nltk.download('stopwords')\n        stopwords = nltk.corpus.stopwords.words('english')\n    \n    # Clean and tokenize the texts\n    cleaned_texts = []\n    for text in texts:\n        # Remove non-alphanumeric characters except space\n        cleaned_text = ALPHANUMERIC.sub(' ', text)\n        # Convert to lowercase\n        cleaned_text = cleaned_text.lower()\n        # Tokenize\n        tokens = cleaned_text.split()\n        # Remove stopwords\n        tokens = [token for token in tokens if token not in stopwords]\n        cleaned_texts.append(tokens)\n    \n    # Train Word2Vec model\n    word2vec_model = Word2Vec(cleaned_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return word2vec_model"}
{"task_id": "BigCodeBench/678", "solution": "import pandas as pd\nimport json\nimport os\nimport shutil\ndef task_func(path):\n    # Initialize an empty list to store DataFrames\n    dfs = []\n    \n    # Ensure the path exists\n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    \n    # Process each JSON file\n    for json_file in sorted(json_files):\n        # Construct the full file path\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert the data to a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Insert a \"Source\" column with the filename\n        df['Source'] = json_file\n        \n        # Append the DataFrame to the list\n        dfs.append(df)\n    \n    # Concatenate all DataFrames into a single DataFrame\n    df = pd.concat(dfs, ignore_index=True)\n    \n    # Create the \"processed\" subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move all JSON files to the \"processed\" subdirectory\n    for json_file in json_files:\n        shutil.move(os.path.join(path, json_file), os.path.join(processed_path, json_file))\n    \n    return df", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport shutil\n\ndef task_func(path):\n    # Initialize an empty list to store DataFrames\n    dfs = []\n    \n    # Ensure the path exists\n    if not os.path.exists(path):\n        os.makedirs(path)\n    \n    # Get a list of all JSON files in the directory\n    json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n    \n    # Process each JSON file\n    for json_file in sorted(json_files):\n        # Construct the full file path\n        file_path = os.path.join(path, json_file)\n        \n        # Read the JSON file\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n        \n        # Convert the data to a DataFrame\n        df = pd.DataFrame(data)\n        \n        # Insert a \"Source\" column with the filename\n        df['Source'] = json_file\n        \n        # Append the DataFrame to the list\n        dfs.append(df)\n    \n    # Concatenate all DataFrames into a single DataFrame\n    df = pd.concat(dfs, ignore_index=True)\n    \n    # Create the \"processed\" subdirectory if it doesn't exist\n    processed_path = os.path.join(path, 'processed')\n    if not os.path.exists(processed_path):\n        os.makedirs(processed_path)\n    \n    # Move all JSON files to the \"processed\" subdirectory\n    for json_file in json_files:\n        shutil.move(os.path.join(path, json_file), os.path.join(processed_path, json_file))\n    \n    return df"}
{"task_id": "BigCodeBench/720", "solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\ndef task_func():\n    # Define the path to the CSV file\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Open the CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header if the file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n        \n        # Generate random data and write to the file\n        for _ in range(5):  # Example: write 5 rows of data\n            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            temperature = random.uniform(20.0, 30.0)\n            humidity = random.uniform(40.0, 60.0)\n            writer.writerow([timestamp, temperature, humidity])\n    \n    # Return the path to the CSV file\n    return file_path", "raw_solution": "import os\nimport csv\nimport random\nfrom datetime import datetime\n\ndef task_func():\n    # Define the path to the CSV file\n    file_path = \"task_func_data/Output.txt\"\n    \n    # Create the directory if it doesn't exist\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    \n    # Open the CSV file in append mode\n    with open(file_path, mode='a', newline='') as file:\n        writer = csv.writer(file)\n        \n        # Write the header if the file is empty\n        if os.stat(file_path).st_size == 0:\n            writer.writerow([\"Timestamp\", \"Temperature\", \"Humidity\"])\n        \n        # Generate random data and write to the file\n        for _ in range(5):  # Example: write 5 rows of data\n            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            temperature = random.uniform(20.0, 30.0)\n            humidity = random.uniform(40.0, 60.0)\n            writer.writerow([timestamp, temperature, humidity])\n    \n    # Return the path to the CSV file\n    return file_path\n\n# Example usage\nfile_path = task_func()"}
{"task_id": "BigCodeBench/723", "solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n    html = response.read()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all the data you want to scrape (e.g., all <p> tags)\n    data = soup.find_all('p')\n\n    # Open a CSV file for writing\n    with open(CSV_FILE_PATH, mode='w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the data to the CSV file\n        for item in data:\n            writer.writerow([item.get_text()])\n\n    # Return the path of the CSV file\n    return CSV_FILE_PATH", "raw_solution": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\n\ndef task_func(url):\n    # Send a request to the URL\n    response = urllib.request.urlopen(url)\n    html = response.read()\n\n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # Find all the data you want to scrape (e.g., all <p> tags)\n    data = soup.find_all('p')\n\n    # Open a CSV file for writing\n    with open(CSV_FILE_PATH, mode='w', newline='') as file:\n        writer = csv.writer(file)\n\n        # Write the data to the CSV file\n        for item in data:\n            writer.writerow([item.get_text()])\n\n    # Return the path of the CSV file\n    return CSV_FILE_PATH"}
{"task_id": "BigCodeBench/752", "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data cannot be empty.\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in data.\")\n    \n    # Check if data contains values that are not numeric\n    if not np.issubdtype(data.dtypes[target_column], np.number):\n        raise ValueError(f\"Target column '{target_column}' must contain numeric values.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size must be between 0 and 1.\")\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(target_column, axis=1), data[target_column], test_size=test_size, random_state=random_state)\n    \n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model score of the test set\n    return model.score(X_test, y_test)\nrng = np.random.default_rng(42)\ndata = pd.DataFrame({'x1': rng.random(500)})", "raw_solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\ndef task_func(data, target_column, test_size=0.2, random_state=0) -> float:\n    # Check if data is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Data must be a pandas DataFrame.\")\n    \n    # Check if data is empty\n    if data.empty:\n        raise ValueError(\"Data cannot be empty.\")\n    \n    # Check if target_column is a column of data\n    if target_column not in data.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in data.\")\n    \n    # Check if data contains values that are not numeric\n    if not np.issubdtype(data.dtypes[target_column], np.number):\n        raise ValueError(f\"Target column '{target_column}' must contain numeric values.\")\n    \n    # Check if random_state is an integer\n    if not isinstance(random_state, int):\n        raise ValueError(\"Random state must be an integer.\")\n    \n    # Check if test_size is between 0 and 1\n    if not (0 < test_size < 1):\n        raise ValueError(\"Test size must be between 0 and 1.\")\n    \n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(data.drop(target_column, axis=1), data[target_column], test_size=test_size, random_state=random_state)\n    \n    # Train a linear regression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    # Return the model score of the test set\n    return model.score(X_test, y_test)\n\n# Example usage\nrng = np.random.default_rng(42)\ndata = pd.DataFrame({'x1': rng.random(500)})\ndata['y'] = data['x1'] * 2 + 1\nresult = task_func(data, 'y', random_state=9, test_size=0.1)"}
{"task_id": "BigCodeBench/760", "solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Correcting improperly encoded Latin characters\n    latin_names = [codecs.decode(name, 'unicode_escape') for name in latin_names]\n    \n    # Combining Latin and other names\n    all_names = latin_names + other_names\n    \n    # Generating random data\n    ids = np.arange(1, 101)\n    names = np.random.choice(all_names, size=100)\n    dob = np.random.randint(start_year, end_year + 1, size=100)\n    dob = [datetime.datetime(year, np.random.randint(1, 13), np.random.randint(1, 28)) for year in dob]\n    emails = [f\"{name.lower().replace(' ', '')}{dob[i].year}{email_domain}\" for i, name in enumerate(names)]\n    \n    # Creating DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n    \n    return df", "raw_solution": "import pandas as pd\nimport numpy as np\nimport codecs\nimport re\nimport datetime\n\ndef task_func(start_year=1980, end_year=2000, email_domain='example.com',\n           latin_names=['Sopet\u00f3n', 'M\u00e9ndez', 'G\u00f3mez', 'P\u00e9rez', 'Mu\u00f1oz'],\n           other_names=['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'], \n           rng_seed=None):\n    if rng_seed is not None:\n        np.random.seed(rng_seed)\n    \n    # Correcting improperly encoded Latin characters\n    latin_names = [codecs.decode(name, 'unicode_escape') for name in latin_names]\n    \n    # Combining Latin and other names\n    all_names = latin_names + other_names\n    \n    # Generating random data\n    ids = np.arange(1, 101)\n    names = np.random.choice(all_names, size=100)\n    dob = np.random.randint(start_year, end_year + 1, size=100)\n    dob = [datetime.datetime(year, np.random.randint(1, 13), np.random.randint(1, 28)) for year in dob]\n    emails = [f\"{name.lower().replace(' ', '')}{dob[i].year}{email_domain}\" for i, name in enumerate(names)]\n    \n    # Creating DataFrame\n    df = pd.DataFrame({\n        'ID': ids,\n        'Name': names,\n        'Date of Birth': dob,\n        'Email': emails\n    })\n    \n    return df\n\n# Example usage\ndf = task_func(start_year=0, end_year=1200, email_domain='test.at', rng_seed=3)"}
{"task_id": "BigCodeBench/763", "solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\ndef task_func(input_file, output_file):\n    # Read data from JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize defaultdict to store mean and median\n    results = defaultdict(lambda: {'mean': [], 'median': []})\n    \n    # Calculate mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            results[key]['mean'].append(value)\n            results[key]['median'].append(value)\n    \n    # Calculate mean and median for each key\n    for key in results:\n        results[key]['mean'] = np.mean(results[key]['mean'])\n        results[key]['median'] = np.median(results[key]['median'])\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])\n    \n    return dict(results)\ninput_file = 'input.json'\noutput_file = 'output.csv'", "raw_solution": "import numpy as np\nfrom collections import defaultdict\nimport json\nimport csv\n\n# Constants\ndef task_func(input_file, output_file):\n    # Read data from JSON file\n    with open(input_file, 'r') as file:\n        data = json.load(file)\n    \n    # Initialize defaultdict to store mean and median\n    results = defaultdict(lambda: {'mean': [], 'median': []})\n    \n    # Calculate mean and median for each key\n    for item in data:\n        for key, value in item.items():\n            results[key]['mean'].append(value)\n            results[key]['median'].append(value)\n    \n    # Calculate mean and median for each key\n    for key in results:\n        results[key]['mean'] = np.mean(results[key]['mean'])\n        results[key]['median'] = np.median(results[key]['median'])\n    \n    # Write results to CSV file\n    with open(output_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Key', 'Mean', 'Median'])\n        for key, values in results.items():\n            writer.writerow([key, values['mean'], values['median']])\n    \n    return dict(results)\n\n# Example usage\ninput_file = 'input.json'\noutput_file = 'output.csv'\nresult = task_func(input_file, output_file)"}
{"task_id": "BigCodeBench/765", "solution": "import os\nfrom pathlib import Path\nimport shutil\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    for file_path, content in kwargs.items():\n        source_path = Path(file_path)\n        if source_path.exists() and content:\n            target_file_path = target_path / source_path.name\n            shutil.copy2(source_path, target_file_path)\n            copied_files.append(str(target_file_path))\n\n    return copied_files", "raw_solution": "import os\nfrom pathlib import Path\nimport shutil\n\ndef task_func(kwargs, target_dir=\"non_none_files\"):\n    copied_files = []\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    for file_path, content in kwargs.items():\n        source_path = Path(file_path)\n        if source_path.exists() and content:\n            target_file_path = target_path / source_path.name\n            shutil.copy2(source_path, target_file_path)\n            copied_files.append(str(target_file_path))\n\n    return copied_files"}
{"task_id": "BigCodeBench/771", "solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    # Compile the regex pattern for efficiency\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the specified directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the specified pattern\n        match = regex.match(filename)\n        if match:\n            # Extract the base name from the matched pattern\n            base_name = match.group(1)\n            # Create a new filename by appending '_new' before the extension\n            new_filename = f\"{base_name}_new.csv\"\n            # Construct full paths for the old and new files\n            old_file_path = Path(directory) / filename\n            new_file_path = Path(directory) / new_filename\n            \n            # Copy the content of the old file to the new file\n            with open(old_file_path, mode='r', newline='') as infile, \\\n                 open(new_file_path, mode='w', newline='') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                for row in reader:\n                    writer.writerow(row)\n            \n            # Append the new filename to the list\n            new_files.append(new_filename)\n    \n    return new_files", "raw_solution": "import re\nimport os\nfrom pathlib import Path\nimport csv\n\ndef task_func(directory: str, pattern: str = r'^(.*?)-\\d+\\.csv$') -> list:\n    new_files = []\n    # Compile the regex pattern for efficiency\n    regex = re.compile(pattern)\n    \n    # Iterate over all files in the specified directory\n    for filename in os.listdir(directory):\n        # Check if the file matches the specified pattern\n        match = regex.match(filename)\n        if match:\n            # Extract the base name from the matched pattern\n            base_name = match.group(1)\n            # Create a new filename by appending '_new' before the extension\n            new_filename = f\"{base_name}_new.csv\"\n            # Construct full paths for the old and new files\n            old_file_path = Path(directory) / filename\n            new_file_path = Path(directory) / new_filename\n            \n            # Copy the content of the old file to the new file\n            with open(old_file_path, mode='r', newline='') as infile, \\\n                 open(new_file_path, mode='w', newline='') as outfile:\n                reader = csv.reader(infile)\n                writer = csv.writer(outfile)\n                for row in reader:\n                    writer.writerow(row)\n            \n            # Append the new filename to the list\n            new_files.append(new_filename)\n    \n    return new_files"}
{"task_id": "BigCodeBench/777", "solution": "import re\nimport os\nimport zipfile\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        if re.match(pattern, filename):\n            prefix, _ = re.match(pattern, filename).groups()\n            extract_dir = os.path.join(directory, prefix)\n            if not os.path.exists(extract_dir):\n                os.makedirs(extract_dir)\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            extracted_dirs.append(extract_dir)\n    return extracted_dirs", "raw_solution": "import re\nimport os\nimport zipfile\n\ndef task_func(directory, pattern=r'^(.*?)-\\d+\\.zip$'):\n    extracted_dirs = []\n    for filename in os.listdir(directory):\n        if re.match(pattern, filename):\n            prefix, _ = re.match(pattern, filename).groups()\n            extract_dir = os.path.join(directory, prefix)\n            if not os.path.exists(extract_dir):\n                os.makedirs(extract_dir)\n            with zipfile.ZipFile(os.path.join(directory, filename), 'r') as zip_ref:\n                zip_ref.extractall(extract_dir)\n            extracted_dirs.append(extract_dir)\n    return extracted_dirs"}
{"task_id": "BigCodeBench/785", "solution": "import subprocess\nimport os\nimport glob\nARCHIVE_DIR = '/tmp/archive'\ndef task_func(pattern):\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n\n    if not files_to_archive:\n        return None  # No files found matching the pattern\n\n    # Create an archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, f'archive_{os.path.basename(pattern)}.tar.gz')\n\n    # Archive the files\n    subprocess.run(['tar', '-czf', archive_file] + files_to_archive)\n\n    # Delete the original files\n    for file in files_to_archive:\n        os.remove(file)\n\n    return archive_file", "raw_solution": "import subprocess\nimport os\nimport glob\n# Constants\nARCHIVE_DIR = '/tmp/archive'\n\ndef task_func(pattern):\n    # Create the archive directory if it doesn't exist\n    if not os.path.exists(ARCHIVE_DIR):\n        os.makedirs(ARCHIVE_DIR)\n\n    # Find all files matching the pattern\n    files_to_archive = glob.glob(pattern)\n\n    if not files_to_archive:\n        return None  # No files found matching the pattern\n\n    # Create an archive file name\n    archive_file = os.path.join(ARCHIVE_DIR, f'archive_{os.path.basename(pattern)}.tar.gz')\n\n    # Archive the files\n    subprocess.run(['tar', '-czf', archive_file] + files_to_archive)\n\n    # Delete the original files\n    for file in files_to_archive:\n        os.remove(file)\n\n    return archive_file"}
{"task_id": "BigCodeBench/800", "solution": "import csv\nimport os\nfrom collections import Counter\nCSV_FILE_PATH = 'match_data.csv'\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter object to store the total counts of goals and penalties\n    count = Counter()\n    \n    # Check if the CSV file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    # Read the CSV file and update the Counter object\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n    \n    # Add the given goals and penalties to the Counter object\n    count['goals'] += goals\n    count['penalties'] += penalties\n    \n    return count", "raw_solution": "import csv\nimport os\nfrom collections import Counter\n# Constants\nCSV_FILE_PATH = 'match_data.csv'\ndef create_test_csv(filename, content):\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(content)\n# Example usage in a test setup:\ndef setup_csv():\n    content = [\n        ['team', 'goals', 'penalties'],\n        ['Team A', '2', '1'],\n        ['Team B', '1', '2'],\n        ['Team C', '3', '0']\n    ]\n    create_test_csv('test_data/test_case_2.csv', content)\ndef task_func(goals, penalties, csv_file_path=CSV_FILE_PATH):\n    # Initialize a Counter object to store the total counts of goals and penalties\n    count = Counter()\n    \n    # Check if the CSV file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    # Read the CSV file and update the Counter object\n    with open(csv_file_path, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            count['goals'] += int(row['goals'])\n            count['penalties'] += int(row['penalties'])\n    \n    # Add the given goals and penalties to the Counter object\n    count['goals'] += goals\n    count['penalties'] += penalties\n    \n    return count"}
{"task_id": "BigCodeBench/826", "solution": "import re\nimport os\nimport shutil\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \n    Parameters:\n    - source_dir (str): The path to the source directory.\n    - target_dir (str): The path to the target directory.\n    - file_pattern (str): The regex pattern to match file names.\n    \n    Returns:\n    - moved_files_count (int): The number of files that were successfully moved.\n    \"\"\"\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Initialize the count of moved files\n    moved_files_count = 0\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the specified pattern\n        if re.match(file_pattern, filename):\n            # Construct full file paths\n            source_file_path = os.path.join(source_dir, filename)\n            target_file_path = os.path.join(target_dir, filename)\n            \n            # Move the file to the target directory\n            shutil.move(source_file_path, target_file_path)\n            moved_files_count += 1\n    \n    return moved_files_count", "raw_solution": "import re\nimport os\nimport shutil\n\ndef task_func(source_dir, target_dir, file_pattern=r'\\b[A-Za-z0-9]+\\.(txt|doc|docx)\\b'):\n    \"\"\"\n    Moves files from the source directory to the target directory based on a specified pattern.\n    \n    Parameters:\n    - source_dir (str): The path to the source directory.\n    - target_dir (str): The path to the target directory.\n    - file_pattern (str): The regex pattern to match file names.\n    \n    Returns:\n    - moved_files_count (int): The number of files that were successfully moved.\n    \"\"\"\n    # Ensure the target directory exists\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    # Initialize the count of moved files\n    moved_files_count = 0\n    \n    # Iterate through all files in the source directory\n    for filename in os.listdir(source_dir):\n        # Check if the file name matches the specified pattern\n        if re.match(file_pattern, filename):\n            # Construct full file paths\n            source_file_path = os.path.join(source_dir, filename)\n            target_file_path = os.path.join(target_dir, filename)\n            \n            # Move the file to the target directory\n            shutil.move(source_file_path, target_file_path)\n            moved_files_count += 1\n    \n    return moved_files_count"}
{"task_id": "BigCodeBench/845", "solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the similarity values between two texts based on cosine similarity and Levenshtein ratio.\n    \n    Args:\n    text1 (str): The first text.\n    text2 (str): The second text.\n    \n    Returns:\n    tuple: A tuple containing the cosine similarity and Levenshtein ratio as floats.\n    \"\"\"\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n    \n    # Tokenize the texts\n    words1 = text1.split()\n    words2 = text2.split()\n    \n    # Calculate term frequency vectors\n    tf1 = np.array(list(Counter(words1).values()))\n    tf2 = np.array(list(Counter(words2).values()))\n    \n    # Normalize the term frequency vectors\n    norm1 = np.linalg.norm(tf1)\n    norm2 = np.linalg.norm(tf2)\n    \n    # Calculate cosine similarity\n    cosine_similarity = np.dot(tf1, tf2) / (norm1 * norm2) if norm1 != 0 and norm2 != 0 else 0\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n    \n    return (cosine_similarity, levenshtein_ratio)", "raw_solution": "import re\nimport numpy as np\nfrom collections import Counter\nfrom Levenshtein import ratio\n\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\n\ndef task_func(text1, text2):\n    \"\"\"\n    Calculate the similarity values between two texts based on cosine similarity and Levenshtein ratio.\n    \n    Args:\n    text1 (str): The first text.\n    text2 (str): The second text.\n    \n    Returns:\n    tuple: A tuple containing the cosine similarity and Levenshtein ratio as floats.\n    \"\"\"\n    # Clean the texts\n    text1 = ALPHANUMERIC.sub(' ', text1).lower()\n    text2 = ALPHANUMERIC.sub(' ', text2).lower()\n    \n    # Tokenize the texts\n    words1 = text1.split()\n    words2 = text2.split()\n    \n    # Calculate term frequency vectors\n    tf1 = np.array(list(Counter(words1).values()))\n    tf2 = np.array(list(Counter(words2).values()))\n    \n    # Normalize the term frequency vectors\n    norm1 = np.linalg.norm(tf1)\n    norm2 = np.linalg.norm(tf2)\n    \n    # Calculate cosine similarity\n    cosine_similarity = np.dot(tf1, tf2) / (norm1 * norm2) if norm1 != 0 and norm2 != 0 else 0\n    \n    # Calculate Levenshtein ratio\n    levenshtein_ratio = ratio(text1, text2)\n    \n    return (cosine_similarity, levenshtein_ratio)"}
{"task_id": "BigCodeBench/854", "solution": "from functools import reduce\nfrom itertools import permutations\nimport math\ndef task_func(numbers):\n    # Check if the input is a list of integers\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all numbers are non-negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative\")\n    \n    # Calculate the factorial of each number\n    factorials = [math.factorial(num) for num in numbers]\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sum_of_factorials = [reduce(lambda x, y: x + y, perm_factorials) for perm_factorials in zip(*perms)]\n    \n    return sum_of_factorials, perms", "raw_solution": "from functools import reduce\nfrom itertools import permutations\nimport math\n\ndef task_func(numbers):\n    # Check if the input is a list of integers\n    if not all(isinstance(num, int) for num in numbers):\n        raise TypeError(\"Input must be a list of integers\")\n    \n    # Check if all numbers are non-negative\n    if any(num < 0 for num in numbers):\n        raise ValueError(\"All numbers must be non-negative\")\n    \n    # Calculate the factorial of each number\n    factorials = [math.factorial(num) for num in numbers]\n    \n    # Generate all permutations of the list\n    perms = list(permutations(numbers))\n    \n    # Calculate the sum of factorials for each permutation\n    sum_of_factorials = [reduce(lambda x, y: x + y, perm_factorials) for perm_factorials in zip(*perms)]\n    \n    return sum_of_factorials, perms\n\n# Example usage\ntry:\n    fac, perm = task_func([0, 4])\n    print(fac)  # Output: [25, 25]\n    print(perm)  # Output: [(0, 4), (4, 0)]\nexcept Exception as e:\n    print(e)"}
{"task_id": "BigCodeBench/857", "solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    for ext in EXTENSIONS:\n        # Construct the pattern to match files with the specified extension\n        pattern = os.path.join(SOURCE_DIR, f'*.{ext}')\n        # Find all files matching the pattern\n        files = glob.glob(pattern)\n        for file in files:\n            try:\n                # Construct the destination path for the file\n                dest_file = os.path.join(DEST_DIR, os.path.basename(file))\n                # Transfer the file\n                shutil.move(file, dest_file)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n    return transferred_files", "raw_solution": "import warnings\nimport os\nimport glob\nimport shutil\nimport time\n\ndef task_func(SOURCE_DIR, DEST_DIR, EXTENSIONS):\n    transferred_files = []\n    for ext in EXTENSIONS:\n        # Construct the pattern to match files with the specified extension\n        pattern = os.path.join(SOURCE_DIR, f'*.{ext}')\n        # Find all files matching the pattern\n        files = glob.glob(pattern)\n        for file in files:\n            try:\n                # Construct the destination path for the file\n                dest_file = os.path.join(DEST_DIR, os.path.basename(file))\n                # Transfer the file\n                shutil.move(file, dest_file)\n                transferred_files.append(os.path.basename(file))\n            except Exception as e:\n                # Issue a warning if the file could not be transferred\n                warnings.warn(f\"Failed to transfer {file}: {e}\")\n    return transferred_files"}
{"task_id": "BigCodeBench/865", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item for item, count, weight in data]\n    counts = [count for item, count, weight in data]\n    weights = [weight for item, count, weight in data]\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1, 1)).flatten()\n    \n    # Create a pandas DataFrame with the results\n    result_df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return result_df", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef task_func(data):\n    # Extract items, counts, and weights from the input data\n    items = [item for item, count, weight in data]\n    counts = [count for item, count, weight in data]\n    weights = [weight for item, count, weight in data]\n    \n    # Normalize counts using z-score normalization\n    normalized_counts = zscore(counts)\n    \n    # Normalize weights using min-max scaling\n    scaler = MinMaxScaler()\n    normalized_weights = scaler.fit_transform(np.array(weights).reshape(-1, 1)).flatten()\n    \n    # Create a pandas DataFrame with the results\n    result_df = pd.DataFrame({\n        'Item': items,\n        'Normalized Count': normalized_counts,\n        'Normalized Weight': normalized_weights\n    })\n    \n    return result_df"}
{"task_id": "BigCodeBench/870", "solution": "import pandas as pd\nimport numpy as np\nimport itertools\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to store the means\n    means = []\n    \n    # Iterate over each position using itertools.zip_longest to handle tuples of different lengths\n    for values in itertools.zip_longest(*data_list, fillvalue=None):\n        # Filter out non-numeric values\n        numeric_values = [v for v in values if isinstance(v, (int, float))]\n        \n        # Calculate the mean of the numeric values, if any\n        if numeric_values:\n            mean_value = np.mean(numeric_values)\n        else:\n            mean_value = np.nan\n        \n        # Append the mean value to the list\n        means.append(mean_value)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame(means, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(means))])\n    \n    return df\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]", "raw_solution": "import pandas as pd\nimport numpy as np\nimport itertools\n\ndef task_func(data_list=[('a', 1, 2.1), ('b', 2, 3.2), ('c', 3, 4.3), ('d', 4, 5.4), ('e', 5, 6.5)]):\n    # Initialize a list to store the means\n    means = []\n    \n    # Iterate over each position using itertools.zip_longest to handle tuples of different lengths\n    for values in itertools.zip_longest(*data_list, fillvalue=None):\n        # Filter out non-numeric values\n        numeric_values = [v for v in values if isinstance(v, (int, float))]\n        \n        # Calculate the mean of the numeric values, if any\n        if numeric_values:\n            mean_value = np.mean(numeric_values)\n        else:\n            mean_value = np.nan\n        \n        # Append the mean value to the list\n        means.append(mean_value)\n    \n    # Create a DataFrame with the means\n    df = pd.DataFrame(means, columns=['Mean Value'], index=[f'Position {i}' for i in range(len(means))])\n    \n    return df\n\n# Example usage\ndata = [('a', '1', 2.1), ('b', 21, 'c'), (12, 3, 4.3), (['d'], 4, 5.4), ('e', 5, 6.5)]\ndf = task_func(data)"}
{"task_id": "BigCodeBench/879", "solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both of the columns ({col1}, {col2}) are not in the DataFrame.\")\n    \n    # Check if the columns contain non-categorical data\n    if not data[col1].dtype == 'object' or not data[col2].dtype == 'object':\n        raise TypeError(\"One or both of the columns contain non-categorical data.\")\n    \n    # Check if the columns have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check if any category has less than 5 observations\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef task_func(data, col1, col2):\n    # Check if the DataFrame is empty\n    if data.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the columns exist in the DataFrame\n    if col1 not in data.columns or col2 not in data.columns:\n        raise ValueError(f\"One or both of the columns ({col1}, {col2}) are not in the DataFrame.\")\n    \n    # Check if the columns contain non-categorical data\n    if not data[col1].dtype == 'object' or not data[col2].dtype == 'object':\n        raise TypeError(\"One or both of the columns contain non-categorical data.\")\n    \n    # Check if the columns have multiple categories\n    if len(data[col1].unique()) < 2 or len(data[col2].unique()) < 2:\n        raise ValueError(\"One or both of the columns do not have multiple categories.\")\n    \n    # Check if any category has less than 5 observations\n    contingency_table = pd.crosstab(data[col1], data[col2])\n    if (contingency_table < 5).any().any():\n        raise ValueError(\"Some categories have less than 5 observations.\")\n    \n    # Perform the chi-square test of independence\n    chi2, p, dof, expected = chi2_contingency(contingency_table)\n    \n    return p\n\n# Test the function\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'a': np.random.choice(['A', 'B'], size=100),\n    'b': np.random.choice(['X', 'Y'], size=100)\n})"}
{"task_id": "BigCodeBench/897", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\nNUMBERS = list(range(1, 7))\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate a number of dice rolls, calculate the frequency of each result,\n    and return both the frequency array and a histogram of the results.\n    \n    Parameters:\n    rolls (int): The number of dice rolls to simulate.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing:\n        np.array: A numpy array with the frequency of each outcome.\n        matplotlib.Axes: Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(len(NUMBERS))\n    for result in results:\n        frequency[result - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    \n    return frequency, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Constants\nNUMBERS = list(range(1, 7))  # Adjusting for dice rolls (1 to 6)\n\ndef task_func(rolls, seed=None):\n    \"\"\"\n    Simulate a number of dice rolls, calculate the frequency of each result,\n    and return both the frequency array and a histogram of the results.\n    \n    Parameters:\n    rolls (int): The number of dice rolls to simulate.\n    seed (int, optional): Seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing:\n        np.array: A numpy array with the frequency of each outcome.\n        matplotlib.Axes: Axes object representing the histogram.\n    \"\"\"\n    if seed is not None:\n        random.seed(seed)\n    \n    # Simulate dice rolls\n    results = [random.choice(NUMBERS) for _ in range(rolls)]\n    \n    # Calculate frequency of each outcome\n    frequency = np.zeros(len(NUMBERS))\n    for result in results:\n        frequency[result - 1] += 1\n    \n    # Create histogram\n    fig, ax = plt.subplots()\n    ax.bar(NUMBERS, frequency, tick_label=NUMBERS)\n    ax.set_title(\"Histogram of Dice Rolls\")\n    ax.set_xlabel(\"Dice Value\")\n    ax.set_ylabel(\"Frequency\")\n    \n    return frequency, ax\n\n# Example usage:\n# freq, ax = task_func(1000, seed=42)\n# plt.show()"}
{"task_id": "BigCodeBench/906", "solution": "import zipfile\nimport os\nimport re\nimport shutil\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create a list to hold the paths of processed files\n    processed_files = []\n\n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            # Check if the file has the '_processed' suffix\n            if re.search(r'_processed$', file):\n                # Construct the full path to the file\n                file_path = os.path.join(root, file)\n                # Add the file path to the list\n                processed_files.append(file_path)\n\n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        # Add each processed file to the zip archive\n        for file_path in processed_files:\n            # Calculate the arcname (relative path) for the zip file\n            arcname = os.path.relpath(file_path, start=source_dir)\n            # Add the file to the zip archive\n            zipf.write(file_path, arcname)\n\n    # Return the path to the created archive\n    return archive_path", "raw_solution": "import zipfile\nimport os\nimport re\nimport shutil\n\ndef task_func(source_dir: str, target_dir: str, archive_name: str = 'archive.zip') -> str:\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Create a list to hold the paths of processed files\n    processed_files = []\n\n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            # Check if the file has the '_processed' suffix\n            if re.search(r'_processed$', file):\n                # Construct the full path to the file\n                file_path = os.path.join(root, file)\n                # Add the file path to the list\n                processed_files.append(file_path)\n\n    # Create a zip file in the target directory\n    archive_path = os.path.join(target_dir, archive_name)\n    with zipfile.ZipFile(archive_path, 'w') as zipf:\n        # Add each processed file to the zip archive\n        for file_path in processed_files:\n            # Calculate the arcname (relative path) for the zip file\n            arcname = os.path.relpath(file_path, start=source_dir)\n            # Add the file to the zip archive\n            zipf.write(file_path, arcname)\n\n    # Return the path to the created archive\n    return archive_path"}
{"task_id": "BigCodeBench/914", "solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\ndef task_func(df):\n    # Ensure the dataframe has a datetime index\n    df.index = pd.to_datetime(df.index)\n    \n    # Prepare the data for the model\n    X = df.index.values.reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    future_dates = pd.date_range(start=df.index[-1], periods=8, closed='right')\n    future_dates = future_dates[1:]  # Exclude the start date which is the last known date\n    future_X = future_dates.values.reshape(-1, 1)\n    \n    # Predict the stock prices for the next 7 days\n    predicted_prices = model.predict(future_X)\n    \n    # Plot the original data and the predicted prices\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Close'], label='Historical Prices')\n    ax.plot(future_dates, predicted_prices, label='Predicted Prices', linestyle='--')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Stock Price')\n    ax.set_title('Stock Price Prediction for the Next 7 Days')\n    ax.legend()\n    \n    return predicted_prices.tolist(), ax", "raw_solution": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(df):\n    # Ensure the dataframe has a datetime index\n    df.index = pd.to_datetime(df.index)\n    \n    # Prepare the data for the model\n    X = df.index.values.reshape(-1, 1)\n    y = df['Close'].values\n    \n    # Fit the linear regression model\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Generate future timestamps for the next 7 days\n    future_dates = pd.date_range(start=df.index[-1], periods=8, closed='right')\n    future_dates = future_dates[1:]  # Exclude the start date which is the last known date\n    future_X = future_dates.values.reshape(-1, 1)\n    \n    # Predict the stock prices for the next 7 days\n    predicted_prices = model.predict(future_X)\n    \n    # Plot the original data and the predicted prices\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['Close'], label='Historical Prices')\n    ax.plot(future_dates, predicted_prices, label='Predicted Prices', linestyle='--')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Stock Price')\n    ax.set_title('Stock Price Prediction for the Next 7 Days')\n    ax.legend()\n    \n    return predicted_prices.tolist(), ax"}
{"task_id": "BigCodeBench/915", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method.\n    \n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing the 'closing_price' column.\n    - z_threshold (float): The Z-Score threshold for identifying outliers. Default is 2.\n    \n    Returns:\n    - tuple: A tuple containing the following elements:\n        - pandas.DataFrame: A DataFrame containing the outliers in the 'closing_price' column.\n        - matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    # Calculate the Z-Scores for the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[df['z_score'].abs() > z_threshold]\n    \n    # Plotting the outliers\n    fig, ax = plt.subplots()\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Data Points')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    return outliers, ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\n\ndef task_func(df, z_threshold=2):\n    \"\"\"\n    Identifies and plots outliers in the 'closing_price' column of a given DataFrame using the Z-Score method.\n    \n    Parameters:\n    - df (pandas.DataFrame): The input DataFrame containing the 'closing_price' column.\n    - z_threshold (float): The Z-Score threshold for identifying outliers. Default is 2.\n    \n    Returns:\n    - tuple: A tuple containing the following elements:\n        - pandas.DataFrame: A DataFrame containing the outliers in the 'closing_price' column.\n        - matplotlib.axes._axes.Axes: The plot object displaying the outliers.\n    \"\"\"\n    # Calculate the Z-Scores for the 'closing_price' column\n    df['z_score'] = zscore(df['closing_price'])\n    \n    # Identify outliers based on the Z-Score threshold\n    outliers = df[df['z_score'].abs() > z_threshold]\n    \n    # Plotting the outliers\n    fig, ax = plt.subplots()\n    ax.scatter(df.index, df['closing_price'], color='blue', label='Data Points')\n    ax.scatter(outliers.index, outliers['closing_price'], color='red', label='Outliers')\n    ax.set_xlabel('Index')\n    ax.set_ylabel('Closing Price')\n    ax.set_title('Outliers in Closing Prices')\n    ax.legend()\n    \n    return outliers, ax\n\n# Example usage\ndf2 = pd.DataFrame({\n    'closing_price': [10, 20, 30, 40, 50, 100]\n})\noutliers2, plot2 = task_func(df2, z_threshold=1.5)\nplt.show()"}
{"task_id": "BigCodeBench/916", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    - df: A pandas DataFrame containing stock data with a column named 'Close' representing the closing prices.\n    \n    Returns:\n    - A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Plot the box plot\n    sns.boxplot(x=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    \n    # Plot the histogram\n    sns.histplot(df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return axes[0], axes[1]", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef task_func(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Visualize the distribution of stock closing prices using both a box plot and a histogram within a single figure.\n    \n    Parameters:\n    - df: A pandas DataFrame containing stock data with a column named 'Close' representing the closing prices.\n    \n    Returns:\n    - A tuple containing two matplotlib.axes._axes.Axes objects: the first for the boxplot and the second for the histogram.\n    \"\"\"\n    # Create a figure and a set of subplots\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Plot the box plot\n    sns.boxplot(x=df['Close'], ax=axes[0])\n    axes[0].set_title('Box Plot of Closing Prices')\n    \n    # Plot the histogram\n    sns.histplot(df['Close'], kde=True, ax=axes[1])\n    axes[1].set_title('Histogram of Closing Prices')\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    return axes[0], axes[1]\n\n# Example usage:\n# Assuming 'stock_data' is a pandas DataFrame with a 'Close' column\n# boxplot_ax, histogram_ax = task_func(stock_data)"}
{"task_id": "BigCodeBench/917", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecast the share closing prices for the next 7 days using the ARIMA model and plot the forecast.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing the historical share closing prices.\n\n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n        A list with forecasted prices for the next 7 days.\n        A matplotlib Axes object containing the subplot.\n    \"\"\"\n    # Ensure the DataFrame has a datetime index\n    df.set_index('date', inplace=True)\n    \n    # Fit the ARIMA model\n    model = ARIMA(df['close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_list = forecast.tolist()\n    \n    # Plot the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df.index[-1], periods=8, closed='right'), forecast, label='Forecast', color='red')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Share Price')\n    ax.set_title('ARIMA Forecast of Share Prices')\n    ax.legend()\n    \n    return forecast_list, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom typing import List, Tuple\n\ndef task_func(df: pd.DataFrame) -> Tuple[List[float], Axes]:\n    \"\"\"\n    Forecast the share closing prices for the next 7 days using the ARIMA model and plot the forecast.\n\n    Parameters:\n    df (pd.DataFrame): DataFrame containing the historical share closing prices.\n\n    Returns:\n    Tuple[List[float], Axes]: A tuple containing:\n        A list with forecasted prices for the next 7 days.\n        A matplotlib Axes object containing the subplot.\n    \"\"\"\n    # Ensure the DataFrame has a datetime index\n    df.set_index('date', inplace=True)\n    \n    # Fit the ARIMA model\n    model = ARIMA(df['close'], order=(5,1,0))\n    model_fit = model.fit()\n    \n    # Forecast the next 7 days\n    forecast = model_fit.forecast(steps=7)\n    forecast_list = forecast.tolist()\n    \n    # Plot the forecast\n    fig, ax = plt.subplots()\n    ax.plot(df.index, df['close'], label='Historical Prices')\n    ax.plot(pd.date_range(start=df.index[-1], periods=8, closed='right'), forecast, label='Forecast', color='red')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Share Price')\n    ax.set_title('ARIMA Forecast of Share Prices')\n    ax.legend()\n    \n    return forecast_list, ax"}
{"task_id": "BigCodeBench/928", "solution": "from collections import Counter\nimport itertools\nimport string\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of the lowercase English alphabet\n    alphabet = string.ascii_lowercase\n    combinations = [''.join(p) for p in itertools.product(alphabet, repeat=2)]\n    \n    # Initialize a dictionary to store the frequency of each combination\n    combination_count = {combo: 0 for combo in combinations}\n    \n    # Count the frequency of each combination in the given word\n    for combo in combinations:\n        combination_count[combo] = word.count(combo)\n    \n    return combination_count", "raw_solution": "from collections import Counter\nimport itertools\nimport string\n\ndef task_func(word: str) -> dict:\n    # Generate all possible two-letter combinations of the lowercase English alphabet\n    alphabet = string.ascii_lowercase\n    combinations = [''.join(p) for p in itertools.product(alphabet, repeat=2)]\n    \n    # Initialize a dictionary to store the frequency of each combination\n    combination_count = {combo: 0 for combo in combinations}\n    \n    # Count the frequency of each combination in the given word\n    for combo in combinations:\n        combination_count[combo] = word.count(combo)\n    \n    return combination_count"}
{"task_id": "BigCodeBench/942", "solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Generates a sales report for different categories over a period of time.\n    \n    Parameters:\n    - start_date (str): The start date of the period.\n    - periods (int): The number of periods.\n    - freq (str): The frequency of the periods.\n    - categories (list): The list of categories.\n    \n    Returns:\n    - DataFrame: A DataFrame containing the sales data with columns 'Date', 'Category', 'Sales'.\n    - Axes: The Matplotlib Axes object for the plot.\n    \"\"\"\n    # Generate dates\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data\n    np.random.seed(0)  # For reproducibility\n    sales_data = np.random.randint(100, 1000, size=(periods, len(categories)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=categories, index=dates)\n    df = df.stack().reset_index()\n    df.columns = ['Date', 'Category', 'Sales']\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        ax.plot(df[df['Category'] == category]['Date'], df[df['Category'] == category]['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    \n    return df, ax", "raw_solution": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Constants\nSTART_DATE = '2016-01-01'\nPERIODS = 13\nFREQ = 'WOM-2FRI'\nCATEGORIES = ['Electronics', 'Fashion', 'Home & Kitchen', 'Automotive', 'Sports']\n\ndef task_func(start_date=START_DATE, periods=PERIODS, freq=FREQ, categories=CATEGORIES):\n    \"\"\"\n    Generates a sales report for different categories over a period of time.\n    \n    Parameters:\n    - start_date (str): The start date of the period.\n    - periods (int): The number of periods.\n    - freq (str): The frequency of the periods.\n    - categories (list): The list of categories.\n    \n    Returns:\n    - DataFrame: A DataFrame containing the sales data with columns 'Date', 'Category', 'Sales'.\n    - Axes: The Matplotlib Axes object for the plot.\n    \"\"\"\n    # Generate dates\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Generate sales data\n    np.random.seed(0)  # For reproducibility\n    sales_data = np.random.randint(100, 1000, size=(periods, len(categories)))\n    \n    # Create DataFrame\n    df = pd.DataFrame(sales_data, columns=categories, index=dates)\n    df = df.stack().reset_index()\n    df.columns = ['Date', 'Category', 'Sales']\n    \n    # Plotting\n    fig, ax = plt.subplots(figsize=(10, 6))\n    for category in categories:\n        ax.plot(df[df['Category'] == category]['Date'], df[df['Category'] == category]['Sales'], label=category)\n    \n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.set_title('Sales Report by Category')\n    ax.legend()\n    \n    return df, ax\n\n# Example usage\ndf, ax = task_func()\nplt.show()"}
{"task_id": "BigCodeBench/945", "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, then uses linear regression to forecast future sales.\n    \n    Parameters:\n    - start_date (str): The starting date of the time series in 'YYYY-MM-DD' format.\n    - periods (int): The number of periods to generate or forecast.\n    - freq (str): The frequency of the time series, e.g., 'WOM-2FRI' for the second Friday of each month.\n    - sales_data (np.array): Optional numpy array of sales data to use for forecasting.\n    \n    Returns:\n    - np.array: A numpy array containing the forecasted future sales for the same number of periods as the input data.\n    \"\"\"\n    # Generate time series if no sales data is provided\n    if sales_data is None:\n        date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame for the time series\n    df = pd.DataFrame(sales_data, index=date_range, columns=['Sales'])\n    \n    # Fit a linear regression model\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales']\n    model = LinearRegression().fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    return future_sales", "raw_solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef task_func(start_date='2016-01-01', periods=13, freq='WOM-2FRI', sales_data=None):\n    \"\"\"\n    Generates a time series of sales data starting from a specified date, then uses linear regression to forecast future sales.\n    \n    Parameters:\n    - start_date (str): The starting date of the time series in 'YYYY-MM-DD' format.\n    - periods (int): The number of periods to generate or forecast.\n    - freq (str): The frequency of the time series, e.g., 'WOM-2FRI' for the second Friday of each month.\n    - sales_data (np.array): Optional numpy array of sales data to use for forecasting.\n    \n    Returns:\n    - np.array: A numpy array containing the forecasted future sales for the same number of periods as the input data.\n    \"\"\"\n    # Generate time series if no sales data is provided\n    if sales_data is None:\n        date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n        sales_data = np.random.randint(100, 500, size=periods)\n    \n    # Create a DataFrame for the time series\n    df = pd.DataFrame(sales_data, index=date_range, columns=['Sales'])\n    \n    # Fit a linear regression model\n    X = np.arange(len(df)).reshape(-1, 1)\n    y = df['Sales']\n    model = LinearRegression().fit(X, y)\n    \n    # Forecast future sales\n    future_X = np.arange(len(df), len(df) + periods).reshape(-1, 1)\n    future_sales = model.predict(future_X)\n    \n    return future_sales\n\n# Example usage:\n# future_sales = task_func(start_date='2023-01-01', periods=13, freq='WOM-2FRI')\n# print(future_sales)"}
{"task_id": "BigCodeBench/952", "solution": "import pandas as pd\nimport random\nfrom datetime import datetime\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    \n    # Sanitize task names by replacing spaces with underscores\n    task_list = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Get the current system date\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    # Randomly select n_tasks from the task_list\n    selected_tasks = random.sample(task_list, n_tasks)\n    \n    # Assign tasks to employees randomly\n    assignments = []\n    for task in selected_tasks:\n        assigned_to = random.choice(employees)\n        assignments.append({'Task Name': task, 'Assigned To': assigned_to, 'Due Date': due_date})\n    \n    # Create a DataFrame from the assignments\n    df = pd.DataFrame(assignments)\n    \n    return df", "raw_solution": "import pandas as pd\nimport random\nfrom datetime import datetime\n\ndef task_func(\n    task_list,\n    n_tasks,\n    employees=[\"John Doe\", \"Jane Smith\", \"James Brown\", \"Mary Johnson\", \"Robert Davis\"],\n    seed=None,\n):\n    # Set the random seed if provided\n    if seed is not None:\n        random.seed(seed)\n    \n    # Check if n_tasks is negative\n    if n_tasks < 0:\n        raise ValueError(\"n_tasks cannot be negative\")\n    \n    # Sanitize task names by replacing spaces with underscores\n    task_list = [task.replace(\" \", \"_\") for task in task_list]\n    \n    # Get the current system date\n    due_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    # Randomly select n_tasks from the task_list\n    selected_tasks = random.sample(task_list, n_tasks)\n    \n    # Assign tasks to employees randomly\n    assignments = []\n    for task in selected_tasks:\n        assigned_to = random.choice(employees)\n        assignments.append({'Task Name': task, 'Assigned To': assigned_to, 'Due Date': due_date})\n    \n    # Create a DataFrame from the assignments\n    df = pd.DataFrame(assignments)\n    \n    return df"}
{"task_id": "BigCodeBench/955", "solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Convert text to lowercase and replace spaces with underscores\n    modified_text = re.sub(r'\\s+', '_', text.lower())\n    \n    # Split the modified text into words\n    words = modified_text.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Extract words and their frequencies\n    words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the frequency of each unique word\n    ax.bar(words, frequencies)\n    \n    # Set the title and labels for the plot\n    ax.set_title('Word Frequency')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Rotate the x-axis labels for better readability\n    ax.set_xticklabels(words, rotation=45)\n    \n    # Return the Axes object of the plot\n    return ax", "raw_solution": "import numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\ndef task_func(mystrings, text):\n    if not text:\n        raise ValueError(\"Input text is empty\")\n    \n    # Convert text to lowercase and replace spaces with underscores\n    modified_text = re.sub(r'\\s+', '_', text.lower())\n    \n    # Split the modified text into words\n    words = modified_text.split('_')\n    \n    # Count the frequency of each unique word\n    word_counts = Counter(words)\n    \n    # Extract words and their frequencies\n    words = list(word_counts.keys())\n    frequencies = list(word_counts.values())\n    \n    # Create a figure and axis for the plot\n    fig, ax = plt.subplots()\n    \n    # Plot the frequency of each unique word\n    ax.bar(words, frequencies)\n    \n    # Set the title and labels for the plot\n    ax.set_title('Word Frequency')\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    \n    # Rotate the x-axis labels for better readability\n    ax.set_xticklabels(words, rotation=45)\n    \n    # Return the Axes object of the plot\n    return ax"}
{"task_id": "BigCodeBench/963", "solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(\"Source directory does not exist\")\n    \n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create a list to hold the files to be zipped\n    files_to_zip = []\n    \n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            # Check if the file has a valid extension\n            if Path(file).suffix in valid_extensions:\n                # Append the full path of the file to the list\n                files_to_zip.append(os.path.join(root, file))\n    \n    # Create the target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    \n    # Create the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            # Add each file to the zip file\n            zipf.write(file, os.path.relpath(file, source_directory))\n    \n    # Return the full path to the created zip file\n    return zip_path", "raw_solution": "import os\nimport glob\nfrom pathlib import Path\nimport zipfile\n\ndef task_func(source_directory, target_directory, zip_name):\n    # Check if the source directory exists\n    if not os.path.exists(source_directory):\n        raise OSError(\"Source directory does not exist\")\n    \n    # Define valid extensions\n    valid_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Create a list to hold the files to be zipped\n    files_to_zip = []\n    \n    # Walk through the source directory\n    for root, dirs, files in os.walk(source_directory):\n        for file in files:\n            # Check if the file has a valid extension\n            if Path(file).suffix in valid_extensions:\n                # Append the full path of the file to the list\n                files_to_zip.append(os.path.join(root, file))\n    \n    # Create the target directory if it does not exist\n    if not os.path.exists(target_directory):\n        os.makedirs(target_directory)\n    \n    # Create the zip file\n    zip_path = os.path.join(target_directory, f\"{zip_name}.zip\")\n    with zipfile.ZipFile(zip_path, 'w') as zipf:\n        for file in files_to_zip:\n            # Add each file to the zip file\n            zipf.write(file, os.path.relpath(file, source_directory))\n    \n    # Return the full path to the created zip file\n    return zip_path"}
{"task_id": "BigCodeBench/964", "solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nfrom openpyxl import load_workbook\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Initialize a counter for successful conversions\n    successful_conversions = 0\n    \n    # Iterate over all files in the source directory\n    for file_path in source_path.iterdir():\n        if file_path.is_file() and file_path.suffix in supported_extensions:\n            # Read the content of the file based on its extension\n            if file_path.suffix == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            elif file_path.suffix == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n            elif file_path.suffix == '.xlsx':\n                workbook = load_workbook(file_path)\n                sheet = workbook.active\n                content = '\\n'.join([str(cell.value) for row in sheet.iter_rows(values_only=True) for cell in row])\n            elif file_path.suffix == '.csv':\n                df = pd.read_csv(file_path)\n                content = '\\n'.join(df.values.flatten().astype(str))\n            \n            # Create a CSV file with the content\n            csv_file_path = target_path / (file_path.stem + '.csv')\n            df = pd.DataFrame({'Text': [content]})\n            df.to_csv(csv_file_path, index=False)\n            \n            # Increment the counter for successful conversions\n            successful_conversions += 1\n    \n    return successful_conversions", "raw_solution": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport docx\nfrom openpyxl import load_workbook\n\ndef task_func(source_directory: str, target_directory: str) -> int:\n    # Check if the source directory exists\n    source_path = Path(source_directory)\n    if not source_path.exists():\n        raise FileNotFoundError(f\"The source directory {source_directory} does not exist.\")\n    \n    # Create the target directory if it doesn't exist\n    target_path = Path(target_directory)\n    target_path.mkdir(parents=True, exist_ok=True)\n    \n    # List of supported file extensions\n    supported_extensions = ['.txt', '.docx', '.xlsx', '.csv']\n    \n    # Initialize a counter for successful conversions\n    successful_conversions = 0\n    \n    # Iterate over all files in the source directory\n    for file_path in source_path.iterdir():\n        if file_path.is_file() and file_path.suffix in supported_extensions:\n            # Read the content of the file based on its extension\n            if file_path.suffix == '.txt':\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    content = file.read()\n            elif file_path.suffix == '.docx':\n                doc = docx.Document(file_path)\n                content = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n            elif file_path.suffix == '.xlsx':\n                workbook = load_workbook(file_path)\n                sheet = workbook.active\n                content = '\\n'.join([str(cell.value) for row in sheet.iter_rows(values_only=True) for cell in row])\n            elif file_path.suffix == '.csv':\n                df = pd.read_csv(file_path)\n                content = '\\n'.join(df.values.flatten().astype(str))\n            \n            # Create a CSV file with the content\n            csv_file_path = target_path / (file_path.stem + '.csv')\n            df = pd.DataFrame({'Text': [content]})\n            df.to_csv(csv_file_path, index=False)\n            \n            # Increment the counter for successful conversions\n            successful_conversions += 1\n    \n    return successful_conversions"}
{"task_id": "BigCodeBench/969", "solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the DataFrame contains NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Create a copy of the DataFrame to store the results\n    result_df = df.copy()\n    \n    # Iterate over each column in the DataFrame\n    for column in df.columns:\n        # Check if the column is numeric\n        if not np.issubdtype(df[column].dtype, np.number):\n            raise TypeError(f\"Column '{column}' contains non-numeric data types.\")\n        \n        # Compute the cumulative sum of the column\n        cumsum = df[column].cumsum()\n        \n        # Fit and transform the cumulative sum using MinMaxScaler\n        scaled_cumsum = scaler.fit_transform(cumsum.values.reshape(-1, 1))\n        \n        # Assign the scaled cumulative sum back to the result DataFrame\n        result_df[column] = scaled_cumsum.flatten()\n    \n    return result_df", "raw_solution": "import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\ndef task_func(df: pd.DataFrame) -> pd.DataFrame:\n    # Check if the DataFrame is empty\n    if df.empty:\n        raise ValueError(\"The DataFrame is empty.\")\n    \n    # Check if the DataFrame contains NaN values\n    if df.isnull().values.any():\n        raise ValueError(\"The DataFrame contains NaN values.\")\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Create a copy of the DataFrame to store the results\n    result_df = df.copy()\n    \n    # Iterate over each column in the DataFrame\n    for column in df.columns:\n        # Check if the column is numeric\n        if not np.issubdtype(df[column].dtype, np.number):\n            raise TypeError(f\"Column '{column}' contains non-numeric data types.\")\n        \n        # Compute the cumulative sum of the column\n        cumsum = df[column].cumsum()\n        \n        # Fit and transform the cumulative sum using MinMaxScaler\n        scaled_cumsum = scaler.fit_transform(cumsum.values.reshape(-1, 1))\n        \n        # Assign the scaled cumulative sum back to the result DataFrame\n        result_df[column] = scaled_cumsum.flatten()\n    \n    return result_df"}
{"task_id": "BigCodeBench/971", "solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\ndef task_func(directory_path: str):\n    # Check if the provided directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # Initialize an empty list to store the results\n    result = []\n    \n    # Iterate over each file in the directory\n    for file_name in os.listdir(directory_path):\n        # Construct the full path to the file\n        file_path = os.path.join(directory_path, file_name)\n        \n        # Get the file size in bytes\n        file_size = os.path.getsize(file_path)\n        \n        # Get the creation time of the file\n        creation_time = datetime.fromtimestamp(os.path.getctime(file_path), timezone.utc).isoformat()\n        \n        # Get the last modification time of the file\n        modification_time = datetime.fromtimestamp(os.path.getmtime(file_path), timezone.utc).isoformat()\n        \n        # Append the file details as a tuple to the result list\n        result.append((file_name, file_size, creation_time, modification_time))\n    \n    # Return the list of tuples containing file details\n    return result", "raw_solution": "import os\nfrom pathlib import Path\nfrom datetime import datetime, timezone\n\ndef task_func(directory_path: str):\n    # Check if the provided directory exists\n    if not os.path.exists(directory_path):\n        raise ValueError(\"The provided directory does not exist.\")\n    \n    # Initialize an empty list to store the results\n    result = []\n    \n    # Iterate over each file in the directory\n    for file_name in os.listdir(directory_path):\n        # Construct the full path to the file\n        file_path = os.path.join(directory_path, file_name)\n        \n        # Get the file size in bytes\n        file_size = os.path.getsize(file_path)\n        \n        # Get the creation time of the file\n        creation_time = datetime.fromtimestamp(os.path.getctime(file_path), timezone.utc).isoformat()\n        \n        # Get the last modification time of the file\n        modification_time = datetime.fromtimestamp(os.path.getmtime(file_path), timezone.utc).isoformat()\n        \n        # Append the file details as a tuple to the result list\n        result.append((file_name, file_size, creation_time, modification_time))\n    \n    # Return the list of tuples containing file details\n    return result\n\n# Example usage:\n# result = task_func('/path/to/empty_directory')\n# print(result)  # Output: []"}
{"task_id": "BigCodeBench/985", "solution": "import pandas as pd\nimport json\nimport os\nimport math\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Check if JSON data is empty\n    if not json_data:\n        raise ValueError(\"JSON data is empty\")\n    \n    # Load JSON data into a dictionary\n    data = json.loads(json_data)\n    \n    # Initialize a list to store country and population data\n    country_population = []\n    \n    # Iterate through the JSON data\n    for country, population in data.items():\n        # Check if country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name encountered\")\n        \n        # Check if population is numeric and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative population encountered\")\n        \n        # Round down the population to the nearest integer if it's a float\n        population = math.floor(population)\n        \n        # Append the country and population to the list\n        country_population.append([country, population])\n    \n    # Create a DataFrame from the list of country and population data\n    df = pd.DataFrame(country_population, columns=[\"Country\", \"Population\"])\n    \n    # Check if the output directory exists, if not, create it\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Construct the file path for the CSV report\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(f\"Cannot write to the specified directory: {output_dir}\")\n    \n    # Return the file path of the generated CSV report\n    return file_path\njson_data = '{\"China\": 1400000000, \"India\": 1300000000, \"USA\": 330000000}'\noutput_dir = \"output\"\nfile_name = \"country_population_report.csv\"", "raw_solution": "import pandas as pd\nimport json\nimport os\nimport math\n\ndef task_func(json_data, output_dir=\".\", file_name=\"country_population_report.csv\"):\n    # Check if JSON data is empty\n    if not json_data:\n        raise ValueError(\"JSON data is empty\")\n    \n    # Load JSON data into a dictionary\n    data = json.loads(json_data)\n    \n    # Initialize a list to store country and population data\n    country_population = []\n    \n    # Iterate through the JSON data\n    for country, population in data.items():\n        # Check if country name is a string\n        if not isinstance(country, str):\n            raise ValueError(\"Non-string country name encountered\")\n        \n        # Check if population is numeric and non-negative\n        if not isinstance(population, (int, float)) or population < 0:\n            raise ValueError(\"Non-numeric or negative population encountered\")\n        \n        # Round down the population to the nearest integer if it's a float\n        population = math.floor(population)\n        \n        # Append the country and population to the list\n        country_population.append([country, population])\n    \n    # Create a DataFrame from the list of country and population data\n    df = pd.DataFrame(country_population, columns=[\"Country\", \"Population\"])\n    \n    # Check if the output directory exists, if not, create it\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    # Construct the file path for the CSV report\n    file_path = os.path.join(output_dir, file_name)\n    \n    # Write the DataFrame to a CSV file\n    try:\n        df.to_csv(file_path, index=False)\n    except IOError:\n        raise IOError(f\"Cannot write to the specified directory: {output_dir}\")\n    \n    # Return the file path of the generated CSV report\n    return file_path\n\n# Example usage:\njson_data = '{\"China\": 1400000000, \"India\": 1300000000, \"USA\": 330000000}'\noutput_dir = \"output\"\nfile_name = \"country_population_report.csv\"\nfile_path = task_func(json_data, output_dir, file_name)"}
{"task_id": "BigCodeBench/988", "solution": "import os\nimport re\nfrom pathlib import Path\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Check if the specified directory exists and is a directory\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"The specified directory does not exist or is not a directory.\")\n    \n    # Initialize the result dictionary\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        \n        # Check each predicate against the item name\n        for predicate in predicates:\n            if re.match(predicate, item):\n                results[item] = True\n                break\n        else:\n            results[item] = False\n    \n    return results", "raw_solution": "import os\nimport re\nfrom pathlib import Path\n\ndef task_func(dir_path: str, predicates: list) -> dict:\n    # Check if predicates are provided\n    if not predicates:\n        raise ValueError(\"No valid predicates provided.\")\n    \n    # Check if the specified directory exists and is a directory\n    if not os.path.exists(dir_path) or not os.path.isdir(dir_path):\n        raise FileNotFoundError(\"The specified directory does not exist or is not a directory.\")\n    \n    # Initialize the result dictionary\n    results = {}\n    \n    # Iterate over each item in the directory\n    for item in os.listdir(dir_path):\n        item_path = os.path.join(dir_path, item)\n        \n        # Check each predicate against the item name\n        for predicate in predicates:\n            if re.match(predicate, item):\n                results[item] = True\n                break\n        else:\n            results[item] = False\n    \n    return results"}
{"task_id": "BigCodeBench/990", "solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\ndef task_func(hex_string):\n    # Convert hexadecimal string to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n    \n    # Convert bytes to UTF-8 string\n    utf8_string = bytes_data.decode('utf-8', errors='ignore')\n    \n    # Encode UTF-8 string into various formats\n    encoded_dict = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string if utf8_string.isascii() else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return encoded_dict", "raw_solution": "import binascii\nimport base64\nimport urllib.parse\nimport codecs\n\ndef task_func(hex_string):\n    # Convert hexadecimal string to bytes\n    bytes_data = binascii.unhexlify(hex_string)\n    \n    # Convert bytes to UTF-8 string\n    utf8_string = bytes_data.decode('utf-8', errors='ignore')\n    \n    # Encode UTF-8 string into various formats\n    encoded_dict = {\n        'hex': hex_string,\n        'base64': base64.b64encode(bytes_data).decode('utf-8'),\n        'utf-8': utf8_string,\n        'utf-16': utf8_string.encode('utf-16').decode('utf-16'),\n        'utf-32': utf8_string.encode('utf-32').decode('utf-32'),\n        'ASCII': utf8_string if utf8_string.isascii() else 'Not representable in ASCII',\n        'URL': urllib.parse.quote(utf8_string),\n        'ROT13': codecs.encode(utf8_string, 'rot_13')\n    }\n    \n    return encoded_dict\n\n# Test the function"}
{"task_id": "BigCodeBench/998", "solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\ndef task_func(url):\n    try:\n        # Download the tar.gz file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        \n        # Compare the calculated MD5 checksum with the expected value\n        if md5_hash.hexdigest() == EXPECTED_MD5_CHECKSUM:\n            # Extract the contents of the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall(path=\"extracted_files\")\n            return True\n        else:\n            # Delete the downloaded file if the checksum does not match\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        # Handle any exceptions that may occur during the process\n        print(f\"An error occurred: {e}\")\n        return False\nurl = \"http://example.com/path/to/downloaded_files.tar.gz\"", "raw_solution": "import urllib.request\nimport os\nimport hashlib\nimport tarfile\n\n# Constants\nTARGET_TAR_FILE = \"downloaded_files.tar.gz\"\nEXPECTED_MD5_CHECKSUM = \"d41d8cd98f00b204e9800998ecf8427e\"\n\ndef task_func(url):\n    try:\n        # Download the tar.gz file\n        urllib.request.urlretrieve(url, TARGET_TAR_FILE)\n        \n        # Calculate the MD5 checksum of the downloaded file\n        md5_hash = hashlib.md5()\n        with open(TARGET_TAR_FILE, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(chunk)\n        \n        # Compare the calculated MD5 checksum with the expected value\n        if md5_hash.hexdigest() == EXPECTED_MD5_CHECKSUM:\n            # Extract the contents of the tar.gz file\n            with tarfile.open(TARGET_TAR_FILE, \"r:gz\") as tar:\n                tar.extractall(path=\"extracted_files\")\n            return True\n        else:\n            # Delete the downloaded file if the checksum does not match\n            os.remove(TARGET_TAR_FILE)\n            return False\n    except Exception as e:\n        # Handle any exceptions that may occur during the process\n        print(f\"An error occurred: {e}\")\n        return False\n\n# Example usage\nurl = \"http://example.com/path/to/downloaded_files.tar.gz\"\nresult = task_func(url)"}
{"task_id": "BigCodeBench/999", "solution": "import urllib.request\nimport os\nimport csv\nimport collections\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a dictionary to count occurrences\n        value_counts = collections.defaultdict(int)\n        \n        # Open the CSV file and read the data\n        with open(csv_file_path, mode='r', newline='') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                os.remove(csv_file_path)  # Delete the downloaded file\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value_counts[row[column_name]] += 1\n        \n        # Delete the downloaded file after processing\n        os.remove(csv_file_path)\n        \n        return dict(value_counts)\n    \n    except Exception as e:\n        # Handle any exceptions that occur during the process\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import urllib.request\nimport os\nimport csv\nimport collections\n\ndef task_func(url, column_name, csv_file_path):\n    try:\n        # Download the CSV file\n        urllib.request.urlretrieve(url, csv_file_path)\n        \n        # Initialize a dictionary to count occurrences\n        value_counts = collections.defaultdict(int)\n        \n        # Open the CSV file and read the data\n        with open(csv_file_path, mode='r', newline='') as file:\n            reader = csv.DictReader(file)\n            \n            # Check if the specified column exists\n            if column_name not in reader.fieldnames:\n                os.remove(csv_file_path)  # Delete the downloaded file\n                raise ValueError(f\"The provided column_name '{column_name}' does not exist in the CSV file.\")\n            \n            # Count occurrences of each value in the specified column\n            for row in reader:\n                value_counts[row[column_name]] += 1\n        \n        # Delete the downloaded file after processing\n        os.remove(csv_file_path)\n        \n        return dict(value_counts)\n    \n    except Exception as e:\n        # Handle any exceptions that occur during the process\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage:\n# result = task_func('http://example.com/data.csv', 'column_name', 'path_to_save.csv')\n# print(result)"}
{"task_id": "BigCodeBench/1003", "solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\ndef task_func(url):\n    try:\n        # Fetch the XML file from the specified URL\n        response = urllib.request.urlopen(url)\n        xml_data = response.read()\n        \n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n        \n        # Check if the root element is 'items'\n        if root.tag != 'items':\n            raise ValueError(\"XML structure does not match expected format.\")\n        \n        # Extract data from 'item' elements\n        data = []\n        for item in root.findall('item'):\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text\n            data.append(item_data)\n        \n        # Create a DataFrame from the extracted data\n        df = pd.DataFrame(data)\n        \n        return df\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch XML from URL: {e.reason}\")\n    except etree.XMLSyntaxError:\n        raise ValueError(\"Invalid XML syntax.\")\n    except Exception as e:\n        raise ValueError(f\"An unexpected error occurred: {e}\")", "raw_solution": "import urllib.request\nfrom lxml import etree\nimport pandas as pd\n\ndef task_func(url):\n    try:\n        # Fetch the XML file from the specified URL\n        response = urllib.request.urlopen(url)\n        xml_data = response.read()\n        \n        # Parse the XML data\n        root = etree.fromstring(xml_data)\n        \n        # Check if the root element is 'items'\n        if root.tag != 'items':\n            raise ValueError(\"XML structure does not match expected format.\")\n        \n        # Extract data from 'item' elements\n        data = []\n        for item in root.findall('item'):\n            item_data = {}\n            for child in item:\n                item_data[child.tag] = child.text\n            data.append(item_data)\n        \n        # Create a DataFrame from the extracted data\n        df = pd.DataFrame(data)\n        \n        return df\n    except urllib.error.URLError as e:\n        raise ValueError(f\"Failed to fetch XML from URL: {e.reason}\")\n    except etree.XMLSyntaxError:\n        raise ValueError(\"Invalid XML syntax.\")\n    except Exception as e:\n        raise ValueError(f\"An unexpected error occurred: {e}\")\n\n# Example usage:\n# df = task_func('http://example.com/valid_structure.xml')\n# print(df)"}
{"task_id": "BigCodeBench/1004", "solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(url):\n    # Download the text file from the specified URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Use a regular expression to find words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_count = Counter(words)\n\n    # Get the ten most common words\n    most_common_words = word_count.most_common(10)\n\n    # Prepare data for plotting\n    words, counts = zip(*most_common_words)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n\n    return word_count, ax", "raw_solution": "import urllib.request\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(url):\n    # Download the text file from the specified URL\n    response = urllib.request.urlopen(url)\n    text = response.read().decode('utf-8')\n\n    # Use a regular expression to find words\n    words = re.findall(r'\\b\\w+\\b', text)\n\n    # Count the frequency of each word\n    word_count = Counter(words)\n\n    # Get the ten most common words\n    most_common_words = word_count.most_common(10)\n\n    # Prepare data for plotting\n    words, counts = zip(*most_common_words)\n\n    # Create a bar chart\n    fig, ax = plt.subplots()\n    ax.bar(words, counts)\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Frequent Words')\n\n    return word_count, ax"}
{"task_id": "BigCodeBench/1006", "solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Send a GET request to the URL\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Check if the URL points to a ZIP file\n        if response.headers['Content-Type'] != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Define the path to save the downloaded ZIP file\n        zip_file_path = os.path.join(download_path, \"downloaded_file.zip\")\n\n        # Write the content of the response to a file\n        with open(zip_file_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Try to extract the ZIP file\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Return the path to the directory containing the extracted contents\n        return os.path.join(download_path, \"extracted_files\")\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Unable to download the file from the provided URL. {str(e)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"", "raw_solution": "import os\nimport requests\nfrom zipfile import ZipFile, BadZipFile\n\ndef task_func(url, download_path=\"mnt/data/downloads/\"):\n    try:\n        # Ensure the download directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # Send a GET request to the URL\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Check if the URL points to a ZIP file\n        if response.headers['Content-Type'] != 'application/zip':\n            return \"Error: The URL does not point to a ZIP file.\"\n\n        # Define the path to save the downloaded ZIP file\n        zip_file_path = os.path.join(download_path, \"downloaded_file.zip\")\n\n        # Write the content of the response to a file\n        with open(zip_file_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Try to extract the ZIP file\n        try:\n            with ZipFile(zip_file_path, 'r') as zip_ref:\n                zip_ref.extractall(download_path)\n        except BadZipFile:\n            return \"Error: The downloaded file is not a valid ZIP file.\"\n\n        # Return the path to the directory containing the extracted contents\n        return os.path.join(download_path, \"extracted_files\")\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Unable to download the file from the provided URL. {str(e)}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Example usage:\n# result = task_func(\"https://example.com/file.zip\")\n# print(result)"}
{"task_id": "BigCodeBench/1008", "solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.exceptions.HTTPError as e:\n        raise e\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    headers = [header.text for header in rows[0].find_all('th')]\n    data = [[cell.text for cell in row.find_all(['td', 'th'])] for row in rows[1:]]\n\n    df = pd.DataFrame(data, columns=headers)\n    return df", "raw_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom io import StringIO\n\ndef task_func(url, table_id):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raises an HTTPError for bad responses\n    except requests.exceptions.HTTPError as e:\n        raise e\n\n    soup = BeautifulSoup(response.text, 'html.parser')\n    table = soup.find('table', {'id': table_id})\n\n    if not table:\n        raise ValueError(\"Table with the specified ID not found.\")\n\n    rows = table.find_all('tr')\n    if not rows:\n        return pd.DataFrame()\n\n    headers = [header.text for header in rows[0].find_all('th')]\n    data = [[cell.text for cell in row.find_all(['td', 'th'])] for row in rows[1:]]\n\n    df = pd.DataFrame(data, columns=headers)\n    return df"}
{"task_id": "BigCodeBench/1012", "solution": "import requests\nfrom pathlib import Path\nimport zipfile\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\ndef task_func(url, filename):\n    # Ensure the download directory exists\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    # Ensure the unzipped directory exists\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n    # Construct the full path for the downloaded file\n    download_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Save the file to disk\n        with open(download_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Construct the full path for the zip file\n        zip_path = download_path\n\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the filenames in the unzipped directory\n        filenames = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return (\"Success\", filenames)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n    except zipfile.BadZipFile as e:\n        return (\"Error: File-related exception (BadZipFile)\", [])\n    except Exception as e:\n        return (\"Error: File-related exception\", [])", "raw_solution": "import requests\nfrom pathlib import Path\nimport zipfile\n\n# Constants\nDOWNLOAD_DIR = Path(\"downloads\")\nZIP_DIR = Path(\"unzipped_files\")\n\ndef task_func(url, filename):\n    # Ensure the download directory exists\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    # Ensure the unzipped directory exists\n    ZIP_DIR.mkdir(parents=True, exist_ok=True)\n\n    # Construct the full path for the downloaded file\n    download_path = DOWNLOAD_DIR / filename\n\n    try:\n        # Download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Save the file to disk\n        with open(download_path, 'wb') as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        # Construct the full path for the zip file\n        zip_path = download_path\n\n        # Extract the zip file\n        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(ZIP_DIR)\n\n        # List the filenames in the unzipped directory\n        filenames = [f.name for f in ZIP_DIR.iterdir() if f.is_file()]\n\n        return (\"Success\", filenames)\n\n    except requests.exceptions.RequestException as e:\n        return (\"Error: Network-related exception\", [])\n    except zipfile.BadZipFile as e:\n        return (\"Error: File-related exception (BadZipFile)\", [])\n    except Exception as e:\n        return (\"Error: File-related exception\", [])\n\n# Example usage:\n# status, files = task_func(\"http://example.com/file.zip\", \"file.zip\")\n# print(status, files)"}
{"task_id": "BigCodeBench/1013", "solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Check if the request was successful\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all anchor tags\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Iterate over all anchor tags\n    for link in links:\n        href = link.get('href')\n        if href:\n            # Join the base URL with the relative URL to get an absolute URL\n            absolute_url = urljoin(base_url, href)\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Write the unique absolute links to a CSV file\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(unique_links)", "raw_solution": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Check if the request was successful\n\n    # Parse the HTML content of the page\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all anchor tags\n    links = soup.find_all('a')\n\n    # Set to store unique absolute URLs\n    unique_links = set()\n\n    # Iterate over all anchor tags\n    for link in links:\n        href = link.get('href')\n        if href:\n            # Join the base URL with the relative URL to get an absolute URL\n            absolute_url = urljoin(base_url, href)\n            # Add the absolute URL to the set of unique links\n            unique_links.add(absolute_url)\n\n    # Write the unique absolute links to a CSV file\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        for link in unique_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(unique_links)"}
{"task_id": "BigCodeBench/1015", "solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the webpage\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n\n        # Extract the table data\n        table = tree.xpath('//table')\n        if not table:\n            return 0  # No table found\n\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(str(table[0]))[0]\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n\n        # Replace the table in the database with the new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n\n        # Close the database connection\n        conn.close()\n\n        # Return the number of rows in the parsed HTML table\n        return len(df)\n\n    except requests.RequestException as e:\n        print(f\"Network error: {e}\")\n        return 0\n\n    except sqlite3.DatabaseError as e:\n        print(f\"Database error: {e}\")\n        return 0", "raw_solution": "import requests\nfrom lxml import html\nimport pandas as pd\nimport sqlite3\n\ndef task_func(webpage_url: str, database_name: str = \"my_database.db\") -> int:\n    try:\n        # Fetch the HTML content from the webpage\n        response = requests.get(webpage_url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Parse the HTML content\n        tree = html.fromstring(response.content)\n\n        # Extract the table data\n        table = tree.xpath('//table')\n        if not table:\n            return 0  # No table found\n\n        # Convert the table to a pandas DataFrame\n        df = pd.read_html(str(table[0]))[0]\n\n        # Connect to the SQLite database\n        conn = sqlite3.connect(database_name)\n\n        # Replace the table in the database with the new data\n        df.to_sql('my_table', conn, if_exists='replace', index=False)\n\n        # Close the database connection\n        conn.close()\n\n        # Return the number of rows in the parsed HTML table\n        return len(df)\n\n    except requests.RequestException as e:\n        print(f\"Network error: {e}\")\n        return 0\n\n    except sqlite3.DatabaseError as e:\n        print(f\"Database error: {e}\")\n        return 0"}
{"task_id": "BigCodeBench/1019", "solution": "from PIL import Image\nimport codecs\nimport pytesseract\nIMAGE_PATH = \"image.png\"\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Attempt OCR extraction\n        image = Image.open(filename)\n        text = pytesseract.image_to_string(image)\n        if text:\n            return text.encode(from_encoding).decode(to_encoding)\n    except Exception as e:\n        print(f\"OCR extraction failed: {e}\")\n\n    try:\n        # Attempt comment processing\n        with open(filename, 'rb') as f:\n            image = Image.open(f)\n            comment = image.info.get('comment', b'').decode(from_encoding)\n            if comment:\n                return comment.encode(from_encoding).decode(to_encoding)\n    except Exception as e:\n        print(f\"Comment processing failed: {e}\")\n\n    return \"\"", "raw_solution": "from PIL import Image\nimport codecs\nimport pytesseract\n\nIMAGE_PATH = \"image.png\"\n\ndef task_func(filename=IMAGE_PATH, from_encoding=\"cp1251\", to_encoding=\"utf8\"):\n    try:\n        # Attempt OCR extraction\n        image = Image.open(filename)\n        text = pytesseract.image_to_string(image)\n        if text:\n            return text.encode(from_encoding).decode(to_encoding)\n    except Exception as e:\n        print(f\"OCR extraction failed: {e}\")\n\n    try:\n        # Attempt comment processing\n        with open(filename, 'rb') as f:\n            image = Image.open(f)\n            comment = image.info.get('comment', b'').decode(from_encoding)\n            if comment:\n                return comment.encode(from_encoding).decode(to_encoding)\n    except Exception as e:\n        print(f\"Comment processing failed: {e}\")\n\n    return \"\"\n\n# Example usage\nresult = task_func(IMAGE_PATH, \"cp1251\", \"utf8\")"}
{"task_id": "BigCodeBench/1020", "solution": "import json\nimport requests\nimport chardet\nAPI_URL = \"http://api.example.com/data\"\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Send HTTP GET request with a timeout of 5 seconds\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Get the raw content of the response\n        content = response.content\n\n        # Handle empty content\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if not from_encoding:\n            result = chardet.detect(content)\n            from_encoding = result['encoding']\n\n        # Decode the content using the detected encoding\n        decoded_content = content.decode(from_encoding)\n\n        # Re-encode the content to the specified encoding\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        # Decode the re-encoded content back to a string\n        final_content = reencoded_content.decode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(final_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"HTTP request error: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"JSON parsing error: {e}\")\n        return {}\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return {}", "raw_solution": "import json\nimport requests\nimport chardet\n\n# Constants\nAPI_URL = \"http://api.example.com/data\"\n\ndef task_func(url=API_URL, from_encoding=None, to_encoding=\"utf8\"):\n    try:\n        # Send HTTP GET request with a timeout of 5 seconds\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Get the raw content of the response\n        content = response.content\n\n        # Handle empty content\n        if not content:\n            return {}\n\n        # Detect encoding if not provided\n        if not from_encoding:\n            result = chardet.detect(content)\n            from_encoding = result['encoding']\n\n        # Decode the content using the detected encoding\n        decoded_content = content.decode(from_encoding)\n\n        # Re-encode the content to the specified encoding\n        reencoded_content = decoded_content.encode(to_encoding)\n\n        # Decode the re-encoded content back to a string\n        final_content = reencoded_content.decode(to_encoding)\n\n        # Parse the JSON data\n        json_data = json.loads(final_content)\n\n        return json_data\n\n    except requests.exceptions.RequestException as e:\n        print(f\"HTTP request error: {e}\")\n        return {}\n    except ValueError as e:\n        print(f\"JSON parsing error: {e}\")\n        return {}\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return {}\n\n# Example usage"}
{"task_id": "BigCodeBench/1022", "solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return an empty DataFrame if the file is empty\n        return pd.DataFrame()\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n    \n    # Convert the specified column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    \n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n    \n    return df", "raw_solution": "import pandas as pd\nimport os\nfrom datetime import datetime\nfrom pandas.errors import EmptyDataError\n\ndef task_func(csv_file_path, column_name, date_format=\"%Y-%m-%d\"):\n    # Check if the file exists\n    if not os.path.exists(csv_file_path):\n        raise FileNotFoundError(f\"The file {csv_file_path} does not exist.\")\n    \n    try:\n        # Read the CSV file\n        df = pd.read_csv(csv_file_path)\n    except EmptyDataError:\n        # Return an empty DataFrame if the file is empty\n        return pd.DataFrame()\n    \n    # Check if the specified column exists\n    if column_name not in df.columns:\n        raise ValueError(f\"The column {column_name} is not present in the CSV file.\")\n    \n    # Convert the specified column to datetime objects\n    df[column_name] = pd.to_datetime(df[column_name], format=date_format)\n    \n    # Filter rows based on the current date\n    current_date = datetime.now().date()\n    df = df[df[column_name].dt.date == current_date]\n    \n    # Sort the resulting data\n    df = df.sort_values(by=column_name)\n    \n    return df"}
{"task_id": "BigCodeBench/1039", "solution": "import ssl\nimport os\nimport hashlib\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_client_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_client_socket.recv(buffer_size).decode('utf-8').strip()\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            return 'File not found'\n\n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            while chunk := file.read(buffer_size):\n                sha256_hash.update(chunk)\n\n        # Send the SHA256 hash back to the client\n        secure_client_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close the secure client socket\n        secure_client_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f'Error: {str(e)}'", "raw_solution": "import ssl\nimport os\nimport hashlib\n\ndef task_func(client_socket, cert_file, key_file, buffer_size=1024):\n    try:\n        # Wrap the client socket with SSL/TLS\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(certfile=cert_file, keyfile=key_file)\n        secure_client_socket = context.wrap_socket(client_socket, server_side=True)\n\n        # Receive the file path from the client\n        file_path = secure_client_socket.recv(buffer_size).decode('utf-8').strip()\n\n        # Check if the file exists\n        if not os.path.exists(file_path):\n            return 'File not found'\n\n        # Calculate the SHA256 hash of the file\n        sha256_hash = hashlib.sha256()\n        with open(file_path, 'rb') as file:\n            while chunk := file.read(buffer_size):\n                sha256_hash.update(chunk)\n\n        # Send the SHA256 hash back to the client\n        secure_client_socket.sendall(sha256_hash.hexdigest().encode('utf-8'))\n\n        # Close the secure client socket\n        secure_client_socket.close()\n\n        return sha256_hash.hexdigest()\n\n    except Exception as e:\n        return f'Error: {str(e)}'"}
{"task_id": "BigCodeBench/1040", "solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    start_time = datetime.now()\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n    server_socket.setblocking(False)\n\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n\n    try:\n        while (datetime.now() - start_time) < timedelta(seconds=run_duration):\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n            for s in readable:\n                if s is server_socket:\n                    connection, client_address = s.accept()\n                    connection.setblocking(False)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                        response = data.decode('utf-8') + \" \" + current_time\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.send(next_msg.encode('utf-8'))\n\n            for s in exceptional:\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    finally:\n        server_socket.close()\n        return f\"Server stopped after {run_duration} seconds.\"", "raw_solution": "import socket\nimport select\nimport queue\nfrom datetime import datetime, timedelta\n\ndef task_func(\n    server_address=\"localhost\", server_port=12345, buffer_size=1024, run_duration=5\n):\n    start_time = datetime.now()\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind((server_address, server_port))\n    server_socket.listen(5)\n    server_socket.setblocking(False)\n\n    inputs = [server_socket]\n    outputs = []\n    message_queues = {}\n\n    try:\n        while (datetime.now() - start_time) < timedelta(seconds=run_duration):\n            readable, writable, exceptional = select.select(inputs, outputs, inputs, 0.1)\n\n            for s in readable:\n                if s is server_socket:\n                    connection, client_address = s.accept()\n                    connection.setblocking(False)\n                    inputs.append(connection)\n                    message_queues[connection] = queue.Queue()\n                else:\n                    data = s.recv(buffer_size)\n                    if data:\n                        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                        response = data.decode('utf-8') + \" \" + current_time\n                        message_queues[s].put(response)\n                        if s not in outputs:\n                            outputs.append(s)\n                    else:\n                        if s in outputs:\n                            outputs.remove(s)\n                        inputs.remove(s)\n                        s.close()\n                        del message_queues[s]\n\n            for s in writable:\n                try:\n                    next_msg = message_queues[s].get_nowait()\n                except queue.Empty:\n                    outputs.remove(s)\n                else:\n                    s.send(next_msg.encode('utf-8'))\n\n            for s in exceptional:\n                inputs.remove(s)\n                if s in outputs:\n                    outputs.remove(s)\n                s.close()\n                del message_queues[s]\n\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n    finally:\n        server_socket.close()\n        return f\"Server stopped after {run_duration} seconds.\""}
{"task_id": "BigCodeBench/1042", "solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\ndef task_func(client_socket):\n    # Receive the message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Ask for sender's email, recipient's email, and sender's email password\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n    \n    # Create an email message\n    email = EmailMessage()\n    email.set_content(message)\n    email['Subject'] = 'Message from Client'\n    email['From'] = sender_email\n    email['To'] = recipient_email\n    \n    # Connect to the SMTP server and send the email\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()\n        server.login(sender_email, sender_password)\n        server.send_message(email)\n    \n    # Close the client socket\n    client_socket.close()", "raw_solution": "import smtplib\nfrom email.message import EmailMessage\nimport getpass\n\nSERVER_ADDRESS = \"localhost\"\nSERVER_PORT = 25\nBUFFER_SIZE = 1024\nSMTP_SERVER = \"smtp.gmail.com\"\nSMTP_PORT = 587\n\ndef task_func(client_socket):\n    # Receive the message from the client socket\n    message = client_socket.recv(BUFFER_SIZE).decode('utf-8')\n    \n    # Ask for sender's email, recipient's email, and sender's email password\n    sender_email = input(\"Enter sender's email: \")\n    recipient_email = input(\"Enter recipient's email: \")\n    sender_password = getpass.getpass(\"Enter sender's email password: \")\n    \n    # Create an email message\n    email = EmailMessage()\n    email.set_content(message)\n    email['Subject'] = 'Message from Client'\n    email['From'] = sender_email\n    email['To'] = recipient_email\n    \n    # Connect to the SMTP server and send the email\n    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n        server.starttls()\n        server.login(sender_email, sender_password)\n        server.send_message(email)\n    \n    # Close the client socket\n    client_socket.close()"}
{"task_id": "BigCodeBench/1053", "solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        # Check if the file has a header\n        if 'Text' not in df.columns:\n            df.columns = ['Text']\n        # Initialize CountVectorizer with stopwords\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        # Fit and transform the text data\n        X = vectorizer.fit_transform(df['Text'])\n        # Get the feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n        # Get the counts of each word\n        counts = X.toarray().sum(axis=0)\n        # Create a dictionary of word counts\n        word_counts = dict(zip(feature_names, counts))\n        # Sort the dictionary by counts in descending order\n        sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n        # Get the top 10 most common words\n        top_10_words, top_10_counts = zip(*sorted_word_counts[:10])\n        # Create a bar plot\n        plt.figure(figsize=(10, 5))\n        plt.bar(top_10_words, top_10_counts, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        # If save_path is provided, save the plot to the specified path\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        # Otherwise, return the Axes object of the plot\n        else:\n            return plt.gca()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None", "raw_solution": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n# Constants\nSTOP_WORDS = [\"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"and\", \"or\"]\ndef task_func(file_path, save_path=None):\n    try:\n        # Read the CSV file\n        df = pd.read_csv(file_path)\n        # Check if the file has a header\n        if 'Text' not in df.columns:\n            df.columns = ['Text']\n        # Initialize CountVectorizer with stopwords\n        vectorizer = CountVectorizer(stop_words=STOP_WORDS)\n        # Fit and transform the text data\n        X = vectorizer.fit_transform(df['Text'])\n        # Get the feature names (words)\n        feature_names = vectorizer.get_feature_names_out()\n        # Get the counts of each word\n        counts = X.toarray().sum(axis=0)\n        # Create a dictionary of word counts\n        word_counts = dict(zip(feature_names, counts))\n        # Sort the dictionary by counts in descending order\n        sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n        # Get the top 10 most common words\n        top_10_words, top_10_counts = zip(*sorted_word_counts[:10])\n        # Create a bar plot\n        plt.figure(figsize=(10, 5))\n        plt.bar(top_10_words, top_10_counts, color='blue')\n        plt.xlabel('Words')\n        plt.ylabel('Frequency')\n        plt.title('Top 10 Most Common Words')\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        # If save_path is provided, save the plot to the specified path\n        if save_path:\n            plt.savefig(save_path)\n            plt.close()\n            return None\n        # Otherwise, return the Axes object of the plot\n        else:\n            return plt.gca()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None"}
{"task_id": "BigCodeBench/1057", "solution": "import pandas as pd\nimport itertools\nimport numpy as np\ndef task_func(animals=None, foods=None):\n    # Define default lists if not provided\n    default_animals = ['lion', 'tiger', 'bear']\n    default_foods = ['meat', 'fish', 'vegetables']\n    \n    # Use default lists if provided lists are empty or None\n    animals = animals if animals and animals else default_animals\n    foods = foods if foods and foods else default_foods\n    \n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations to randomize the DataFrame layout\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame from the shuffled combinations\n    df = pd.DataFrame(combinations, columns=['animal:food'])\n    \n    return df", "raw_solution": "import pandas as pd\nimport itertools\nimport numpy as np\n\ndef task_func(animals=None, foods=None):\n    # Define default lists if not provided\n    default_animals = ['lion', 'tiger', 'bear']\n    default_foods = ['meat', 'fish', 'vegetables']\n    \n    # Use default lists if provided lists are empty or None\n    animals = animals if animals and animals else default_animals\n    foods = foods if foods and foods else default_foods\n    \n    # Generate all combinations of animals and foods\n    combinations = list(itertools.product(animals, foods))\n    \n    # Shuffle the combinations to randomize the DataFrame layout\n    np.random.shuffle(combinations)\n    \n    # Create a DataFrame from the shuffled combinations\n    df = pd.DataFrame(combinations, columns=['animal:food'])\n    \n    return df\n\n# Example usage:\n# df = task_func(animals=['dog', 'cat'], foods=['bone', 'fish'])\n# print(df)"}
{"task_id": "BigCodeBench/1077", "solution": "from datetime import datetime\nimport pytz\nimport numpy as np\ndef task_func(time_strings, timezone):\n    # Convert the timezone string to a pytz timezone object\n    tz = pytz.timezone(timezone)\n    \n    # Convert each time string to a datetime object in the specified timezone\n    dt_objects = [datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time difference in seconds between each consecutive pair of timestamps\n    time_differences = [abs((dt_objects[i] - dt_objects[i-1]).total_seconds()) for i in range(1, len(dt_objects))]\n    \n    # If there are less than two timestamps, return 0.0\n    if len(time_differences) < 1:\n        return 0.0\n    \n    # Calculate and return the mean (average) time difference in seconds\n    return np.mean(time_differences)", "raw_solution": "from datetime import datetime\nimport pytz\nimport numpy as np\n\ndef task_func(time_strings, timezone):\n    # Convert the timezone string to a pytz timezone object\n    tz = pytz.timezone(timezone)\n    \n    # Convert each time string to a datetime object in the specified timezone\n    dt_objects = [datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').replace(tzinfo=pytz.utc).astimezone(tz) for ts in time_strings]\n    \n    # Calculate the absolute time difference in seconds between each consecutive pair of timestamps\n    time_differences = [abs((dt_objects[i] - dt_objects[i-1]).total_seconds()) for i in range(1, len(dt_objects))]\n    \n    # If there are less than two timestamps, return 0.0\n    if len(time_differences) < 1:\n        return 0.0\n    \n    # Calculate and return the mean (average) time difference in seconds\n    return np.mean(time_differences)"}
{"task_id": "BigCodeBench/1085", "solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Create a bar chart for the top 10 words\n    fig, ax = plt.subplots()\n    ax.bar([word for word, count in top_10_words], [count for word, count in top_10_words])\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return top_10_words, ax", "raw_solution": "import re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\ndef task_func(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove punctuation using regex\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Split the text into words\n    words = text.split()\n    \n    # Count the frequency of each word\n    word_counts = Counter(words)\n    \n    # Get the top 10 most common words\n    top_10_words = word_counts.most_common(10)\n    \n    # Create a bar chart for the top 10 words\n    fig, ax = plt.subplots()\n    ax.bar([word for word, count in top_10_words], [count for word, count in top_10_words])\n    ax.set_xlabel('Words')\n    ax.set_ylabel('Frequency')\n    ax.set_title('Top 10 Most Common Words')\n    \n    return top_10_words, ax"}
{"task_id": "BigCodeBench/1124", "solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = re.findall(url_pattern, myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    for url in urls:\n        try:\n            # Send a GET request to the URL\n            response = requests.get(url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            \n            # Parse the HTML content of the page\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Find the title tag\n            title_tag = soup.find('title')\n            \n            if title_tag:\n                return title_tag.get_text()\n            else:\n                return \"No title tag found in the webpage.\"\n        except requests.exceptions.RequestException as e:\n            return f\"Unable to fetch the content of the URL: {url}\"", "raw_solution": "import re\nfrom urllib.parse import urlparse\nfrom bs4 import BeautifulSoup\nimport requests\n\ndef task_func(myString):\n    # Regular expression to find URLs in the string\n    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    urls = re.findall(url_pattern, myString)\n    \n    if not urls:\n        return \"No valid URL found in the provided string.\"\n    \n    for url in urls:\n        try:\n            # Send a GET request to the URL\n            response = requests.get(url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            \n            # Parse the HTML content of the page\n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Find the title tag\n            title_tag = soup.find('title')\n            \n            if title_tag:\n                return title_tag.get_text()\n            else:\n                return \"No title tag found in the webpage.\"\n        except requests.exceptions.RequestException as e:\n            return f\"Unable to fetch the content of the URL: {url}\""}
{"task_id": "BigCodeBench/1129", "solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Retrieve the URL associated with the specified key\n    url = data.get(unknown_key)\n    \n    if not url:\n        raise ValueError(f\"Key '{unknown_key}' not found in JSON data.\")\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        save_path = os.path.join(save_dir, filename)\n    else:\n        save_path = os.path.join(os.getcwd(), filename)\n    \n    # Save the file\n    with open(save_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(save_path)", "raw_solution": "import json\nimport requests\nimport os\nfrom datetime import datetime\n\ndef task_func(json_data, unknown_key, save_dir=None):\n    # Parse the JSON data\n    data = json.loads(json_data)\n    \n    # Retrieve the URL associated with the specified key\n    url = data.get(unknown_key)\n    \n    if not url:\n        raise ValueError(f\"Key '{unknown_key}' not found in JSON data.\")\n    \n    # Download the file from the URL\n    response = requests.get(url)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n    \n    # Generate a timestamped filename\n    timestamp = datetime.now().strftime('%Y%m%d%H%M%S%f')\n    filename = f\"{unknown_key}_{timestamp}.txt\"\n    \n    # Determine the save directory\n    if save_dir:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        save_path = os.path.join(save_dir, filename)\n    else:\n        save_path = os.path.join(os.getcwd(), filename)\n    \n    # Save the file\n    with open(save_path, 'wb') as file:\n        file.write(response.content)\n    \n    # Return the absolute path of the saved file\n    return os.path.abspath(save_path)"}
{"task_id": "BigCodeBench/1137", "solution": "import bs4\nimport requests\nimport re\nimport json\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern to match phone numbers\n    phone_pattern = re.compile(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # Fetch the content from the URL\n    if url.startswith('http'):\n        response = requests.get(url)\n        response.raise_for_status()  # Check for HTTP errors\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Extract text from the parsed HTML\n    text = soup.get_text()\n\n    # Find all phone numbers in the text\n    phone_numbers = phone_pattern.findall(text)\n\n    # Save the phone numbers to a JSON file\n    with open(output_path, 'w') as json_file:\n        json.dump(phone_numbers, json_file, indent=4)\n\n    return phone_numbers", "raw_solution": "import bs4\nimport requests\nimport re\nimport json\n\ndef task_func(url: str, output_path: str) -> list:\n    # Regular expression pattern to match phone numbers\n    phone_pattern = re.compile(r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b')\n\n    # Fetch the content from the URL\n    if url.startswith('http'):\n        response = requests.get(url)\n        response.raise_for_status()  # Check for HTTP errors\n        content = response.text\n    else:\n        with open(url, 'r') as file:\n            content = file.read()\n\n    # Use BeautifulSoup to parse the HTML content\n    soup = bs4.BeautifulSoup(content, 'html.parser')\n\n    # Extract text from the parsed HTML\n    text = soup.get_text()\n\n    # Find all phone numbers in the text\n    phone_numbers = phone_pattern.findall(text)\n\n    # Save the phone numbers to a JSON file\n    with open(output_path, 'w') as json_file:\n        json.dump(phone_numbers, json_file, indent=4)\n\n    return phone_numbers"}